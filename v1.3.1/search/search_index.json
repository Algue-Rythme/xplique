{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\ud83e\udd8a Xplique (pronounced \\\u025bks.plik\\ ) is a Python toolkit dedicated to explainability. The goal of this library is to gather the state of the art of Explainable AI to help you understand your complex neural network models. Originally built for Tensorflow's model it also works for PyTorch models partially. \ud83d\udcd8 Explore Xplique docs | Explore Xplique tutorials \ud83d\udd25 Attributions \u00b7 Concept \u00b7 Feature Visualization \u00b7 Metrics The library is composed of several modules, the Attributions Methods module implements various methods (e.g Saliency, Grad-CAM, Integrated-Gradients...), with explanations, examples and links to official papers. The Feature Visualization module allows to see how neural networks build their understanding of images by finding inputs that maximize neurons, channels, layers or compositions of these elements. The Concepts module allows you to extract human concepts from a model and to test their usefulness with respect to a class. Finally, the Metrics module covers the current metrics used in explainability. Used in conjunction with the Attribution Methods module, it allows you to test the different methods or evaluate the explanations of a model. \ud83d\udd25 Tutorials \u00b6 We propose some Hands-on tutorials to get familiar with the library and its api Attribution Methods : Getting started Attribution Methods : Sanity checks paper Attribution Methods : Tabular data and Regression Attribution Methods : Object Detection Attribution Methods : Semantic Segmentation FORGRad : Gradient strikes back with FORGrad Attribution Methods : Metrics PyTorch models : Getting started Concepts Methods : Testing with Concept Activation Vectors Concepts Methods : CRAFT: Getting started on Tensorflow Concepts Methods : CRAFT: Getting started on Pytorch Feature Visualization : Getting started Feature Visualization : Getting started Modern Feature Visualization with MaCo : Getting started You can find a certain number of other practical tutorials just here . This section is actively developed and more contents will be included. We will try to cover all the possible usage of the library, feel free to contact us if you have any suggestions or recommendations towards tutorials you would like to see. \ud83d\ude80 Quick Start \u00b6 Xplique requires a version of python higher than 3.7 and several libraries including Tensorflow and Numpy. Installation can be done using Pypi: pip install xplique Now that Xplique is installed, here are some basic examples of what you can do with the available modules. Attributions Methods Let's start with a simple example, by computing Grad-CAM for several images (or a complete dataset) on a trained model. from xplique.attributions import GradCAM # load images, labels and model # ... explainer = GradCAM ( model ) explanations = explainer . explain ( images , labels ) # or just `explainer(images, labels)` All attributions methods share a common API described in the attributions API documentation . Attributions Metrics In order to measure if the explanations provided by our method are faithful (it reflects well the functioning of the model) we can use a fidelity metric such as Deletion from xplique.attributions import GradCAM from xplique.metrics import Deletion # load images, labels and model # ... explainer = GradCAM ( model ) explanations = explainer ( inputs , labels ) metric = Deletion ( model , inputs , labels ) score_grad_cam = metric ( explanations ) All attributions metrics share a common API. You can find out more about it here . Concepts Extraction CAV \u00b6 Concerning the concept-based methods, we can for example extract a concept vector from a layer of a model. In order to do this, we use two datasets, one containing inputs containing the concept: positive_samples , the other containing other entries which do not contain the concept: negative_samples . from xplique.concepts import Cav # load a model, samples that contain a concept # (positive) and samples who don't (negative) # ... extractor = Cav ( model , 'mixed3' ) concept_vector = extractor ( positive_samples , negative_samples ) More information on CAV here and on TCAV here . CRAFT \u00b6 Use Craft to investigate a single class. from xplique.concepts import CraftTf as Craft # Cut the model in two parts: g and h # Create a Craft concept extractor from these 2 models craft = Craft ( input_to_latent_model = g , latent_to_logit_model = h ) # Use Craft to compute the concepts for a specific class craft . fit ( images_preprocessed , class_id = rabbit_class_id ) # Compute Sobol indices to understand which concept matters importances = craft . estimate_importance () # Display those concepts by showing the 10 best crops for each concept craft . plot_concepts_crops ( nb_crops = 10 ) More information in the CRAFT documentation . Feature Visualization Finally, in order to find an image that maximizes a neuron and at the same time a layer, we build two objectives that we combine together. We then call the optimizer which returns our images from xplique.features_visualizations import Objective from xplique.features_visualizations import optimize # load a model... neuron_obj = Objective . neuron ( model , \"logits\" , 200 ) channel_obj = Objective . layer ( model , \"mixed3\" , 10 ) obj = neuron_obj + 2.0 * channel_obj images , obj_names = optimize ( obj ) Want to know more ? Check the Feature Viz documentation PyTorch with Xplique Even though the library was mainly designed to be a Tensorflow toolbox we have been working on a very practical wrapper to facilitate the integration of your PyTorch models into Xplique's framework! import torch from xplique.wrappers import TorchWrapper from xplique.attributions import Saliency from xplique.metrics import Deletion # load images, targets and model # ... device = 'cuda' if torch . cuda . is_available () else 'cpu' wrapped_model = TorchWrapper ( torch_model , device ) explainer = Saliency ( wrapped_model ) explanations = explainer ( inputs , targets ) metric = Deletion ( wrapped_model , inputs , targets ) score_saliency = metric ( explanations ) Want to know more ? Check the PyTorch documentation \ud83d\udce6 What's Included \u00b6 There are 4 modules in Xplique, Attribution methods , Attribution metrics , Concepts , and Feature visualization . In particular, the attribution methods module supports a huge diversity of tasks: Classification , Regression , Object Detection , and Semantic Segmentation . For diverse data types: Images, Time Series, and Tabular data . The methods compatible with such task and methods compatible with Tensorflow or PyTorch are highlighted in the following table: Table of attributions available Attribution Method Type of Model Source Images Time Series and Tabular Data Tutorial Deconvolution TF Paper C:\u2714\ufe0f OD:\u274c SS:\u274c C:\u2714\ufe0f R:\u2714\ufe0f Grad-CAM TF Paper C:\u2714\ufe0f OD:\u274c SS:\u274c \u274c Grad-CAM++ TF Paper C:\u2714\ufe0f OD:\u274c SS:\u274c \u274c Gradient Input TF, PyTorch** Paper C:\u2714\ufe0f OD:\u2714\ufe0f SS:\u2714\ufe0f C:\u2714\ufe0f R:\u2714\ufe0f Guided Backprop TF Paper C:\u2714\ufe0f OD:\u274c SS:\u274c C:\u2714\ufe0f R:\u2714\ufe0f Integrated Gradients TF, PyTorch** Paper C:\u2714\ufe0f OD:\u2714\ufe0f SS:\u2714\ufe0f C:\u2714\ufe0f R:\u2714\ufe0f Kernel SHAP TF, PyTorch* , Callable Paper C:\u2714\ufe0f OD:\u2714\ufe0f SS:\u2714\ufe0f C:\u2714\ufe0f R:\u2714\ufe0f Lime TF, PyTorch* , Callable Paper C:\u2714\ufe0f OD:\u2714\ufe0f SS:\u2714\ufe0f C:\u2714\ufe0f R:\u2714\ufe0f Occlusion TF, PyTorch* , Callable Paper C:\u2714\ufe0f OD:\u2714\ufe0f SS:\u2714\ufe0f C:\u2714\ufe0f R:\u2714\ufe0f Rise TF, PyTorch* , Callable Paper C:\u2714\ufe0f OD:\u2714\ufe0f SS:\u2714\ufe0f C:\u2714\ufe0f R:\u2714\ufe0f Saliency TF, PyTorch** Paper C:\u2714\ufe0f OD:\u2714\ufe0f SS:\u2714\ufe0f C:\u2714\ufe0f R:\u2714\ufe0f SmoothGrad TF, PyTorch** Paper C:\u2714\ufe0f OD:\u2714\ufe0f SS:\u2714\ufe0f C:\u2714\ufe0f R:\u2714\ufe0f SquareGrad TF, PyTorch** Paper C:\u2714\ufe0f OD:\u2714\ufe0f SS:\u2714\ufe0f C:\u2714\ufe0f R:\u2714\ufe0f VarGrad TF, PyTorch** Paper C:\u2714\ufe0f OD:\u2714\ufe0f SS:\u2714\ufe0f C:\u2714\ufe0f R:\u2714\ufe0f Sobol Attribution TF, PyTorch** Paper C:\u2714\ufe0f OD:\u2714\ufe0f SS:\u2714\ufe0f \ud83d\udd35 Hsic Attribution TF, PyTorch** Paper C:\u2714\ufe0f OD:\u2714\ufe0f SS:\u2714\ufe0f \ud83d\udd35 FORGrad enhancement TF, PyTorch** Paper C:\u2714\ufe0f OD:\u2714\ufe0f SS:\u2714\ufe0f \u274c TF : Tensorflow compatible C : Classification | R : Regression | OD : Object Detection | SS : Semantic Segmentation * : See the Callable documentation ** : See the Xplique for PyTorch documentation , and the PyTorch models : Getting started notebook. \u2714\ufe0f : Supported by Xplique | \u274c : Not applicable | \ud83d\udd35 : Work in Progress Table of attribution's metric available Attribution Metrics Type of Model Property Source MuFidelity TF, PyTorch** Fidelity Paper Deletion TF, PyTorch** Fidelity Paper Insertion TF, PyTorch** Fidelity Paper Average Stability TF, PyTorch** Stability Paper MeGe TF, PyTorch** Representativity Paper ReCo TF, PyTorch** Consistency Paper (WIP) e-robustness TF : Tensorflow compatible ** : See the Xplique for PyTorch documentation , and the PyTorch models : Getting started notebook. Table of concept methods available Concepts method Type of Model Source Tutorial Concept Activation Vector (CAV) TF Paper Testing CAV (TCAV) TF Paper CRAFT Tensorflow TF Paper CRAFT PyTorch PyTorch** Paper (WIP) Robust TCAV (WIP) Automatic Concept Extraction (ACE) TF : Tensorflow compatible ** : See the Xplique for Pytorch documentation , and the PyTorch's model : Getting started notebook Table of Feature Visualization methods available Feature Visualization (Paper) Type of Model Details Neurons TF Optimizes for specific neurons Layer TF Optimizes for specific layers Channel TF Optimizes for specific channels Direction TF Optimizes for specific vector Fourrier Preconditioning TF Optimize in Fourier basis (see preconditioning ) Objective combination TF Allows to combine objectives MaCo TF Fixed Magnitude optimisation, see Paper TF : Tensorflow compatible \ud83d\udc4d Contributing \u00b6 Feel free to propose your ideas or come and contribute with us on the Xplique toolbox! We have a specific document where we describe in a simple way how to make your first pull request: just here . \ud83d\udc40 See Also \u00b6 This library is one approach of many to explain your model. We don't expect it to be the perfect solution; we create it to explore one point in the space of possibilities. Other interesting tools to explain your model: Lucid the wonderful library specialized in feature visualization from OpenAI. Captum the PyTorch library for Interpretability research Tf-explain that implement multiples attribution methods and propose callbacks API for tensorflow. Alibi Explain for model inspection and interpretation SHAP a very popular library to compute local explanations using the classic Shapley values from game theory and their related extensions To learn more about Explainable AI in general: Interpretable Machine Learning by Christophe Molnar. Interpretability Beyond Feature Attribution by Been Kim. Explaining ML Predictions: State-of-the-art, Challenges, and Opportunities by Himabindu Lakkaraju, Julius Adebayo and Sameer Singh. A Roadmap for the Rigorous Science of Interpretability by Finale Doshi-Velez. DEEL White paper a summary of the DEEL team on the challenges of certifiable AI and the role of explainability for this purpose More from the DEEL project: deel-lip a Python library for training k-Lipschitz neural networks on TF. deel-torchlip a Python library for training k-Lipschitz neural networks on PyTorch. Influenciae Python toolkit dedicated to computing influence values for the discovery of potentially problematic samples in a dataset. LARD Landing Approach Runway Detection (LARD) is a dataset of aerial front view images of runways designed for aircraft landing phase PUNCC Puncc (Predictive uncertainty calibration and conformalization) is an open-source Python library that integrates a collection of state-of-the-art conformal prediction algorithms and related techniques for regression and classification problems OODEEL OODeel is a library that performs post-hoc deep OOD detection on already trained neural network image classifiers. The philosophy of the library is to favor quality over quantity and to foster easy adoption DEEL White paper a summary of the DEEL team on the challenges of certifiable AI and the role of data quality, representativity and explainability for this purpose. \ud83d\ude4f Acknowledgments \u00b6 This project received funding from the French \u201dInvesting for the Future \u2013 PIA3\u201d program within the Artificial and Natural Intelligence Toulouse Institute (ANITI). The authors gratefully acknowledge the support of the DEEL project. \ud83d\udc68\u200d\ud83c\udf93 Creators \u00b6 This library was started as a side-project by Thomas FEL who is currently a graduate student at the Artificial and Natural Intelligence Toulouse Institute under the direction of Thomas SERRE . His thesis work focuses on explainability for deep neural networks. He then received help from some members of the DEEL team to enhance the library namely from Lucas Hervier and Antonin Poch\u00e9 . \ud83d\uddde\ufe0f Citation \u00b6 If you use Xplique as part of your workflow in a scientific publication, please consider citing the \ud83d\uddde\ufe0f Xplique official paper : @article{fel2022xplique, title={Xplique: A Deep Learning Explainability Toolbox}, author={Fel, Thomas and Hervier, Lucas and Vigouroux, David and Poche, Antonin and Plakoo, Justin and Cadene, Remi and Chalvidal, Mathieu and Colin, Julien and Boissin, Thibaut and Bethune, Louis and Picard, Agustin and Nicodeme, Claire and Gardes, Laurent and Flandin, Gregory and Serre, Thomas}, journal={Workshop on Explainable Artificial Intelligence for Computer Vision (CVPR)}, year={2022} } \ud83d\udcdd License \u00b6 The package is released under MIT license .","title":"Home"},{"location":"#tutorials","text":"We propose some Hands-on tutorials to get familiar with the library and its api Attribution Methods : Getting started Attribution Methods : Sanity checks paper Attribution Methods : Tabular data and Regression Attribution Methods : Object Detection Attribution Methods : Semantic Segmentation FORGRad : Gradient strikes back with FORGrad Attribution Methods : Metrics PyTorch models : Getting started Concepts Methods : Testing with Concept Activation Vectors Concepts Methods : CRAFT: Getting started on Tensorflow Concepts Methods : CRAFT: Getting started on Pytorch Feature Visualization : Getting started Feature Visualization : Getting started Modern Feature Visualization with MaCo : Getting started You can find a certain number of other practical tutorials just here . This section is actively developed and more contents will be included. We will try to cover all the possible usage of the library, feel free to contact us if you have any suggestions or recommendations towards tutorials you would like to see.","title":"\ud83d\udd25 Tutorials"},{"location":"#quick-start","text":"Xplique requires a version of python higher than 3.7 and several libraries including Tensorflow and Numpy. Installation can be done using Pypi: pip install xplique Now that Xplique is installed, here are some basic examples of what you can do with the available modules. Attributions Methods Let's start with a simple example, by computing Grad-CAM for several images (or a complete dataset) on a trained model. from xplique.attributions import GradCAM # load images, labels and model # ... explainer = GradCAM ( model ) explanations = explainer . explain ( images , labels ) # or just `explainer(images, labels)` All attributions methods share a common API described in the attributions API documentation . Attributions Metrics In order to measure if the explanations provided by our method are faithful (it reflects well the functioning of the model) we can use a fidelity metric such as Deletion from xplique.attributions import GradCAM from xplique.metrics import Deletion # load images, labels and model # ... explainer = GradCAM ( model ) explanations = explainer ( inputs , labels ) metric = Deletion ( model , inputs , labels ) score_grad_cam = metric ( explanations ) All attributions metrics share a common API. You can find out more about it here . Concepts Extraction","title":"\ud83d\ude80 Quick Start"},{"location":"#whats-included","text":"There are 4 modules in Xplique, Attribution methods , Attribution metrics , Concepts , and Feature visualization . In particular, the attribution methods module supports a huge diversity of tasks: Classification , Regression , Object Detection , and Semantic Segmentation . For diverse data types: Images, Time Series, and Tabular data . The methods compatible with such task and methods compatible with Tensorflow or PyTorch are highlighted in the following table: Table of attributions available Attribution Method Type of Model Source Images Time Series and Tabular Data Tutorial Deconvolution TF Paper C:\u2714\ufe0f OD:\u274c SS:\u274c C:\u2714\ufe0f R:\u2714\ufe0f Grad-CAM TF Paper C:\u2714\ufe0f OD:\u274c SS:\u274c \u274c Grad-CAM++ TF Paper C:\u2714\ufe0f OD:\u274c SS:\u274c \u274c Gradient Input TF, PyTorch** Paper C:\u2714\ufe0f OD:\u2714\ufe0f SS:\u2714\ufe0f C:\u2714\ufe0f R:\u2714\ufe0f Guided Backprop TF Paper C:\u2714\ufe0f OD:\u274c SS:\u274c C:\u2714\ufe0f R:\u2714\ufe0f Integrated Gradients TF, PyTorch** Paper C:\u2714\ufe0f OD:\u2714\ufe0f SS:\u2714\ufe0f C:\u2714\ufe0f R:\u2714\ufe0f Kernel SHAP TF, PyTorch* , Callable Paper C:\u2714\ufe0f OD:\u2714\ufe0f SS:\u2714\ufe0f C:\u2714\ufe0f R:\u2714\ufe0f Lime TF, PyTorch* , Callable Paper C:\u2714\ufe0f OD:\u2714\ufe0f SS:\u2714\ufe0f C:\u2714\ufe0f R:\u2714\ufe0f Occlusion TF, PyTorch* , Callable Paper C:\u2714\ufe0f OD:\u2714\ufe0f SS:\u2714\ufe0f C:\u2714\ufe0f R:\u2714\ufe0f Rise TF, PyTorch* , Callable Paper C:\u2714\ufe0f OD:\u2714\ufe0f SS:\u2714\ufe0f C:\u2714\ufe0f R:\u2714\ufe0f Saliency TF, PyTorch** Paper C:\u2714\ufe0f OD:\u2714\ufe0f SS:\u2714\ufe0f C:\u2714\ufe0f R:\u2714\ufe0f SmoothGrad TF, PyTorch** Paper C:\u2714\ufe0f OD:\u2714\ufe0f SS:\u2714\ufe0f C:\u2714\ufe0f R:\u2714\ufe0f SquareGrad TF, PyTorch** Paper C:\u2714\ufe0f OD:\u2714\ufe0f SS:\u2714\ufe0f C:\u2714\ufe0f R:\u2714\ufe0f VarGrad TF, PyTorch** Paper C:\u2714\ufe0f OD:\u2714\ufe0f SS:\u2714\ufe0f C:\u2714\ufe0f R:\u2714\ufe0f Sobol Attribution TF, PyTorch** Paper C:\u2714\ufe0f OD:\u2714\ufe0f SS:\u2714\ufe0f \ud83d\udd35 Hsic Attribution TF, PyTorch** Paper C:\u2714\ufe0f OD:\u2714\ufe0f SS:\u2714\ufe0f \ud83d\udd35 FORGrad enhancement TF, PyTorch** Paper C:\u2714\ufe0f OD:\u2714\ufe0f SS:\u2714\ufe0f \u274c TF : Tensorflow compatible C : Classification | R : Regression | OD : Object Detection | SS : Semantic Segmentation * : See the Callable documentation ** : See the Xplique for PyTorch documentation , and the PyTorch models : Getting started notebook. \u2714\ufe0f : Supported by Xplique | \u274c : Not applicable | \ud83d\udd35 : Work in Progress Table of attribution's metric available Attribution Metrics Type of Model Property Source MuFidelity TF, PyTorch** Fidelity Paper Deletion TF, PyTorch** Fidelity Paper Insertion TF, PyTorch** Fidelity Paper Average Stability TF, PyTorch** Stability Paper MeGe TF, PyTorch** Representativity Paper ReCo TF, PyTorch** Consistency Paper (WIP) e-robustness TF : Tensorflow compatible ** : See the Xplique for PyTorch documentation , and the PyTorch models : Getting started notebook. Table of concept methods available Concepts method Type of Model Source Tutorial Concept Activation Vector (CAV) TF Paper Testing CAV (TCAV) TF Paper CRAFT Tensorflow TF Paper CRAFT PyTorch PyTorch** Paper (WIP) Robust TCAV (WIP) Automatic Concept Extraction (ACE) TF : Tensorflow compatible ** : See the Xplique for Pytorch documentation , and the PyTorch's model : Getting started notebook Table of Feature Visualization methods available Feature Visualization (Paper) Type of Model Details Neurons TF Optimizes for specific neurons Layer TF Optimizes for specific layers Channel TF Optimizes for specific channels Direction TF Optimizes for specific vector Fourrier Preconditioning TF Optimize in Fourier basis (see preconditioning ) Objective combination TF Allows to combine objectives MaCo TF Fixed Magnitude optimisation, see Paper TF : Tensorflow compatible","title":"\ud83d\udce6 What's Included"},{"location":"#contributing","text":"Feel free to propose your ideas or come and contribute with us on the Xplique toolbox! We have a specific document where we describe in a simple way how to make your first pull request: just here .","title":"\ud83d\udc4d Contributing"},{"location":"#see-also","text":"This library is one approach of many to explain your model. We don't expect it to be the perfect solution; we create it to explore one point in the space of possibilities. Other interesting tools to explain your model: Lucid the wonderful library specialized in feature visualization from OpenAI. Captum the PyTorch library for Interpretability research Tf-explain that implement multiples attribution methods and propose callbacks API for tensorflow. Alibi Explain for model inspection and interpretation SHAP a very popular library to compute local explanations using the classic Shapley values from game theory and their related extensions To learn more about Explainable AI in general: Interpretable Machine Learning by Christophe Molnar. Interpretability Beyond Feature Attribution by Been Kim. Explaining ML Predictions: State-of-the-art, Challenges, and Opportunities by Himabindu Lakkaraju, Julius Adebayo and Sameer Singh. A Roadmap for the Rigorous Science of Interpretability by Finale Doshi-Velez. DEEL White paper a summary of the DEEL team on the challenges of certifiable AI and the role of explainability for this purpose More from the DEEL project: deel-lip a Python library for training k-Lipschitz neural networks on TF. deel-torchlip a Python library for training k-Lipschitz neural networks on PyTorch. Influenciae Python toolkit dedicated to computing influence values for the discovery of potentially problematic samples in a dataset. LARD Landing Approach Runway Detection (LARD) is a dataset of aerial front view images of runways designed for aircraft landing phase PUNCC Puncc (Predictive uncertainty calibration and conformalization) is an open-source Python library that integrates a collection of state-of-the-art conformal prediction algorithms and related techniques for regression and classification problems OODEEL OODeel is a library that performs post-hoc deep OOD detection on already trained neural network image classifiers. The philosophy of the library is to favor quality over quantity and to foster easy adoption DEEL White paper a summary of the DEEL team on the challenges of certifiable AI and the role of data quality, representativity and explainability for this purpose.","title":"\ud83d\udc40 See Also"},{"location":"#acknowledgments","text":"This project received funding from the French \u201dInvesting for the Future \u2013 PIA3\u201d program within the Artificial and Natural Intelligence Toulouse Institute (ANITI). The authors gratefully acknowledge the support of the DEEL project.","title":"\ud83d\ude4f Acknowledgments"},{"location":"#creators","text":"This library was started as a side-project by Thomas FEL who is currently a graduate student at the Artificial and Natural Intelligence Toulouse Institute under the direction of Thomas SERRE . His thesis work focuses on explainability for deep neural networks. He then received help from some members of the DEEL team to enhance the library namely from Lucas Hervier and Antonin Poch\u00e9 .","title":"\ud83d\udc68\u200d\ud83c\udf93 Creators"},{"location":"#citation","text":"If you use Xplique as part of your workflow in a scientific publication, please consider citing the \ud83d\uddde\ufe0f Xplique official paper : @article{fel2022xplique, title={Xplique: A Deep Learning Explainability Toolbox}, author={Fel, Thomas and Hervier, Lucas and Vigouroux, David and Poche, Antonin and Plakoo, Justin and Cadene, Remi and Chalvidal, Mathieu and Colin, Julien and Boissin, Thibaut and Bethune, Louis and Picard, Agustin and Nicodeme, Claire and Gardes, Laurent and Flandin, Gregory and Serre, Thomas}, journal={Workshop on Explainable Artificial Intelligence for Computer Vision (CVPR)}, year={2022} }","title":"\ud83d\uddde\ufe0f Citation"},{"location":"#license","text":"The package is released under MIT license .","title":"\ud83d\udcdd License"},{"location":"contributing/","text":"Contributing \ud83d\ude4f \u00b6 Thanks for taking the time to contribute! \ud83c\udf89\ud83d\udc4d From opening a bug report to creating a pull request: every contribution is appreciated and welcome. If you're planning to implement a new feature or change the api please create an issue first. This way we can ensure that your precious work is not in vain. Setup with make \u2699\ufe0f \u00b6 Clone the repo git clone https://github.com/deel-ai/xplique.git . Go to your freshly downloaded repo cd xplique Create a virtual environment and install the necessary dependencies for development make prepare-dev && source xplique_dev_env/bin/activate . You are ready to install the library pip install -e . or run the test suite make test . Welcome to the team \ud83d\udd25\ud83d\ude80 ! Setup without make \u2699\ufe0f \u00b6 Clone the repo git clone https://github.com/deel-ai/xplique.git . Go to your freshly downloaded repo cd xplique Install virtualenv with pip : pip install virtualenv Or with conda : conda install -c conda-forge virtualenv Create a new virtual environment venv xplique_dev_env Activate your new environment . xplique_dev_env/bin/activate Depending on your machine, this operation might be slightly different. For instance, on Windows you should probably do (with cmd.exe): ~/xplique> path\\to\\xplique_dev_env\\bin\\activate.bat Or with Powershell: PS ~/xplique> path\\to\\xplique_dev_env\\bin\\Activate.ps1 Anyway, if you suceed you should see your virtual environment name in front of any other command: (xplique_dev_env) :~/xplique$ You can now install all necessary packages, with pip: pip install -r requirements.txt pip install -r requirements_dev.txt Or with conda: conda install --file requirements.txt conda install --file requirements_dev.txt You are ready to install the library: pip install -e . Or run the test suite: tox You are now ready to code and to be part of the team \ud83d\udd25\ud83d\ude80 ! Tests \u2705 \u00b6 A pretty fair question would be to know what is make test doing ? It is actually just a command which activate your virtual environment and launch the tox command. So basically, if you do not succeed to use make just activate your virtual env and do tox ! tox on the otherhand will do the following: - run pytest on the tests folder with python 3.6, python 3.7 and python 3.8 Note: If you do not have those 3 interpreters the tests would be only performs with your current interpreter - run pylint on the xplique main files, also with python 3.6, python 3.7 and python 3.8 Note: It is possible that pylint throw false-positive errors. If the linting test failed please check first pylint output to point out the reasons. Please, make sure you run all the tests at least once before opening a pull request. A word toward Pylint for those that don't know it: Pylint is a Python static code analysis tool which looks for programming errors, helps enforcing a coding standard, sniffs for code smells and offers simple refactoring suggestions. Basically, it will check that your code follow a certain number of convention. Any Pull Request will go through a Github workflow ensuring that your code respect the Pylint conventions (most of them at least). Submitting Changes \ud83d\udd03 \u00b6 After getting some feedback, push to your fork and submit a pull request. We may suggest some changes or improvements or alternatives, but for small changes your pull request should be accepted quickly. Something that will increase the chance that your pull request is accepted: Write tests and ensure that the existing ones pass. If make test is succesful, you have fair chances to pass the CI workflows (linting and test) Follow the existing coding style. Write a good commit message (we follow a lowercase convention). For a major fix/feature make sure your PR has an issue and if it doesn't, please create one. This would help discussion with the community, and polishing ideas in case of a new feature. Documentation \ud83d\udcda \u00b6 Xplique is a small library but documentation is often a huge time sink for users. That's why we greatly appreciate any time spent fixing typos or clarifying sections in the documentation. To setup a local live-server to update the documentation: make serve-doc or activate your virtual env and: CUDA_VISIBLE_DEVICES=-1 mkdocs serve","title":"Contributing"},{"location":"contributing/#contributing","text":"Thanks for taking the time to contribute! \ud83c\udf89\ud83d\udc4d From opening a bug report to creating a pull request: every contribution is appreciated and welcome. If you're planning to implement a new feature or change the api please create an issue first. This way we can ensure that your precious work is not in vain.","title":"Contributing \ud83d\ude4f"},{"location":"contributing/#setup-with-make","text":"Clone the repo git clone https://github.com/deel-ai/xplique.git . Go to your freshly downloaded repo cd xplique Create a virtual environment and install the necessary dependencies for development make prepare-dev && source xplique_dev_env/bin/activate . You are ready to install the library pip install -e . or run the test suite make test . Welcome to the team \ud83d\udd25\ud83d\ude80 !","title":"Setup with make \u2699\ufe0f"},{"location":"contributing/#setup-without-make","text":"Clone the repo git clone https://github.com/deel-ai/xplique.git . Go to your freshly downloaded repo cd xplique Install virtualenv with pip : pip install virtualenv Or with conda : conda install -c conda-forge virtualenv Create a new virtual environment venv xplique_dev_env Activate your new environment . xplique_dev_env/bin/activate Depending on your machine, this operation might be slightly different. For instance, on Windows you should probably do (with cmd.exe): ~/xplique> path\\to\\xplique_dev_env\\bin\\activate.bat Or with Powershell: PS ~/xplique> path\\to\\xplique_dev_env\\bin\\Activate.ps1 Anyway, if you suceed you should see your virtual environment name in front of any other command: (xplique_dev_env) :~/xplique$ You can now install all necessary packages, with pip: pip install -r requirements.txt pip install -r requirements_dev.txt Or with conda: conda install --file requirements.txt conda install --file requirements_dev.txt You are ready to install the library: pip install -e . Or run the test suite: tox You are now ready to code and to be part of the team \ud83d\udd25\ud83d\ude80 !","title":"Setup without make \u2699\ufe0f"},{"location":"contributing/#tests","text":"A pretty fair question would be to know what is make test doing ? It is actually just a command which activate your virtual environment and launch the tox command. So basically, if you do not succeed to use make just activate your virtual env and do tox ! tox on the otherhand will do the following: - run pytest on the tests folder with python 3.6, python 3.7 and python 3.8 Note: If you do not have those 3 interpreters the tests would be only performs with your current interpreter - run pylint on the xplique main files, also with python 3.6, python 3.7 and python 3.8 Note: It is possible that pylint throw false-positive errors. If the linting test failed please check first pylint output to point out the reasons. Please, make sure you run all the tests at least once before opening a pull request. A word toward Pylint for those that don't know it: Pylint is a Python static code analysis tool which looks for programming errors, helps enforcing a coding standard, sniffs for code smells and offers simple refactoring suggestions. Basically, it will check that your code follow a certain number of convention. Any Pull Request will go through a Github workflow ensuring that your code respect the Pylint conventions (most of them at least).","title":"Tests \u2705"},{"location":"contributing/#submitting-changes","text":"After getting some feedback, push to your fork and submit a pull request. We may suggest some changes or improvements or alternatives, but for small changes your pull request should be accepted quickly. Something that will increase the chance that your pull request is accepted: Write tests and ensure that the existing ones pass. If make test is succesful, you have fair chances to pass the CI workflows (linting and test) Follow the existing coding style. Write a good commit message (we follow a lowercase convention). For a major fix/feature make sure your PR has an issue and if it doesn't, please create one. This would help discussion with the community, and polishing ideas in case of a new feature.","title":"Submitting Changes \ud83d\udd03"},{"location":"contributing/#documentation","text":"Xplique is a small library but documentation is often a huge time sink for users. That's why we greatly appreciate any time spent fixing typos or clarifying sections in the documentation. To setup a local live-server to update the documentation: make serve-doc or activate your virtual env and: CUDA_VISIBLE_DEVICES=-1 mkdocs serve","title":"Documentation \ud83d\udcda"},{"location":"tutorials/","text":"Tutorials: Notebooks \ud83d\udcd4 \u00b6 We propose here several tutorials to discover the different functionalities that the library has to offer. We decided to host those tutorials on Google Colab mainly because you will be able to play the notebooks with a GPU which should greatly improve your User eXperience. Here is the lists of the availables tutorial for now: Getting Started \u00b6 Tutorial Name Notebook Getting Started Sanity checks for Saliency Maps Times Series and Regression Tabular data and Regression Object detection Semantic Segmentation Metrics Concept Activation Vectors Feature Visualization Attributions \u00b6 Category Tutorial Name Notebook BlackBox KernelShap BlackBox Lime BlackBox Occlusion BlackBox Rise WhiteBox DeconvNet WhiteBox GradCAM WhiteBox GradCAM++ WhiteBox GradientInput WhiteBox GuidedBackpropagation WhiteBox IntegratedGradients WhiteBox Saliency WhiteBox SmoothGrad WhiteBox SquareGrad WhiteBox VarGrad Tabular Data Regression Tabular Data Times Series Regression Time Series Metrics \u00b6 Category Tutorial Name Notebook Fidelity MuFidelity Fidelity Insertion Fidelity Deletion Stability AverageStability (WIP) PyTorch Wrapper \u00b6 Tutorial Name Notebook PyTorch models: Getting started Metrics: With PyTorch models Object detection on PyTorch model Semantic Segmentation on PyTorch model Concepts extraction \u00b6 Category Tutorial Name Notebook Labelled concept methods CAV + TCAV Automatic concept extraction CRAFT Tensorflow Automatic concept extraction CRAFT Pytorch Feature Visualization \u00b6 Tutorial Name Notebook Feature Visualization: Getting started Modern Feature Visualization: MaCo","title":"Tutorials"},{"location":"tutorials/#tutorials-notebooks","text":"We propose here several tutorials to discover the different functionalities that the library has to offer. We decided to host those tutorials on Google Colab mainly because you will be able to play the notebooks with a GPU which should greatly improve your User eXperience. Here is the lists of the availables tutorial for now:","title":"Tutorials: Notebooks \ud83d\udcd4"},{"location":"tutorials/#getting-started","text":"Tutorial Name Notebook Getting Started Sanity checks for Saliency Maps Times Series and Regression Tabular data and Regression Object detection Semantic Segmentation Metrics Concept Activation Vectors Feature Visualization","title":"Getting Started"},{"location":"tutorials/#attributions","text":"Category Tutorial Name Notebook BlackBox KernelShap BlackBox Lime BlackBox Occlusion BlackBox Rise WhiteBox DeconvNet WhiteBox GradCAM WhiteBox GradCAM++ WhiteBox GradientInput WhiteBox GuidedBackpropagation WhiteBox IntegratedGradients WhiteBox Saliency WhiteBox SmoothGrad WhiteBox SquareGrad WhiteBox VarGrad Tabular Data Regression Tabular Data Times Series Regression Time Series","title":"Attributions"},{"location":"tutorials/#metrics","text":"Category Tutorial Name Notebook Fidelity MuFidelity Fidelity Insertion Fidelity Deletion Stability AverageStability (WIP)","title":"Metrics"},{"location":"tutorials/#pytorch-wrapper","text":"Tutorial Name Notebook PyTorch models: Getting started Metrics: With PyTorch models Object detection on PyTorch model Semantic Segmentation on PyTorch model","title":"PyTorch Wrapper"},{"location":"tutorials/#concepts-extraction","text":"Category Tutorial Name Notebook Labelled concept methods CAV + TCAV Automatic concept extraction CRAFT Tensorflow Automatic concept extraction CRAFT Pytorch","title":"Concepts extraction"},{"location":"tutorials/#feature-visualization","text":"Tutorial Name Notebook Feature Visualization: Getting started Modern Feature Visualization: MaCo","title":"Feature Visualization"},{"location":"api/attributions/api_attributions/","text":"API: Attributions Methods \u00b6 Attribution Methods : Getting started Context \u00b6 In 2013, Simonyan et al. proposed a first attribution method, opening the way to a wide range of approaches which could be defined as follow: Definition The main objective in attributions techniques is to highlight the discriminating variables for decision-making. For instance, with Computer Vision (CV) tasks, the main goal is to underline the pixels contributing the most in the input image(s) leading to the model\u2019s output(s). Common API \u00b6 explainer = Method ( model , batch_size , operator ) explanation = explainer ( inputs , targets ) The API have two steps: explainer instantiation : Method is an attribution method among those displayed methods tables . It inherits from the Base class BlackBoxExplainer . Their initialization takes 3 parameters apart from the specific ones and generates an explainer : model : the model from which we want to obtain attributions (e.g: InceptionV3, ResNet, ...), see the model section for more details and specifications. batch_size : an integer which allows to either process inputs per batch (gradient-based methods) or process perturbed samples of an input per batch (inputs are therefore processed one by one). operator : enum identifying the task of the model (which is Classification by default), string identifying the task, or function to explain, see the task and operator section for more detail. explainer call : The call to explainer generates the explanations, it takes two parameters: inputs : the samples on which the explanations are requested, see inputs section for more detail. targets : another parameter to specify what to explain in the inputs , it changes depending on the operator , see targets section for more detail. Info The __call__ method of explainers is an alias for the explain method. Info This documentation page covers the different parameters of the common API of attributions methods. It is common between the different tasks covered by Xplique for attribution methods. Methods \u00b6 Even though we made an harmonized API for all attributions methods, it might be relevant for the user to distinguish Perturbation-based methods and Gradient-based methods , also often referenced respectively as black-box and white-box methods, as their hyperparameters settings might be quite different. Perturbation-based approaches \u00b6 Perturbation based methods focus on perturbing an input with a variety of techniques and, with the analysis of the resulting outputs, define an attribution representation. Thus, there is no need to explicitly know the model architecture as long as forward pass is available, which explains why they are also referenced as black-box methods. Therefore, to use perturbation-based approaches you do not need a TF model. To know more, please see the Callable documentation. Xplique includes the following black-box attributions: Method Name and Documentation link Tutorial Available with TF Available with PyTorch* KernelShap \u2714 \u2714 Lime \u2714 \u2714 Occlusion \u2714 \u2714 Rise \u2714 \u2714 Sobol Attribution \u2714 \u2714 Hsic Attribution \u2714 \u2714 *: Before using a PyTorch model it is highly recommended to read the dedicated documentation Gradient-based approaches \u00b6 Those approaches are also called white-box methods as they require a full access to the model's architecture , notably it must allow computing gradients . Indeed, the core idea with the gradient-based approaches is to use back-propagation, not to update the model\u2019s weights (which is already trained) but to reveal the most contributing inputs, potentially in a specific layer. All methods are available when the model works with TensorFlow but most methods also work with PyTorch (see Xplique for PyTorch documentation ) Method Name and Documentation link Tutorial Available with TF Available with PyTorch* DeconvNet \u2714 \u274c GradCAM \u2714 \u274c GradCAM++ \u2714 \u274c GradientInput \u2714 \u2714 GuidedBackpropagation \u2714 \u274c IntegratedGradients \u2714 \u2714 Saliency \u2714 \u2714 SmoothGrad \u2714 \u2714 SquareGrad \u2714 \u2714 VarGrad \u2714 \u2714 *: Before using a PyTorch model it is highly recommended to read the dedicated documentation In addition, these methods inherit from WhiteBoxExplainer (itself inheriting from BlackBoxExplainer ). Thus, two additional __init__ arguments are added: output_layer . It is the layer to target for the output (e.g logits or after softmax). If an int is provided, it will be interpreted as a layer index, if a string is provided it will look for the layer name. Default to the last layer. reducer . For images, most gradient-based provide a value for each channel, however, for consistency, it was decided that for images, explanations will have the shape \\((n, h, w, 1)\\) . Therefore, gradient-based methods need to reduce the channel dimension of their image explanations and the reducer parameter choose how to do it among { \"mean\" , \"min\" , \"max\" , \"sum\" , None }. In the case None is give, the channel dimension is not reduced. The default value is \"mean\" for methods excepts Saliency which is \"max\" to comply with the paper and GradCAM and GradCAMPP which are not concerned. Tip It is recommended to use the layer before Softmax. Warning The output_layer parameter will work well with TensorFlow models. However, it will not work with PyTorch models. For PyTorch, one should directly manipulate the model to focus on the layers of interest. Info The \"white-box\" explainers that work with PyTorch are those that only require the gradient of the model without having to \"modify\" some part of the model (e.g. Deconvnet will commute all original ReLU by a custom ReLU policy) model \u00b6 model is the primary parameter of attribution methods: it represents model from which explanations are required. Even though we tried to support a wide-range of models, our attributions framework relies on some assumptions which we propose to see in this section. Warning In case the model does not respect the specifications, a wrapper will be needed as described in the Models not respecting the specifications section . In practice, we expect the model to be callable for the inputs parameters -- i.e. we can do model(inputs) . We expect this call to produce the outputs variables that are the predictions of the model on those inputs. As for most attribution methods, we need to manipulate and/or link the outputs to the inputs . We assume that the latter follow conventional shapes described in the inputs section . Info Depending on the task and operator there may be supplementary specifications for the model, mainly on the output of the model. Tasks and operator \u00b6 operator is one of the main parameters for both attribution methods and metrics . It defines the function that we want to explain. E.g. : In the case we have a classifier model, the function that we might want to explain is the one that given a target provides us the score of the model for that specific target -- i.e \\(model(input)[target]\\) . Note The operator parameter is a feature available for version > \\(1.\\) . The operator default values are the ones used before the introduction of this new feature! Leitmotiv \u00b6 The operator parameter was introduced to offer users a flexible way to adapt current attribution methods or metrics. It should help them to empirically tackle new use-cases/new tasks. Broadly speaking, it should amplify the user's ability to experiment. However, this also implies that it is the user's responsibility to make sure that its derivations are in-scope of the original method and make sense. operator in practice \u00b6 In practice, the user does not manipulate the function in itself. The use of the operator can be divided in three steps: Specify the operator to use in the method initialization (as shown in the API description ). Possible values are either an enum encoding the task , a string, or a custom operator . Make sure the model follows the model's specification relative to the selected task . Specify what to explain in inputs through targets , the targets parameter specifications depend on the task . The tasks covered \u00b6 The operator parameter depends on the task to explain, as the function to explain depends on the task. In the case of Xplique, the tasks in the following table are supported natively, but new operators are welcome, please feel free to contribute. Task and Documentation link operator parameter value from xplique.Tasks Enum Tutorial link Classification CLASSIFICATION Object Detection OBJECT_DETECTION Regression REGRESSION Semantic Segmentation SEMANTIC_SEGMENTATION Info Classification is the default behavior, i.e. , if no operator value is specified or None is given. Warning To apply Xplique on different tasks, specifying the value of the operator is not enough. Be sure to respect the \"operator in practice\" steps . Operators' Signature \u00b6 An operator is a function that we want to explain. This function takes as input \\(3\\) parameters: the model to explain as in the method instantiation (specifications in the model section ). the inputs parameter representing the samples to explain as in method call (specifications in inputs section ). the targets parameter encoding what to explain in the inputs (specifications in targets section ). This function should return a vector of scalar value of size \\((N,)\\) where \\(N\\) is the number of inputs in inputs -- i.e a scalar score per input. Note For gradient-based methods to work with the operator , it needs to be differentiable with respect to inputs . The operators mechanism \u00b6 Operators behavior for Black-box attribution methods For attribution approaches that do not require gradient computation, we mostly need to query the model. Thus, those methods need an inference function. If you provide an operator , it will be the inference function. More concretely, for this kind of approach, you want to compare some valued function for an original input and perturbed version of it: original_scores = operator ( model , original_inputs , original_targets ) # depending on the attribution method, this `perturbation_function` is different perturbed_inputs , perturbed_targets = perturbation_function ( original_inputs , original_targets ) perturbed_scores = operator ( model , perturbed_inputs , perturbed_targets ) # example of comparison of interest diff_scores = math . sqrt (( original_scores - perturbed_scores ) ** 2 ) Operators behavior for White-box attribution methods These methods usually require some gradients computation. The gradients that will be used are the ones of the operator function (see the get_gradient_of_operator method in the Providing custom operator section). Providing custom operator \u00b6 The operator parameter also supports functions ( i.e. Callable ), this is considered a custom operator and in this case, you should be aware of the following points: An assertion will be made to ensure it respects operators' signature . If you use any white-box explainer, your operator will go through the get_gradient_of_operator function below. Code of the get_gradient_of_operator function. def get_gradient_of_operator ( operator ): \"\"\" Get the gradient of an operator. Parameters ---------- operator Operator of which to compute the gradient. Returns ------- gradient Gradient of the operator. \"\"\" @tf . function def gradient ( model , inputs , targets ): with tf . GradientTape () as tape : tape . watch ( inputs ) scores = operator ( model , inputs , targets ) return tape . gradient ( scores , inputs ) return gradient Tip Writing your operator with only tensorflow functions should increase your chance that this method does not yield any errors. In addition, providing a @tf.function decorator is also welcome! Warning The targets parameter is the key to specifying what to explain and differs greatly depending on the operator. Models not respecting the specifications \u00b6 Warning In any case, when you are out of the scope of the original API, you should take a deep look at the source code to be sure that your Use Case will make sense. My inputs follow a different shape convention \u00b6 In the case where you want to handle images or time series data that does not follow the previous conventions, it is recommended to reshape the data to the expected shape for the explainers (attribution methods) to handle them correctly. Then, you can simply define a wrapper of your model so that data is reshape to your model convenience when it is called. For example, if you have a model that classifies images but want the images to be channel-first ( i.e. with \\((N, C, H, W)\\) shape) then you should: Move the axis so inputs are \\((N, H, W, C)\\) for the explainers Write the following wrapper for your model: Example of a wrapper. class ModelWrapper ( tf . keras . models . Model ): def __init__ ( self , nchw_model ): super ( ModelWrapper , self ) . __init__ () self . model = nchw_model def __call__ ( self , nhwc_inputs ): # transform the NHWC inputs (wanted for the explainers) back to NCHW inputs nchw_inputs = self . _transform_inputs ( nhwc_inputs ) # make predictions outputs = self . nchw_model ( nchw_inputs ) return outputs def _transform_inputs ( self , nhwc_inputs ): # include in this function all transformation # needed for your model to work with NHWC inputs # , here for example we move axis from channels last # to channels first nchw_inputs = np . moveaxis ( nhwc_inputs , [ 3 , 1 , 2 ], [ 1 , 2 , 3 ]) return nchw_inputs wrapped_model = ModelWrapper ( model ) explainer = Saliency ( wrapped_model ) # images should be (N, H, W, C) for the explain call explanations = explainer . explain ( images , labels ) I have a PyTorch model \u00b6 Then you should definitely take a look at the PyTorch documentation ! I have a model that is neither a tf.keras.Model nor a torch.nn.Module \u00b6 Then you should take a look at the Callable documentation or you could take inspiration on the PyTorch Wrapper to write a wrapper that will integrate your model into our API! inputs and data types \u00b6 Warning inputs in this section correspond to the argument in the explain method of BlackBoxExplainer . The model specified at the initialization of the BlackBoxExplainer should be able to be called through model(inputs) . Otherwise, a wrapper needs to be implemented as described in the Models not respecting the specifications section . inputs : Must be one of the following: a tf.data.Dataset (in which case you should not provide targets), a tf.Tensor or a np.ndarray . Examples are provided in the different tutorials: images , time-series , and tabular data . The conventions are as follow: If inputs are images, the expected shape of inputs is \\((N, H, W, C)\\) following the TF's conventions where: \\(N\\) : the number of inputs \\(H\\) : the height of the images \\(W\\) : the width of the images \\(C\\) : the number of channels (works for \\(C=3\\) or \\(C=1\\) , other values might not work or need further customization) If inputs are time-series, the expected shape of inputs is \\((N, T, W)\\) \\(N\\) : the number of inputs \\(T\\) : the temporal dimension of a single input \\(W\\) : the feature dimension of a single input If inputs are tabular data, the expected shape of inputs is \\((N, W)\\) where: \\(N\\) : the number of inputs \\(W\\) : the feature dimension of a single input Tip Please refer to the table of attributions available to see which methods might work with for the different data types. Note If your model is not following the same conventions, please refer to the model not respecting the specification documentation . targets \u00b6 targets : Must be one of the following: a tf.Tensor or a np.ndarray . It has a shape of \\((N, ...)\\) where N should match the first dimension of inputs , while \\(...\\) depend on the task and operators. Indeed, the targets parameter is highly dependent on the operator selected for the attribution methods, hence, for more information please refer to the tasks and operators table which will lead you to the pertinent task documentation page.","title":"API Description"},{"location":"api/attributions/api_attributions/#api-attributions-methods","text":"Attribution Methods : Getting started","title":"API: Attributions Methods"},{"location":"api/attributions/api_attributions/#context","text":"In 2013, Simonyan et al. proposed a first attribution method, opening the way to a wide range of approaches which could be defined as follow: Definition The main objective in attributions techniques is to highlight the discriminating variables for decision-making. For instance, with Computer Vision (CV) tasks, the main goal is to underline the pixels contributing the most in the input image(s) leading to the model\u2019s output(s).","title":"Context"},{"location":"api/attributions/api_attributions/#common-api","text":"explainer = Method ( model , batch_size , operator ) explanation = explainer ( inputs , targets ) The API have two steps: explainer instantiation : Method is an attribution method among those displayed methods tables . It inherits from the Base class BlackBoxExplainer . Their initialization takes 3 parameters apart from the specific ones and generates an explainer : model : the model from which we want to obtain attributions (e.g: InceptionV3, ResNet, ...), see the model section for more details and specifications. batch_size : an integer which allows to either process inputs per batch (gradient-based methods) or process perturbed samples of an input per batch (inputs are therefore processed one by one). operator : enum identifying the task of the model (which is Classification by default), string identifying the task, or function to explain, see the task and operator section for more detail. explainer call : The call to explainer generates the explanations, it takes two parameters: inputs : the samples on which the explanations are requested, see inputs section for more detail. targets : another parameter to specify what to explain in the inputs , it changes depending on the operator , see targets section for more detail. Info The __call__ method of explainers is an alias for the explain method. Info This documentation page covers the different parameters of the common API of attributions methods. It is common between the different tasks covered by Xplique for attribution methods.","title":"Common API"},{"location":"api/attributions/api_attributions/#methods","text":"Even though we made an harmonized API for all attributions methods, it might be relevant for the user to distinguish Perturbation-based methods and Gradient-based methods , also often referenced respectively as black-box and white-box methods, as their hyperparameters settings might be quite different.","title":"Methods"},{"location":"api/attributions/api_attributions/#model","text":"model is the primary parameter of attribution methods: it represents model from which explanations are required. Even though we tried to support a wide-range of models, our attributions framework relies on some assumptions which we propose to see in this section. Warning In case the model does not respect the specifications, a wrapper will be needed as described in the Models not respecting the specifications section . In practice, we expect the model to be callable for the inputs parameters -- i.e. we can do model(inputs) . We expect this call to produce the outputs variables that are the predictions of the model on those inputs. As for most attribution methods, we need to manipulate and/or link the outputs to the inputs . We assume that the latter follow conventional shapes described in the inputs section . Info Depending on the task and operator there may be supplementary specifications for the model, mainly on the output of the model.","title":"model"},{"location":"api/attributions/api_attributions/#tasks-and-operator","text":"operator is one of the main parameters for both attribution methods and metrics . It defines the function that we want to explain. E.g. : In the case we have a classifier model, the function that we might want to explain is the one that given a target provides us the score of the model for that specific target -- i.e \\(model(input)[target]\\) . Note The operator parameter is a feature available for version > \\(1.\\) . The operator default values are the ones used before the introduction of this new feature!","title":"Tasks and operator"},{"location":"api/attributions/api_attributions/#models-not-respecting-the-specifications","text":"Warning In any case, when you are out of the scope of the original API, you should take a deep look at the source code to be sure that your Use Case will make sense.","title":"Models not respecting the specifications"},{"location":"api/attributions/api_attributions/#inputs-and-data-types","text":"Warning inputs in this section correspond to the argument in the explain method of BlackBoxExplainer . The model specified at the initialization of the BlackBoxExplainer should be able to be called through model(inputs) . Otherwise, a wrapper needs to be implemented as described in the Models not respecting the specifications section . inputs : Must be one of the following: a tf.data.Dataset (in which case you should not provide targets), a tf.Tensor or a np.ndarray . Examples are provided in the different tutorials: images , time-series , and tabular data . The conventions are as follow: If inputs are images, the expected shape of inputs is \\((N, H, W, C)\\) following the TF's conventions where: \\(N\\) : the number of inputs \\(H\\) : the height of the images \\(W\\) : the width of the images \\(C\\) : the number of channels (works for \\(C=3\\) or \\(C=1\\) , other values might not work or need further customization) If inputs are time-series, the expected shape of inputs is \\((N, T, W)\\) \\(N\\) : the number of inputs \\(T\\) : the temporal dimension of a single input \\(W\\) : the feature dimension of a single input If inputs are tabular data, the expected shape of inputs is \\((N, W)\\) where: \\(N\\) : the number of inputs \\(W\\) : the feature dimension of a single input Tip Please refer to the table of attributions available to see which methods might work with for the different data types. Note If your model is not following the same conventions, please refer to the model not respecting the specification documentation .","title":"inputs and data types"},{"location":"api/attributions/api_attributions/#targets","text":"targets : Must be one of the following: a tf.Tensor or a np.ndarray . It has a shape of \\((N, ...)\\) where N should match the first dimension of inputs , while \\(...\\) depend on the task and operators. Indeed, the targets parameter is highly dependent on the operator selected for the attribution methods, hence, for more information please refer to the tasks and operators table which will lead you to the pertinent task documentation page.","title":"targets"},{"location":"api/attributions/callable/","text":"\ud83d\udcde Callable or Models handle by BlackBox Attribution methods \u00b6 The model can be something else than a tf.keras.Model if it respects one of the following condition: model(inputs: np.ndarray) return either a np.ndarray or a tf.Tensor The model has a scikit-learn API and has a predict_proba function The model is a xgboost.XGBModel from the XGBoost python library The model is a TF Lite model . Note this feature is experimental. The model is a PyTorch model (see the dedicated documentation ) In fact, what happens when a custom operator is not provided (see operator's documentation ) and model (see model's documentation ) is not a tf.keras.Model , a tf.Module or a tf.keras.layers.Layer is that the predictions_one_hot_callable operator is used: def predictions_one_hot_callable ( model : Callable , inputs : tf . Tensor , targets : tf . Tensor ) -> tf . Tensor : \"\"\" Compute predictions scores, only for the label class, for a batch of samples. Parameters ---------- model Model used for computing predictions. inputs Input samples to be explained. targets One-hot encoded labels or regression target (e.g {+1, -1}), one for each sample. Returns ------- scores Predictions scores computed, only for the label class. \"\"\" if isinstance ( model , tf . lite . Interpreter ): model . resize_tensor_input ( 0 , [ * inputs . shape ], strict = False ) model . allocate_tensors () model . set_tensor ( model . get_input_details ()[ 0 ][ \"index\" ], inputs ) model . invoke () pred = model . get_tensor ( model . get_output_details ()[ 0 ][ \"index\" ]) # can be a sklearn model or xgboost model elif hasattr ( model , 'predict_proba' ): pred = model . predict_proba ( inputs . numpy ()) # can be another model thus it needs to implement a call function else : pred = model ( inputs . numpy ()) # make sure that the prediction shape is coherent if inputs . shape [ 0 ] != 1 : # a batch of prediction is required if len ( pred . shape ) == 1 : # The prediction dimension disappeared pred = tf . expand_dims ( pred , axis = 1 ) pred = tf . cast ( pred , dtype = tf . float32 ) scores = tf . reduce_sum ( pred * targets , axis =- 1 ) return scores Knowing that, you are free to wrap your model to make it work with our API and/or write a more customizable operator (see operator's documentation )!","title":"Callable"},{"location":"api/attributions/callable/#callable-or-models-handle-by-blackbox-attribution-methods","text":"The model can be something else than a tf.keras.Model if it respects one of the following condition: model(inputs: np.ndarray) return either a np.ndarray or a tf.Tensor The model has a scikit-learn API and has a predict_proba function The model is a xgboost.XGBModel from the XGBoost python library The model is a TF Lite model . Note this feature is experimental. The model is a PyTorch model (see the dedicated documentation ) In fact, what happens when a custom operator is not provided (see operator's documentation ) and model (see model's documentation ) is not a tf.keras.Model , a tf.Module or a tf.keras.layers.Layer is that the predictions_one_hot_callable operator is used: def predictions_one_hot_callable ( model : Callable , inputs : tf . Tensor , targets : tf . Tensor ) -> tf . Tensor : \"\"\" Compute predictions scores, only for the label class, for a batch of samples. Parameters ---------- model Model used for computing predictions. inputs Input samples to be explained. targets One-hot encoded labels or regression target (e.g {+1, -1}), one for each sample. Returns ------- scores Predictions scores computed, only for the label class. \"\"\" if isinstance ( model , tf . lite . Interpreter ): model . resize_tensor_input ( 0 , [ * inputs . shape ], strict = False ) model . allocate_tensors () model . set_tensor ( model . get_input_details ()[ 0 ][ \"index\" ], inputs ) model . invoke () pred = model . get_tensor ( model . get_output_details ()[ 0 ][ \"index\" ]) # can be a sklearn model or xgboost model elif hasattr ( model , 'predict_proba' ): pred = model . predict_proba ( inputs . numpy ()) # can be another model thus it needs to implement a call function else : pred = model ( inputs . numpy ()) # make sure that the prediction shape is coherent if inputs . shape [ 0 ] != 1 : # a batch of prediction is required if len ( pred . shape ) == 1 : # The prediction dimension disappeared pred = tf . expand_dims ( pred , axis = 1 ) pred = tf . cast ( pred , dtype = tf . float32 ) scores = tf . reduce_sum ( pred * targets , axis =- 1 ) return scores Knowing that, you are free to wrap your model to make it work with our API and/or write a more customizable operator (see operator's documentation )!","title":"\ud83d\udcde Callable or Models handle by BlackBox Attribution methods"},{"location":"api/attributions/classification/","text":"Classification explanations with Xplique \u00b6 Attributions: Getting started tutorial Which kind of tasks are supported by Xplique? \u00b6 With the operator's api you can treat many different problems with Xplique. There is one operator for each task. Task and Documentation link operator parameter value from xplique.Tasks Enum Tutorial link Classification CLASSIFICATION Object Detection OBJECT_DETECTION Regression REGRESSION Semantic Segmentation SEMANTIC_SEGMENTATION Info They all share the API for Xplique attribution methods . Simple example \u00b6 import xplique from xplique.attributions import Saliency from xplique.metrics import Deletion # load inputs and model # ... # for classification it is recommended to remove softmax layer if there is one # model.layers[-1].activation = tf.keras.activations.linear # for classification, `targets` are the one hot encoding of the predicted class targets = tf . one_hot ( tf . argmax ( model ( inputs ), axis =- 1 ), depth = nb_classes , axis =- 1 ) # compute explanations by specifying the classification operator explainer = Saliency ( model , operator = xplique . Tasks . CLASSIFICATION ) explanations = explainer ( inputs , targets ) # compute metrics on those explanations # if the softmax was removed, # it is possible to specify it to obtain more interpretable metrics metric = Deletion ( model , inputs , targets , operator = xplique . Tasks . CLASSIFICATION , activation = \"softmax\" ) score_saliency = metric ( explanations ) Tip In general, if you are doing classification tasks, it is better to not include the final softmax layer in your model but to work with logits instead! How to use it? \u00b6 To apply attribution methods, the common API documentation describes the parameters and how to fix them. However, depending on the task and thus on the operator , there are three points that vary: The operator parameter value, it is an Enum or a string identifying the task, The model's output specification, as model(inputs) is used in the computation of the operators, and The targets parameter format, indeed, the targets parameter specifies what to explain and the format of such specification depends on the task. The operator \u00b6 How to specify it \u00b6 In Xplique, to adapt attribution methods, you should specify the task to the operator parameter. In the case of classification, with either: Method ( model ) # or Method ( model , operator = \"classification\" ) # or Method ( model , operator = xplique . Tasks . CLASSIFICATION ) Info Classification if the default behavior of Xplique attribution methods, hence there is no need to specify it. Nonetheless, it is recommended to still do so to ensure a good comprehension of what is explained. The computation \u00b6 The classification operator multiplies model's predictions on inputs with targets and sum it for each input to explain. However, only one value should be non-zero in targets , thus, the classification operator returns the model output for the specified (via targets ) class. scores = tf . reduce_sum ( model ( inputs ) * targets , axis =- 1 ) The behavior \u00b6 In the case of perturbation-based methods , the perturbation score corresponds to the difference between the initial logits value for the predicted classes and the same logits for predictions over perturbed inputs. For gradient-based methods , the gradient of logits of interest with respect to the inputs. The logits of interest are specified via the targets parameter described in the related section . Model's output \u00b6 We expect model(inputs) to yield a \\((n, c)\\) tensor or array where \\(n\\) is the number of input samples and \\(c\\) is the number of classes. The targets parameter \u00b6 Role \u00b6 The targets parameter specifies what is to explain in the inputs , it is passed to the explain or to the __call__ method of an explainer or metric and used by the operators. In the case of classification, it indicates the class to explain, or specifies contrastive explanations . Format \u00b6 The targets parameter in the case of classification should have the same shape as the model's output as they are multiplied point-wise. Hence, the shape is \\((n, c)\\) with \\(n\\) the number of samples to be explained (it should match the first dimension of inputs ) and \\(c\\) the number of classes. The targets parameter expects values among \\({-1, 0, 1}\\) but most values should be \\(0\\) and most of the time only one should be \\(1\\) for each sample. \\(-1\\) are only used for contrastive explanations . In practice \u00b6 In the simple example , the targets value provided is computed with tf.one_hot(tf.argmax(model(inputs), axis=-1), axis=-1) . Literally, the one hot encoding of the predicted class, this specifies which class to explain. Tip It is better to explain the predicted class than the expected class as the goal is to explain the model's prediction. What can be explained with it? \u00b6 Explain the predicted class \u00b6 By specifying targets with a one hot encoding of the predicted class, the explanation will highlight which features were important for this prediction. Contrastive explanations \u00b6 By specifying targets with zeros everywhere, 1 for the first class, and -1 for the second class. The explanation will show which features were important to predict the first and and not the second one. Tip If the model made a mistake, an interesting explanation is predicted class versus expected class.","title":"Classification"},{"location":"api/attributions/classification/#classification-explanations-with-xplique","text":"Attributions: Getting started tutorial","title":"Classification explanations with Xplique"},{"location":"api/attributions/classification/#which-kind-of-tasks-are-supported-by-xplique","text":"With the operator's api you can treat many different problems with Xplique. There is one operator for each task. Task and Documentation link operator parameter value from xplique.Tasks Enum Tutorial link Classification CLASSIFICATION Object Detection OBJECT_DETECTION Regression REGRESSION Semantic Segmentation SEMANTIC_SEGMENTATION Info They all share the API for Xplique attribution methods .","title":"Which kind of tasks are supported by Xplique?"},{"location":"api/attributions/classification/#simple-example","text":"import xplique from xplique.attributions import Saliency from xplique.metrics import Deletion # load inputs and model # ... # for classification it is recommended to remove softmax layer if there is one # model.layers[-1].activation = tf.keras.activations.linear # for classification, `targets` are the one hot encoding of the predicted class targets = tf . one_hot ( tf . argmax ( model ( inputs ), axis =- 1 ), depth = nb_classes , axis =- 1 ) # compute explanations by specifying the classification operator explainer = Saliency ( model , operator = xplique . Tasks . CLASSIFICATION ) explanations = explainer ( inputs , targets ) # compute metrics on those explanations # if the softmax was removed, # it is possible to specify it to obtain more interpretable metrics metric = Deletion ( model , inputs , targets , operator = xplique . Tasks . CLASSIFICATION , activation = \"softmax\" ) score_saliency = metric ( explanations ) Tip In general, if you are doing classification tasks, it is better to not include the final softmax layer in your model but to work with logits instead!","title":"Simple example"},{"location":"api/attributions/classification/#how-to-use-it","text":"To apply attribution methods, the common API documentation describes the parameters and how to fix them. However, depending on the task and thus on the operator , there are three points that vary: The operator parameter value, it is an Enum or a string identifying the task, The model's output specification, as model(inputs) is used in the computation of the operators, and The targets parameter format, indeed, the targets parameter specifies what to explain and the format of such specification depends on the task.","title":"How to use it?"},{"location":"api/attributions/classification/#the-operator","text":"","title":"The operator"},{"location":"api/attributions/classification/#models-output","text":"We expect model(inputs) to yield a \\((n, c)\\) tensor or array where \\(n\\) is the number of input samples and \\(c\\) is the number of classes.","title":"Model's output"},{"location":"api/attributions/classification/#the-targets-parameter","text":"","title":"The targets parameter"},{"location":"api/attributions/classification/#what-can-be-explained-with-it","text":"","title":"What can be explained with it?"},{"location":"api/attributions/object_detection/","text":"Object detection with Xplique \u00b6 Attributions: Object Detection tutorial Which kind of tasks are supported by Xplique? \u00b6 With the operator's api you can treat many different problems with Xplique. There is one operator for each task. Task and Documentation link operator parameter value from xplique.Tasks Enum Tutorial link Classification CLASSIFICATION Object Detection OBJECT_DETECTION Regression REGRESSION Semantic Segmentation SEMANTIC_SEGMENTATION Info They all share the API for Xplique attribution methods . Simple example \u00b6 import xplique from xplique.attributions import Saliency from xplique.metrics import Deletion # load images and model # ... predictions = model ( images ) explainer = Saliency ( model , operator = xplique . Tasks . OBJECT_DETECTION ) # explain each image - bounding-box pair separately for all_bbx_for_one_image , image in zip ( predictions , images ): # an image is needed per bounding box, so we tile them repeated_image = tf . tile ( tf . expand_dims ( image , axis = 0 ), ( tf . shape ( all_bbx_for_one_image )[ 0 ], 1 , 1 , 1 )) explanations = explainer ( repeated_image , all_bbx_for_one_image ) # either compute several score or # concatenate repeated images and corresponding boxes in one tensor metric_for_one_image = Deletion ( model , repeated_image , all_bbx_for_one_image , operator = xplique . Tasks . OBJECT_DETECTION ) score_saliency = metric ( explanations ) How to use it? \u00b6 To apply attribution methods, the common API documentation describes the parameters and how to fix them. However, depending on the task and thus on the operator , there are three points that vary: The operator parameter value, it is an Enum or a string identifying the task, The model's output specification, as model(inputs) is used in the computation of the operators, and The targets parameter format, indeed, the targets parameter specifies what to explain and the format of such specification depends on the task. The operator \u00b6 How to specify it \u00b6 In Xplique, to adapt attribution methods, you should specify the task to the operator parameter. In the case of object detection, with either: Method ( model , operator = \"object detection\" ) # or Method ( model , operator = xplique . Tasks . OBJECT_DETECTION ) Info There are several variants of the object detection operator to explain part of the prediction. The computation \u00b6 This operator is a generalization of DRise method introduced by Petsiuk & al. [^1] to most attribution methods. The computation is the same as the one described in the DRise paper. The DRise can be divided into two principles: The matching : DRise extends Rise (described in detail in the Rise tutorial ) to explain object detection. Rise is a perturbation-based method, hence current predictions are compared to predictions on perturbed inputs. However, object detectors predict several boxes with no consistency in the order, thus DRise chooses to match the current bounding box to the most similar one and use the similarity metric as the perturbation score. The similarity metric : This is the score used by DRise to match bounding boxes. It uses the three parts of a bounding box prediction, the position of the box, the box objectness, and the associated class. A score is computed for each of those three parts and these scores are multiplied: \\[ score = intersection\\_score * detection\\_probability * classification\\_score \\] With: $$ intersection_score = IOU(coordinates_{ref}, coordinates_{pred}) $$ \\[ detection\\_probability = objectness_{pred} \\] \\[ classification\\_score = \\frac{\\sum(classes_{ref} * classes_{pred})}{||classes_{ref}|| * ||classes_{pred}||} \\] Info The intersection score of the operator is the IOU (Intersection Over Union) by default but can be modified by specifying as custom intersection score . Info With the DRise formula the methods explain the box position, the box objectness, and the class prediction at the same time. However, the user may want to explain them separately, therefore several variants of this operator are available in Xplique and described in What can we explain and how? section . The behavior \u00b6 In the case of perturbation-based methods , the perturbation score is the similarity metric aforementioned. For gradient-based methods , the gradient of the similarity metric is given, but no matching is necessary as no perturbation is made. Model's output \u00b6 We expect model(inputs) to yield a \\((n, nb\\_boxes, 4 + 1 + nb\\_classes)\\) tensors or array where: \\(n\\) : the number of inputs, it should match the first dimension of inputs . \\(nb\\_boxes\\) : a fixed number of bounding boxes predicted for a given image (no NMS). \\((4 + 1 + nb\\_classes)\\) : the encoding of a bounding box prediction \\(4\\) : the bounding box coordinates \\((x_{top\\_left}, y_{top\\_left}, x_{bottom\\_right}, y_{bottom\\_right})\\) , with \\(x_{top\\_left} < x_{bottom\\_right}\\) and \\(y_{top\\_left} < y_{bottom\\_right}\\) . \\(1\\) : the objectness or detection probability of the bounding box, \\(nb\\_classes\\) : the class of the bounding box, a soft class predictions not a one-hot encoding. Warning Object detection models provided to the explainer should not include NMS and classification should be soft classification not one-hot encoding. Furthermore, if the model does not match the expected format, a wrapper may be needed. (see the tutorial for an example). Info PyTorch models are not natively treated by Xplique, however, a simple wrapper is available in pytorch documentation . The targets parameter \u00b6 Role \u00b6 The targets parameter specifies what is to explain in the inputs , it is passed to the explain or to the __call__ method of an explainer or metric and used by the operators. In the case of object detection, it indicates which box to explain, furthermore, it gives the initial predictions to the operator as the reference for perturbation-based methods. Format \u00b6 The targets parameter in the case of semantic segmentation should have the same shape as the model's output as the same computation are made. Concretely, the targets parameter should have a shape of \\((n, 4 + 1 + nb\\_classes)\\) to explain a bounding box for each input (detail in model's output description ). Additionally, there is a possibility to explain a group of bounding boxes at the same time described in the explaining several bounding boxes section which requires a different shape. In practice \u00b6 To explain each bounding box individually, the images need to be repeated. Indeed, object detector predict several bounding boxes per image and the first dimension of inputs and targets should match as it corresponds to the sample dimension. Therefore, the easiest way to obtain this is for each image to repeat it so that it matches the number of bounding boxes to explain for this image. In the simple example , there is a loop on the images - predictions pair, then images are repeated to match the number of predicted bounding boxes, and finally, the targets parameter takes the predicted bounding boxes. Tip AS specified in the model's output specification , the NMS (Non Maximum Suppression) should not be included in the model. However, it can be used to select the bounding boxes to explain. Warning Repeating images may create a tensor that exceeds memory for large images and/or when many bounding boxes are to be explained. In this case, we advise to make a loop on the images, then a loop on the boxes. Explain several bounding boxes simultaneously \u00b6 The user may not want to explain each bounding box individually but several bounding boxes at the same time ( i.e a set of pedestrian bounding boxes on a sidewalk). In this case, the targets parameter shape will not be \\((n, 4 + 1 + nb\\_classes)\\) but \\((n, nb\\_boxes, 4 + 1 + nb\\_classes)\\) , with \\(nb\\_boxes\\) the number of boxes to explain simultaneously. In this case, \\(nb\\_boxes\\) bounding boxes are associated to each sample and a single attribution map is returned. However, for different images, \\(nb\\_boxes\\) may not be fix and it may not be possible to make a single tensor in this case. Thus, we recommend to treat each group of bounding boxes with a different call to the attribution method with \\(n=1\\) . To return one explanation for several bounding boxes, Xplique takes the mean of the bounding boxes individual explanations and returns it. For a concrete example, please refer to the Attributions: Object detection tutorial. What can be explained and how? \u00b6 The different elements in object detection \u00b6 In object detection, the prediction for a given bounding box include several pieces of information: The box position , the box probability of containing something , and the class of the detected object . Therefore we may want to explain each of them separately, however, the DRise method of matching bounding boxes should be kept in mind. Indeed, the box position cannot be removed from the score, otherwise, the explanation may not correspond to the same object. The different operator's variants and what they explain \u00b6 The Xplique library allows the specification of which part of the prediction to explain via a set 4 operators: the one as defined by the DRise formula and three variants: \"object detection\" : the one described in the operator section : \\[score = intersection\\_score * detection\\_probability * classification\\_score\\] \"object detection box position\" : explains only the bounding box position: \\[score = intersection\\_score\\] \"object detection box proba\" : explains the probability of a bounding box to contain something: \\[score = intersection\\_score * detection\\_probability\\] \"object detection box class\" : explains the class of a bounding box: \\[score = intersection\\_score * classification\\_score\\] Custom intersection score \u00b6 The default intersection score is IOU, but it is possible to define a custom intersection score. The only constraint is that it should follow xplique.commons.object_detection_operator._box_iou signature for it to work. from xplique.attributions import Saliency from xplique.commons.operators import object_detection_operator custom_intersection_score = ... custom_operator = lambda model , inputs , targets : object_detection_operator ( model , inputs , targets , intersection_score = custom_intersection_score ) explainer = Saliency ( model , operator = custom_operator ) ... # All following steps are the same as the examples [^1] Black-box Explanation of Object Detectors via Saliency Maps (2021)","title":"Object Detection"},{"location":"api/attributions/object_detection/#object-detection-with-xplique","text":"Attributions: Object Detection tutorial","title":"Object detection with Xplique"},{"location":"api/attributions/object_detection/#which-kind-of-tasks-are-supported-by-xplique","text":"With the operator's api you can treat many different problems with Xplique. There is one operator for each task. Task and Documentation link operator parameter value from xplique.Tasks Enum Tutorial link Classification CLASSIFICATION Object Detection OBJECT_DETECTION Regression REGRESSION Semantic Segmentation SEMANTIC_SEGMENTATION Info They all share the API for Xplique attribution methods .","title":"Which kind of tasks are supported by Xplique?"},{"location":"api/attributions/object_detection/#simple-example","text":"import xplique from xplique.attributions import Saliency from xplique.metrics import Deletion # load images and model # ... predictions = model ( images ) explainer = Saliency ( model , operator = xplique . Tasks . OBJECT_DETECTION ) # explain each image - bounding-box pair separately for all_bbx_for_one_image , image in zip ( predictions , images ): # an image is needed per bounding box, so we tile them repeated_image = tf . tile ( tf . expand_dims ( image , axis = 0 ), ( tf . shape ( all_bbx_for_one_image )[ 0 ], 1 , 1 , 1 )) explanations = explainer ( repeated_image , all_bbx_for_one_image ) # either compute several score or # concatenate repeated images and corresponding boxes in one tensor metric_for_one_image = Deletion ( model , repeated_image , all_bbx_for_one_image , operator = xplique . Tasks . OBJECT_DETECTION ) score_saliency = metric ( explanations )","title":"Simple example"},{"location":"api/attributions/object_detection/#how-to-use-it","text":"To apply attribution methods, the common API documentation describes the parameters and how to fix them. However, depending on the task and thus on the operator , there are three points that vary: The operator parameter value, it is an Enum or a string identifying the task, The model's output specification, as model(inputs) is used in the computation of the operators, and The targets parameter format, indeed, the targets parameter specifies what to explain and the format of such specification depends on the task.","title":"How to use it?"},{"location":"api/attributions/object_detection/#the-operator","text":"","title":"The operator"},{"location":"api/attributions/object_detection/#models-output","text":"We expect model(inputs) to yield a \\((n, nb\\_boxes, 4 + 1 + nb\\_classes)\\) tensors or array where: \\(n\\) : the number of inputs, it should match the first dimension of inputs . \\(nb\\_boxes\\) : a fixed number of bounding boxes predicted for a given image (no NMS). \\((4 + 1 + nb\\_classes)\\) : the encoding of a bounding box prediction \\(4\\) : the bounding box coordinates \\((x_{top\\_left}, y_{top\\_left}, x_{bottom\\_right}, y_{bottom\\_right})\\) , with \\(x_{top\\_left} < x_{bottom\\_right}\\) and \\(y_{top\\_left} < y_{bottom\\_right}\\) . \\(1\\) : the objectness or detection probability of the bounding box, \\(nb\\_classes\\) : the class of the bounding box, a soft class predictions not a one-hot encoding. Warning Object detection models provided to the explainer should not include NMS and classification should be soft classification not one-hot encoding. Furthermore, if the model does not match the expected format, a wrapper may be needed. (see the tutorial for an example). Info PyTorch models are not natively treated by Xplique, however, a simple wrapper is available in pytorch documentation .","title":"Model's output"},{"location":"api/attributions/object_detection/#the-targets-parameter","text":"","title":"The targets parameter"},{"location":"api/attributions/object_detection/#what-can-be-explained-and-how","text":"","title":"What can be explained and how?"},{"location":"api/attributions/object_detection/#custom-intersection-score","text":"The default intersection score is IOU, but it is possible to define a custom intersection score. The only constraint is that it should follow xplique.commons.object_detection_operator._box_iou signature for it to work. from xplique.attributions import Saliency from xplique.commons.operators import object_detection_operator custom_intersection_score = ... custom_operator = lambda model , inputs , targets : object_detection_operator ( model , inputs , targets , intersection_score = custom_intersection_score ) explainer = Saliency ( model , operator = custom_operator ) ... # All following steps are the same as the examples [^1] Black-box Explanation of Object Detectors via Saliency Maps (2021)","title":"Custom intersection score"},{"location":"api/attributions/pytorch/","text":"PyTorch models with Xplique \u00b6 PyTorch models : Getting started Metrics : With PyTorch models Other tutorials applying Xplique to PyTorch models: Attributions: Object Detection , Attributions: Semantic Segmentation Note We should point out that what we did with PyTorch should be possible for other frameworks. Do not hesitate to give it a try and to make a PR if you have been successful! Is it possible to use Xplique with PyTorch models? \u00b6 Yes , it is! Even though the library was mainly designed to be a Tensorflow toolbox we have been working on a very practical wrapper to facilitate the integration of your PyTorch models into Xplique's framework! Quickstart \u00b6 import torch from xplique.wrappers import TorchWrapper from xplique.attributions import Saliency from xplique.metrics import Deletion # load images, targets and model # ... device = 'cuda' if torch . cuda . is_available () else 'cpu' wrapped_model = TorchWrapper ( torch_model , device ) explainer = Saliency ( wrapped_model , operator = \"classification\" ) explanations = explainer ( inputs , targets ) metric = Deletion ( wrapped_model , inputs , targets , operator = \"classification\" ) score_saliency = metric ( explanations ) Does it work for every module? \u00b6 It has been tested on both the attributions and the metrics modules. Does it work for all attribution methods? \u00b6 Not yet, but it works for most of them (even for gradient-based ones!): Attribution Method PyTorch compatible Deconvolution \u274c Grad-CAM \u274c Grad-CAM++ \u274c Gradient Input \u2705 Guided Backprop \u274c Hsic Attribution \u2705 Integrated Gradients \u2705 Kernel SHAP \u2705 Lime \u2705 Occlusion \u2705 Rise \u2705 Saliency \u2705 SmoothGrad \u2705 Sobol Attribution \u2705 SquareGrad \u2705 VarGrad \u2705 Does it work for all tasks? \u00b6 It works for all tasks covered by Xplique, see the tasks covered and how to specify them . Steps to make Xplique work on PyTorch \u00b6 1. Make sure the inputs follow the Xplique API (and not what the model expects). \u00b6 One thing to keep in mind is that attribution methods expect a specific inputs format as described in the API Description . Especially, for images inputs should be \\((N, H, W, C)\\) following the TF's conventions where: \\(N\\) is the number of inputs \\(H\\) is the height of the images \\(W\\) is the width of the images \\(C\\) is the number of channels However, if you are using a PyTorch models it is most likely expecting images' shape to be \\((N, C, H, W)\\) . So what should you do? If you are using PyTorch's preprocessing functions what you should do is: preprocess as usual convert the data to numpy array use np.moveaxis(np_inputs, [1, 2, 3], [3, 1, 2]) to change shape from \\((N, C, H, W)\\) to \\((N, H, W, C)\\) Notes The third step is necessary only if your data has a channel dimension which is not in the place expected with Tensorflow Tip If you want to be sure how this work you can look at the PyTorch models : Getting started notebook and compare it to the Attribution methods :Getting Started 2. Wrap your model \u00b6 A TorchWrapper object can be initialized with 3 parameters: torch_model: torch.nn.Module : A torch's model that inherits from nn.Module device: Union['torch.device', str] : The device on which the torch's model and inputs should be mounted is_channel_first: Optional[bool] = None : A boolean that is true if the torch's model expect a channel dim and if this one come first The last parameter is the one that needs special care. Indeed, if it is set to True we assume that the torch model expects its inputs to be \\((N, C, H, W)\\) . As the explainer requires inputs to be \\((N, H, W, C)\\) we change the inputs' axis order when a call is made to the wrapped model (transparently for the user). If it is set to False we do not move the axis at all. By default the wrapper is looking for torch.nn.Conv2d layers in the torch model and consider it is channel first if it finds one and not otherwise. Info It is possible that you used special treatments for your models or that it does not follow typical convention. In that case, we encourage you to have a look at the Source Code to adapt it to your needs. 3. Use this wrapped model as a TF's one \u00b6 What are the limitations? \u00b6 As it was previously mentionned this does not work with: Deconvolution, Grad-CAM, Grad-CAM++ and Guided Backpropagation. Furthermore, when one use any white-box explainers one have the possibility to provide an output_layer parameter. This functionnality will not work with PyTorch models. The user will have to manipulate itself its model! Warning The output_layer parameter does not work for PyTorch models! It is possible that all failure cases were not covered in the tests, in that case please open an issue so the team will work on it!","title":"PyTorch"},{"location":"api/attributions/pytorch/#pytorch-models-with-xplique","text":"PyTorch models : Getting started Metrics : With PyTorch models Other tutorials applying Xplique to PyTorch models: Attributions: Object Detection , Attributions: Semantic Segmentation Note We should point out that what we did with PyTorch should be possible for other frameworks. Do not hesitate to give it a try and to make a PR if you have been successful!","title":"PyTorch models with Xplique"},{"location":"api/attributions/pytorch/#is-it-possible-to-use-xplique-with-pytorch-models","text":"Yes , it is! Even though the library was mainly designed to be a Tensorflow toolbox we have been working on a very practical wrapper to facilitate the integration of your PyTorch models into Xplique's framework!","title":"Is it possible to use Xplique with PyTorch models?"},{"location":"api/attributions/pytorch/#does-it-work-for-every-module","text":"It has been tested on both the attributions and the metrics modules.","title":"Does it work for every module?"},{"location":"api/attributions/pytorch/#does-it-work-for-all-attribution-methods","text":"Not yet, but it works for most of them (even for gradient-based ones!): Attribution Method PyTorch compatible Deconvolution \u274c Grad-CAM \u274c Grad-CAM++ \u274c Gradient Input \u2705 Guided Backprop \u274c Hsic Attribution \u2705 Integrated Gradients \u2705 Kernel SHAP \u2705 Lime \u2705 Occlusion \u2705 Rise \u2705 Saliency \u2705 SmoothGrad \u2705 Sobol Attribution \u2705 SquareGrad \u2705 VarGrad \u2705","title":"Does it work for all attribution methods?"},{"location":"api/attributions/pytorch/#does-it-work-for-all-tasks","text":"It works for all tasks covered by Xplique, see the tasks covered and how to specify them .","title":"Does it work for all tasks?"},{"location":"api/attributions/pytorch/#steps-to-make-xplique-work-on-pytorch","text":"","title":"Steps to make Xplique work on PyTorch"},{"location":"api/attributions/pytorch/#what-are-the-limitations","text":"As it was previously mentionned this does not work with: Deconvolution, Grad-CAM, Grad-CAM++ and Guided Backpropagation. Furthermore, when one use any white-box explainers one have the possibility to provide an output_layer parameter. This functionnality will not work with PyTorch models. The user will have to manipulate itself its model! Warning The output_layer parameter does not work for PyTorch models! It is possible that all failure cases were not covered in the tests, in that case please open an issue so the team will work on it!","title":"What are the limitations?"},{"location":"api/attributions/regression/","text":"Regression explanations with Xplique \u00b6 Attributions: Regression and Tabular data tutorial Which kind of tasks are supported by Xplique? \u00b6 With the operator's api you can treat many different problems with Xplique. There is one operator for each task. Task and Documentation link operator parameter value from xplique.Tasks Enum Tutorial link Classification CLASSIFICATION Object Detection OBJECT_DETECTION Regression REGRESSION Semantic Segmentation SEMANTIC_SEGMENTATION Info They all share the API for Xplique attribution methods . Warning In Xplique, for now with regression, predictions can only be explained output by output. Indeed, explaining several output simultaneously brings new problematic and we are currently working on an operator to solve this. Simple example \u00b6 import xplique from xplique.attributions import Saliency from xplique.metrics import Deletion # load inputs and model # ... # for regression, `targets` indicates the output of interest, here output 3 targets = tf . one_hot ([ 2 ], depth = nb_outputs , axis =- 1 ) # compute explanations by specifying the regression operator explainer = Saliency ( model , operator = xplique . Tasks . REGRESSION ) explanations = explainer ( inputs , targets ) # compute metrics on these explanations metric = Deletion ( model , inputs , targets , operator = xplique . Tasks . REGRESSION ) score_saliency = metric ( explanations ) How to use it? \u00b6 To apply attribution methods, the common API documentation describes the parameters and how to fix them. However, depending on the task and thus on the operator , there are three points that vary: The operator parameter value, it is an Enum or a string identifying the task, The model's output specification, as model(inputs) is used in the computation of the operators, and The targets parameter format, indeed, the targets parameter specifies what to explain and the format of such specification depends on the task. The operator \u00b6 How to specify it \u00b6 In Xplique, to adapt attribution methods, you should specify the task to the operator parameter. In the case of regression, with either: Method ( model , operator = \"regression\" ) # or Method ( model , operator = xplique . Tasks . REGRESSION ) The computation \u00b6 The regression operator works similarly to the classification operator, it asks for the output of interest via targets and returns this output. See targets section for more detail. scores = tf . reduce_sum ( model ( inputs ) * targets , axis =- 1 ) The behavior \u00b6 In the case of perturbation-based methods , the perturbation score corresponds to the difference between the initial value of the output of interest and the same output for predictions over perturbed inputs. For gradient-based methods , the gradient of the model's predictions for the output of interest. Model's output \u00b6 We expect model(inputs) to yield a \\((n, d)\\) tensor or array where \\(n\\) is the number of input samples and \\(d\\) is the number of variables the model should predict (possibly one). The targets parameter \u00b6 Role \u00b6 The targets parameter specifies what is to explain in the inputs , it is passed to the explain or to the __call__ method of an explainer or metric and used by the operators. In the case of regression it indicates which of the output should be explained. Format \u00b6 The targets parameter in the case of regression should have the same shape as the model's output as they are multiplied. Hence, the shape is \\((n, d)\\) with \\(n\\) the number of samples to be explained (it should match the first dimension of inputs ) and \\(d\\) is the number of variables (possibly one). In practice \u00b6 In the simple example , the targets value provided is computed with tf.one_hot . Indeed, the regression operator takes as targets the one hot encoding of the index of the output to explain.","title":"Regression"},{"location":"api/attributions/regression/#regression-explanations-with-xplique","text":"Attributions: Regression and Tabular data tutorial","title":"Regression explanations with Xplique"},{"location":"api/attributions/regression/#which-kind-of-tasks-are-supported-by-xplique","text":"With the operator's api you can treat many different problems with Xplique. There is one operator for each task. Task and Documentation link operator parameter value from xplique.Tasks Enum Tutorial link Classification CLASSIFICATION Object Detection OBJECT_DETECTION Regression REGRESSION Semantic Segmentation SEMANTIC_SEGMENTATION Info They all share the API for Xplique attribution methods . Warning In Xplique, for now with regression, predictions can only be explained output by output. Indeed, explaining several output simultaneously brings new problematic and we are currently working on an operator to solve this.","title":"Which kind of tasks are supported by Xplique?"},{"location":"api/attributions/regression/#simple-example","text":"import xplique from xplique.attributions import Saliency from xplique.metrics import Deletion # load inputs and model # ... # for regression, `targets` indicates the output of interest, here output 3 targets = tf . one_hot ([ 2 ], depth = nb_outputs , axis =- 1 ) # compute explanations by specifying the regression operator explainer = Saliency ( model , operator = xplique . Tasks . REGRESSION ) explanations = explainer ( inputs , targets ) # compute metrics on these explanations metric = Deletion ( model , inputs , targets , operator = xplique . Tasks . REGRESSION ) score_saliency = metric ( explanations )","title":"Simple example"},{"location":"api/attributions/regression/#how-to-use-it","text":"To apply attribution methods, the common API documentation describes the parameters and how to fix them. However, depending on the task and thus on the operator , there are three points that vary: The operator parameter value, it is an Enum or a string identifying the task, The model's output specification, as model(inputs) is used in the computation of the operators, and The targets parameter format, indeed, the targets parameter specifies what to explain and the format of such specification depends on the task.","title":"How to use it?"},{"location":"api/attributions/regression/#the-operator","text":"","title":"The operator"},{"location":"api/attributions/regression/#models-output","text":"We expect model(inputs) to yield a \\((n, d)\\) tensor or array where \\(n\\) is the number of input samples and \\(d\\) is the number of variables the model should predict (possibly one).","title":"Model's output"},{"location":"api/attributions/regression/#the-targets-parameter","text":"","title":"The targets parameter"},{"location":"api/attributions/semantic_segmentation/","text":"Semantic segmentation explanations with Xplique \u00b6 Attributions: Semantic segmentation tutorial Which kind of tasks are supported by Xplique? \u00b6 With the operator's api you can treat many different problems with Xplique. There is one operator for each task. Task and Documentation link operator parameter value from xplique.Tasks Enum Tutorial link Classification CLASSIFICATION Object Detection OBJECT_DETECTION Regression REGRESSION Semantic Segmentation SEMANTIC_SEGMENTATION Info They all share the API for Xplique attribution methods . Simple example \u00b6 import xplique from xplique.utils_functions.segmentation import get_connected_zone from xplique.attributions import Saliency from xplique.metrics import Deletion # load images and model # ... # extract targets individually coordinates_of_object = ( 42 , 42 ) predictions = model ( image ) target = get_connected_zone ( predictions , coordinates_of_object ) inputs = tf . expand_dims ( image , 0 ) targets = tf . expand_dims ( target , 0 ) explainer = Saliency ( model , operator = xplique . Tasks . SEMANTIC_SEGMENTATION ) explanations = explainer ( inputs , targets ) metric = Deletion ( model , inputs , targets , operator = xplique . Tasks . SEMANTIC_SEGMENTATION ) score_saliency = metric ( explanations ) How to use it? \u00b6 To apply attribution methods, the common API documentation describes the parameters and how to fix them. However, depending on the task and thus on the operator , there are three points that vary: The operator parameter value, it is an Enum or a string identifying the task, The model's output specification, as model(inputs) is used in the computation of the operators, and The targets parameter format, indeed, the targets parameter specifies what to explain and the format of such specification depends on the task. Info Applying attribution methods to semantic segmentation with Xplique has a particularity: a set of functions from utils_functions.segmentation are used to define targets and are documented in the a specific section . The operator \u00b6 How to specify it \u00b6 In Xplique, to adapt attribution methods, you should specify the task to the operator parameter. In the case of semantic segmentation, with either: Method ( model , operator = \"semantic segmentation\" ) # or Method ( model , operator = xplique . Tasks . SEMANTIC_SEGMENTATION ) The computation \u00b6 The operator for semantic segmentation is similar to the classification one, but the output is not a class but a matrix of class. The operator should take this position into account, thus it manipulates two elements: The zone of interest : it represents the zone/pixels on which we want the explanation to be made. It could be a single object like a person, a group of objects like trees, a part of an object that has been wrongly classified, or even the border of an object. Note that the concept of object here only makes sense for us as the model only classifies pixels, which is why Xplique includes the segmentation utils function . The class of interest : it represents the channel of the prediction we want to explain. Similarly to classification, we could either want to explain a cat or a dog in the same image. Note that in some case, providing several classes could make sense, see the example of applications with explanations of the borders between two objects . Indeed, the semantic segmentation operator multiplies the model's predictions by the targets, which can be considered a mask. Then the operator divide the sum of the remaining predictions over the size of the mask. In some, the operator take the mean predictions over the zone and class of interest \\[ score = mean_{over\\ the\\ zone\\ and\\ class\\ of\\ interest}(model(inputs)) \\] Note that the two information need to be communicated through the targets parameter . The behavior \u00b6 In the case of perturbation-based methods , the perturbation score is the difference between the operator's output for the studied inputs and the perturbed inputs. Where the operator's output is the mean logits value over the class and zone of interest. For gradient-based methods , the gradient of the mean of model's predictions limited to the zone and class of interest. Model's output \u00b6 We expect model(inputs) to yield a \\((n, h, w, c)\\) tensor or array where: \\(n\\) : the number of inputs, it should match the first dimension of inputs \\(h\\) : the height of the images \\(w\\) : the width of the images \\(c\\) : the number of classes Warning The model's output for each pixel is expected to be a soft output and not the class prediction or a one hot encoding of the class. Otherwise the attribution methods will not be able to compare predictions efficiently. Warning Contrary to classification, here a softmax or comparable last layer is necessary as zeros are interpreted by the operator as non-zone of interest. In this sense, strictly positive values are required. The targets parameter \u00b6 Role \u00b6 The targets parameter specifies what is to explain in the inputs , it is passed to the explain or to the __call__ method of an explainer or metric and used by the operators. In the case of semantic segmentation, the targets parameter enables the communication of the two necessary information for the semantic segmentation operator : The zone of interest : to communicate the zone of interest via the targets parameter, the targets value on pixels that are not in the zone of interest should be set to zero. In this way tf.math.sign(targets) creates a mask of the zone of interest. This operation should be done along the \\(h\\) and \\(w\\) dimensions of targets . The class of interest : similarly to the zone of interest, the class of interest is communicated by setting other classes along the \\(c\\) dimension to zero. Format \u00b6 The targets parameter in the case of semantic segmentation should have the same shape as the model's output as a difference is made between the two. Hence, the shape is \\((n, h, w,c )\\) with: - \\(n\\) is the number of inputs, it should match the first dimension of inputs - \\(h\\) is the height of the images - \\(w\\) is the width of the images - \\(c\\) is the number of classes Then it should take values in \\(\\{-1, 0, 1\\}\\) , \\(1\\) in the zone of interest (zone on the \\(h\\) and \\(w\\) dimension) and \\(0\\) elsewhere. Similarly, values not on the channel corresponding to the class of interest (dimension \\(c\\) ) should be \\(0\\) . In the case of the explanation of a border or with contrastive explanations, \\(-1\\) values might be used. In practice \u00b6 The targets parameter is computed via the xplique.utils_functions.segmentation set of functions. They manipulate model's prediction individually, as explanation requests are different between each image. Please refer to the segmentation utils functions for detail on how to design targets . Tip You should not worry about such specification as the segmentation utils functions will do the work in your stead. Warning The targets parameter for each sample should be defined individually. Then the batch dimension should be added manually or individual values should be stacked. The segmentation utils functions \u00b6 Source The segmentation utils functions are a set a utility functions used to compute the targets parameter values. They should be applied to each image separately as each segmentation is different want the things to explain differs between images. Nonetheless, you could use tf.map_fn to apply the same function to several images. An example of application of those functions can be found in the Attribution: Semantic segmentation tutorial. For now, there are four functions: get_class_zone \u00b6 The most simple, where the class of interest is class_id and the zone of interest corresponds to pixels where the class is the argmax along the classes dimension of the model's prediction. This function can be used to design targets to explain: the class of a crowd of objects the class of an object , if there is only one object in the image. the class of a set of objects , if there are few and locally close objects of the same class. get_class_zone( predictions : Union[tf.Tensor, ] , class_id : int) -> tf.Tensor \u00b6 Extract a mask for the class c . The mask correspond to the pixels where the maximum prediction correspond to the class c . Other classes channels are set to zero. Parameters predictions : Union[tf.Tensor, ] Output of the model, it should be the output of a softmax function. We assume the shape (h, w, c). class_id : int Index of the channel of the class of interest. Return class_zone_mask : tf.Tensor Mask of the zone corresponding to the class of interest. Only the corresponding channel is non-zero. The shape is the same as predictions , (h, w, c). get_connected_zone \u00b6 Here coordinates is a \\((h, w)\\) tuple that indicates the indices of a pixel of the image. The class of interest is the argmax along the classes dimension for this given pixel. Then the zone of interest is the set of pixels with the same argmax class that forms a connected zone with the indicated pixel. This function can be seen as selecting a zone with a point in this zone. This function can be used to design targets to explain: the class of an object . the class of a set of objects , if they are connected. the class of part of an object , if this part have been classified differently than the object and the other surrounding objects. get_connected_zone( predictions : Union[tf.Tensor, ] , coordinates : Tuple[int, int]) -> tf.Tensor \u00b6 Extract a connected mask around coordinates . The mask correspond to the pixels where the maximum prediction correspond to the maximum predicted class at coordinates . This class mask is then limited to the connected zone around coordinates . Other classes channels are set to zero. Parameters predictions : Union[tf.Tensor, ] Output of the model, it should be the output of a softmax function. We assume the shape (h, w, c). coordinates : Tuple[int, int] Tuple of coordinates of the point inside the zone of interest. Return connected_zone_mask : tf.Tensor Mask of the connected zone around coordinates with similar class prediction. Only the corresponding channel is non-zero. The shape is the same as predictions , (h, w, c). list_class_connected_zones \u00b6 A mix of get_class_zone and get_connected_zone . class_id indicates the class of interest and each connected zone for this class becomes a zone of interest (apart from zones with size under zone_minimum_size ). It is useful for automatized treatment of explainability, but may generate explanations for zones we may not want to explain. Nonetheless, it can be used to design targets to explain similar elements as get_connected_zone . Warning Contrarily to the other utils function for segmentation, here output is a list of tensors. list_class_connected_zones( predictions : Union[tf.Tensor, ] , class_id : int , zone_minimum_size : int = 100) -> List[tf.Tensor] \u00b6 List all connected zones for a given class. A connected zone is a set of pixels next to each others where the maximum prediction correspond to the same class. This function generate a list of connected zones, each element of the list have a similar format to get_connected_zone outputs. Parameters predictions : Union[tf.Tensor, ] Output of the model, it should be the output of a softmax function. We assume the shape (h, w, c). class_id : int Index of the channel of the class of interest. zone_minimum_size : int = 100 Threshold of number of pixels under which zones are not returned. Return connected_zones_masks_list : List[tf.Tensor] List of the connected zones masks for a given class. Each zone predictions shape is the same as predictions , (h, w, c). Only the corresponding channel is non-zero. get_in_out_border \u00b6 This function allows to compute the targets needed to explain the border of an object . For this function, class_target_mask encodes the class and the zone of interest. From this zone, the in-border (all pixels of the zone with contact to non-zone pixels) and the out-border (all non-zone pixels with contact to pixels of the zone) are computed. Then, the in-borders pixels are set with the predictions values, and out-borders with the opposite of the predictions values. Therefore, explaining this border corresponds to explaining what increased the class predictions inside the zone and decreased it outside, but along the borders of the zone. get_in_out_border( class_zone_mask : Union[tf.Tensor, ]) -> tf.Tensor \u00b6 Extract the border of a zone of interest, then put 1 on the inside border and -1 on the outside border. Parameters class_zone_mask : Union[tf.Tensor, ] Mask delimiting the zone of interest, for the class of interest only one channel should have non-zero values, the one corresponding to the class. We assume the shape (h, w, c) same as the model output for one element. Return class_borders_masks : tf.Tensor Mask of the borders of the zone of the class of interest. Only the corresponding channel is non-zero. Inside borders are set to 1 and outside borders are set to -1 . The shape is the same as class_zone_mask , (h, w, c). get_common_border \u00b6 This function uses two borders computed via the previous function and limits the zone of interest to the common part between both zone of interest. The classes of interest are merged, thus creating a second class of interest. Therefore, this function enables the creation of targets to explain the border between two objects . get_common_border( border_mask_1 : Union[tf.Tensor, ] , border_mask_2 : Union[tf.Tensor, ]) -> tf.Tensor \u00b6 Compute the common part between border_mask_1 and border_mask_2 masks. Those borders should be computed using get_in_out_border . Parameters border_mask_1 : Union[tf.Tensor, ] Border of the first zone of interest. Computed with get_in_out_border . border_mask_2 : Union[tf.Tensor, ] Border of the second zone of interest. Computed with get_in_out_border . Return common_borders_masks : tf.Tensor Mask of the common borders between two zones of interest. Only the two corresponding channels are non-zero. Inside borders are set to 1 and outside borders are set to -1 , Respectively on the two channels. The shape is the same as the input border masks, (h, w, c). What can be explained with it? \u00b6 There are many things that we may want to explain in semantic segmentation, and in this section present different possibilities. The segmentation utils functions allow the design of the targets parameter to specify what to explain. Warning The concept object does not make sense for the model, a semantic segmentation model only classifies pixels. However, what humans want to explain are mainly objects, sets of objects or parts of them. Info As objects do not make sense for the model, to stay coherent when manipulating objects. The only condition is that the predicted class on this connected zone is the same for all pixels. For a concrete example, please refer to the Attributions: Semantic segmentation tutorial. The class of an object \u00b6 Here an object can be a person walking on a street, the dog by his side or a car. However, what humans call an object does not make sense for model, hence explaining an object corresponds to explaining a zone of interest where pixels have the same classification. Warning The zone should be extracted from the model's prediction and not the labels. To explain the difference between labels and predictions there are two possibilities: either the difference is a single zone with a different class than the surroundings, then this zone can be considered an object. or the difference is more complex or mixed with other objects. Then the zones in the union but not in the intersection of both should be iteratively considered objects and explained. It is not recommended to treat them simultaneously. The class of a set of objects \u00b6 A set of objects can be a group of people walking down a street or a set of trees on one side of the road. There are three cases that can be considered set of objects: Connected set of objects, it can be seen as only one big zone and treated the same as in 1. Locally close set of objects, this could also considered a big zone, but it is harder to compute. Set of objects dispersed on the image and hardly countable, if there are a multitude of objects then, it can be seen as a crowd of objects . Otherwise, it should not be treated together. The class of part of an object \u00b6 A part of an object can be the leg of a person, the head of a dog, or a person in a group of people. This is interesting when the part and the object have been classified differently by the model. It should be considered an object as in 1. The class of a crowd of objects \u00b6 A crowd is a set of hardly countable objects, it can be a set of clouds, a multitude of people on the sidewalk or trees in a landscape. The border of an object \u00b6 The border of an object is the limit between the pixels inside the object and those outside of it. Here the object should correspond to a connected zone of pixels where the model predicts the same class. It can be the contour of three people on the side walk or of trees on a landscape. It is interesting when the border is hard to define between similarly colored pixels or when the model prediction is not precise. The border between two objects \u00b6 The border between two objects is the common part between two borders of objects when those two are connected. This can be the border between a person and his wrongly classified leg. Binary semantic segmentation \u00b6 As described in the operator description , the output of the model should have a shape of \\((n, h, w, c)\\) . However, in binary semantic segmentation, the two classes are often encoded by positive and negative value along only one channel with shape \\((n, h, w)\\) . The easiest way to apply xplique on such model is to wrap the model to match the expected format. If we suppose that the output of the binary semantic segmentation model have a shape of \\((n, h, w)\\) , that negative values encode class \\(0\\) , and that positive values encode class \\(1\\) . Then the wrapper can take the form: class Wrapper (): def __init__ ( model ): self . model = model def __call__ ( inputs ): binary_segmentation = self . model ( inputs ) class_0_mask = binary_segmentation < 0 divided = tf . stack ([ - binary_segmentation * tf . cast ( class_0_mask , tf . float32 ), binary_segmentation * tf . cast ( tf . logical_not ( class_0_mask ), tf . float32 )], axis =- 1 ) return tf . nn . softmax ( divided , axis =- 1 ) wrapped_model = wrap ( binary_seg_model )","title":"Semantic Segmentation"},{"location":"api/attributions/semantic_segmentation/#semantic-segmentation-explanations-with-xplique","text":"Attributions: Semantic segmentation tutorial","title":"Semantic segmentation explanations with Xplique"},{"location":"api/attributions/semantic_segmentation/#which-kind-of-tasks-are-supported-by-xplique","text":"With the operator's api you can treat many different problems with Xplique. There is one operator for each task. Task and Documentation link operator parameter value from xplique.Tasks Enum Tutorial link Classification CLASSIFICATION Object Detection OBJECT_DETECTION Regression REGRESSION Semantic Segmentation SEMANTIC_SEGMENTATION Info They all share the API for Xplique attribution methods .","title":"Which kind of tasks are supported by Xplique?"},{"location":"api/attributions/semantic_segmentation/#simple-example","text":"import xplique from xplique.utils_functions.segmentation import get_connected_zone from xplique.attributions import Saliency from xplique.metrics import Deletion # load images and model # ... # extract targets individually coordinates_of_object = ( 42 , 42 ) predictions = model ( image ) target = get_connected_zone ( predictions , coordinates_of_object ) inputs = tf . expand_dims ( image , 0 ) targets = tf . expand_dims ( target , 0 ) explainer = Saliency ( model , operator = xplique . Tasks . SEMANTIC_SEGMENTATION ) explanations = explainer ( inputs , targets ) metric = Deletion ( model , inputs , targets , operator = xplique . Tasks . SEMANTIC_SEGMENTATION ) score_saliency = metric ( explanations )","title":"Simple example"},{"location":"api/attributions/semantic_segmentation/#how-to-use-it","text":"To apply attribution methods, the common API documentation describes the parameters and how to fix them. However, depending on the task and thus on the operator , there are three points that vary: The operator parameter value, it is an Enum or a string identifying the task, The model's output specification, as model(inputs) is used in the computation of the operators, and The targets parameter format, indeed, the targets parameter specifies what to explain and the format of such specification depends on the task. Info Applying attribution methods to semantic segmentation with Xplique has a particularity: a set of functions from utils_functions.segmentation are used to define targets and are documented in the a specific section .","title":"How to use it?"},{"location":"api/attributions/semantic_segmentation/#the-operator","text":"","title":"The operator"},{"location":"api/attributions/semantic_segmentation/#models-output","text":"We expect model(inputs) to yield a \\((n, h, w, c)\\) tensor or array where: \\(n\\) : the number of inputs, it should match the first dimension of inputs \\(h\\) : the height of the images \\(w\\) : the width of the images \\(c\\) : the number of classes Warning The model's output for each pixel is expected to be a soft output and not the class prediction or a one hot encoding of the class. Otherwise the attribution methods will not be able to compare predictions efficiently. Warning Contrary to classification, here a softmax or comparable last layer is necessary as zeros are interpreted by the operator as non-zone of interest. In this sense, strictly positive values are required.","title":"Model's output"},{"location":"api/attributions/semantic_segmentation/#the-targets-parameter","text":"","title":"The targets parameter"},{"location":"api/attributions/semantic_segmentation/#the-segmentation-utils-functions","text":"Source The segmentation utils functions are a set a utility functions used to compute the targets parameter values. They should be applied to each image separately as each segmentation is different want the things to explain differs between images. Nonetheless, you could use tf.map_fn to apply the same function to several images. An example of application of those functions can be found in the Attribution: Semantic segmentation tutorial. For now, there are four functions:","title":"The segmentation utils functions"},{"location":"api/attributions/semantic_segmentation/#what-can-be-explained-with-it","text":"There are many things that we may want to explain in semantic segmentation, and in this section present different possibilities. The segmentation utils functions allow the design of the targets parameter to specify what to explain. Warning The concept object does not make sense for the model, a semantic segmentation model only classifies pixels. However, what humans want to explain are mainly objects, sets of objects or parts of them. Info As objects do not make sense for the model, to stay coherent when manipulating objects. The only condition is that the predicted class on this connected zone is the same for all pixels. For a concrete example, please refer to the Attributions: Semantic segmentation tutorial.","title":"What can be explained with it?"},{"location":"api/attributions/semantic_segmentation/#binary-semantic-segmentation","text":"As described in the operator description , the output of the model should have a shape of \\((n, h, w, c)\\) . However, in binary semantic segmentation, the two classes are often encoded by positive and negative value along only one channel with shape \\((n, h, w)\\) . The easiest way to apply xplique on such model is to wrap the model to match the expected format. If we suppose that the output of the binary semantic segmentation model have a shape of \\((n, h, w)\\) , that negative values encode class \\(0\\) , and that positive values encode class \\(1\\) . Then the wrapper can take the form: class Wrapper (): def __init__ ( model ): self . model = model def __call__ ( inputs ): binary_segmentation = self . model ( inputs ) class_0_mask = binary_segmentation < 0 divided = tf . stack ([ - binary_segmentation * tf . cast ( class_0_mask , tf . float32 ), binary_segmentation * tf . cast ( tf . logical_not ( class_0_mask ), tf . float32 )], axis =- 1 ) return tf . nn . softmax ( divided , axis =- 1 ) wrapped_model = wrap ( binary_seg_model )","title":"Binary semantic segmentation"},{"location":"api/attributions/methods/deconvnet/","text":"Deconvnet \u00b6 View colab tutorial | View source | \ud83d\udcf0 Paper Deconvnet is one of the first attribution method and was proposed in 2013. Its operation is similar to Saliency: it consists in backpropagating the output score with respect to the input, however, at each non-linearity (the ReLUs), only the positive gradient (even of negative activations) are backpropagated. More precisely, with \\(f\\) our classifier and \\(f_l(x)\\) the activation at layer \\(l\\) , we usually have: \\[ \\frac{\\partial f(x)}{\\partial f_{l}(x)} = \\frac{\\partial f(x)}{\\partial \\text{ReLU}(f_{l}(x))} \\frac{\\partial \\text{ReLU}(f_l(x))}{\\partial f_{l}(x)} = \\frac{\\partial f(x)}{\\partial \\text{ReLU}(f_{l}(x))} \\odot \\mathbb{1}(f_{l}(x)) \\] with \\(\\mathbb{1}(.)\\) the indicator function. With Deconvnet, the backpropagation is modified such that : \\[ \\frac{\\partial f(x)}{\\partial f_{l}(x)} = \\frac{\\partial f(x)}{\\partial \\text{ReLU}(f_{l}(x))} \\odot \\mathbb{1}(\\frac{\\partial f(x)}{\\partial \\text{ReLU}(f_{l}(x))}) \\] Example \u00b6 from xplique.attributions import DeconvNet # load images, labels and model # ... method = DeconvNet ( model ) explanations = method . explain ( images , labels ) Notebooks \u00b6 Attribution Methods : Getting started DeconvNet : Going Further DeconvNet \u00b6 Used to compute the DeconvNet method, which modifies the classic Saliency procedure on ReLU's non linearities, allowing only the positive gradients (even from negative inputs) to pass through. __init__( self , model : keras.engine.training.Model , output_layer : Union[str, int, None] = None , batch_size : Union[int, None] = 32 , operator : Union[xplique.commons.operators_operations.Tasks, str , Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float], None] = None, reducer : Union[str, None] = 'mean') \u00b6 Parameters model : keras.engine.training.Model The model from which we want to obtain explanations output_layer : Union[str, int, None] = None Layer to target for the outputs (e.g logits or after softmax). If an int is provided it will be interpreted as a layer index. If a string is provided it will look for the layer name. Default to the last layer. It is recommended to use the layer before Softmax. batch_size : Union[int, None] = 32 Number of inputs to explain at once, if None compute all at once. operator : Union[xplique.commons.operators_operations.Tasks, str, Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float], None] = None Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y]. reducer : Union[str, None] = 'mean' String, name of the reducer to use. Either \"min\", \"mean\", \"max\", \"sum\", or None to ignore. Used only for images to obtain explanation with shape (n, h, w, 1). explain( self , inputs : Union[tf.Dataset, tf.Tensor, ] , targets : Union[tf.Tensor, , None] = None) -> tf.Tensor \u00b6 Compute the explanations of the given inputs. Accept Tensor, numpy array or tf.data.Dataset (in that case targets is None) Parameters inputs : Union[tf.Dataset, tf.Tensor, ] Dataset, Tensor or Array. Input samples to be explained. If Dataset, targets should not be provided (included in Dataset). Expected shape among (N, W), (N, T, W), (N, H, W, C). More information in the documentation. targets : Union[tf.Tensor, , None] = None Tensor or Array. One-hot encoding of the model's output from which an explanation is desired. One encoding per input and only one output at a time. Therefore, the expected shape is (N, output_size). More information in the documentation. Return explanations : tf.Tensor Explanation generated by the method.","title":"DeconvNet"},{"location":"api/attributions/methods/deconvnet/#deconvnet","text":"View colab tutorial | View source | \ud83d\udcf0 Paper Deconvnet is one of the first attribution method and was proposed in 2013. Its operation is similar to Saliency: it consists in backpropagating the output score with respect to the input, however, at each non-linearity (the ReLUs), only the positive gradient (even of negative activations) are backpropagated. More precisely, with \\(f\\) our classifier and \\(f_l(x)\\) the activation at layer \\(l\\) , we usually have: \\[ \\frac{\\partial f(x)}{\\partial f_{l}(x)} = \\frac{\\partial f(x)}{\\partial \\text{ReLU}(f_{l}(x))} \\frac{\\partial \\text{ReLU}(f_l(x))}{\\partial f_{l}(x)} = \\frac{\\partial f(x)}{\\partial \\text{ReLU}(f_{l}(x))} \\odot \\mathbb{1}(f_{l}(x)) \\] with \\(\\mathbb{1}(.)\\) the indicator function. With Deconvnet, the backpropagation is modified such that : \\[ \\frac{\\partial f(x)}{\\partial f_{l}(x)} = \\frac{\\partial f(x)}{\\partial \\text{ReLU}(f_{l}(x))} \\odot \\mathbb{1}(\\frac{\\partial f(x)}{\\partial \\text{ReLU}(f_{l}(x))}) \\]","title":"Deconvnet"},{"location":"api/attributions/methods/deconvnet/#example","text":"from xplique.attributions import DeconvNet # load images, labels and model # ... method = DeconvNet ( model ) explanations = method . explain ( images , labels )","title":"Example"},{"location":"api/attributions/methods/deconvnet/#notebooks","text":"Attribution Methods : Getting started DeconvNet : Going Further","title":"Notebooks"},{"location":"api/attributions/methods/deconvnet/#DeconvNet","text":"Used to compute the DeconvNet method, which modifies the classic Saliency procedure on ReLU's non linearities, allowing only the positive gradients (even from negative inputs) to pass through.","title":"DeconvNet"},{"location":"api/attributions/methods/forgrad/","text":"FORGrad \u00b6 View colab tutorial | View source | \ud83d\udcf0 Paper ForGrad is an enhancement for any attribution method by effectively filtering out high-frequency noise in gradient-based attribution maps, resulting in improved explainability scores and promoting the adoption of more computationally efficient techniques for model interpretability. Quote The application of an optimal low-pass filter to attribution maps improves gradient-based attribution methods significantly, resulting in higher explainability scores across multiple models and elevating gradient-based methods to a top ranking among state-of-the-art techniques, sparking renewed interest in simpler and more computationally efficient explainability approaches. -- Gradient strikes back: How filtering out high frequencies improves explanations (2023) 1 In a more precise manner, to obtain an attribution map \\(\\varphi_\\sigma(x)\\) , we apply a filter \\(w_\\sigma\\) with a cutoff value \\(\\sigma\\) to remove high frequencies, as shown in the equation: \\[ \\varphi_\\sigma(x) = \\mathcal{F}^{-1}((\\mathcal{F} \\cdot \\varphi)(x) \\odot w_\\sigma) \\] The parameter \\(\\sigma\\) controls the amount of frequencies retained and ranges between \\((0, W]\\) , where \\(W\\) represents the dimension of the squared image. A value of \\(0\\) eliminates all frequencies, while \\(W\\) retains all frequencies. The paper presents a method to estimate the optimal cutoff, and for ImageNet images, the recommended default value for the optimal sigma is typically around 15. Example \u00b6 from xplique.attributions import Saliency from xplique.common import forgrad # load images, labels and model # ... method = Saliency ( model ) explanations = method . explain ( images , labels ) explanations_filtered = forgrad ( explanations , sigma = 15 ) Notebooks \u00b6 FORGRad : Gradient strikes back with FORGrad Attribution Methods : Getting started forgrad( explanations : tf.Tensor , sigma : int = 15) -> tf.Tensor \u00b6 ForGRAD is a method that enhances any attributions explanations (particularly useful on gradients based attribution method) by eliminating high frequencies in the explanations. Parameters explanations : tf.Tensor List of explanations to filter. Explanation should be at least 3D (batch, height, width) and should have the same height and width. sigma : int = 15 Bandwith of the low pass filter. The higher the sigma, the more frequencies are kept. Sigma should be positive and less than image size. Default to paper recommendation, 15 for image size 224. Return filtered_explanations : tf.Tensor Explanations low-pass filtered. Gradient strikes back: How filtering out high frequencies improves explanations (2023) \u21a9","title":"ForGRad"},{"location":"api/attributions/methods/forgrad/#forgrad_1","text":"View colab tutorial | View source | \ud83d\udcf0 Paper ForGrad is an enhancement for any attribution method by effectively filtering out high-frequency noise in gradient-based attribution maps, resulting in improved explainability scores and promoting the adoption of more computationally efficient techniques for model interpretability. Quote The application of an optimal low-pass filter to attribution maps improves gradient-based attribution methods significantly, resulting in higher explainability scores across multiple models and elevating gradient-based methods to a top ranking among state-of-the-art techniques, sparking renewed interest in simpler and more computationally efficient explainability approaches. -- Gradient strikes back: How filtering out high frequencies improves explanations (2023) 1 In a more precise manner, to obtain an attribution map \\(\\varphi_\\sigma(x)\\) , we apply a filter \\(w_\\sigma\\) with a cutoff value \\(\\sigma\\) to remove high frequencies, as shown in the equation: \\[ \\varphi_\\sigma(x) = \\mathcal{F}^{-1}((\\mathcal{F} \\cdot \\varphi)(x) \\odot w_\\sigma) \\] The parameter \\(\\sigma\\) controls the amount of frequencies retained and ranges between \\((0, W]\\) , where \\(W\\) represents the dimension of the squared image. A value of \\(0\\) eliminates all frequencies, while \\(W\\) retains all frequencies. The paper presents a method to estimate the optimal cutoff, and for ImageNet images, the recommended default value for the optimal sigma is typically around 15.","title":"FORGrad"},{"location":"api/attributions/methods/forgrad/#example","text":"from xplique.attributions import Saliency from xplique.common import forgrad # load images, labels and model # ... method = Saliency ( model ) explanations = method . explain ( images , labels ) explanations_filtered = forgrad ( explanations , sigma = 15 )","title":"Example"},{"location":"api/attributions/methods/forgrad/#notebooks","text":"FORGRad : Gradient strikes back with FORGrad Attribution Methods : Getting started","title":"Notebooks"},{"location":"api/attributions/methods/grad_cam/","text":"GradCAM \u00b6 View colab tutorial | View source | \ud83d\udcf0 Paper Grad-CAM is a technique for producing visual explanations that can be used on Convolutional Neural Network (CNN) which uses both gradients and the feature maps of the last convolutional layer. Quote Grad-CAM uses the gradients of any target concept (say logits for \u201cdog\u201d or even a caption), flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. -- Visual Explanations from Deep Networks via Gradient-based Localization (2016). More precisely, to obtain the localization map for a prediction \\(f(x)\\) , we need to compute the weights \\(w_k\\) associated to each of the feature map channel \\(A^k \\in \\mathbb{R}^{W \\times H}\\) . As we use the last convolutionnal layer, \\(k\\) will be the number of filters, \\(Z\\) is the number of pixels in each feature map ( \\(Z = W \\times H\\) , e.g. 7x7 for ResNet50). \\[ w_k = \\frac{1}{Z} \\sum_i \\sum_j \\frac{\\partial f(x)}{\\partial A^k_{i,j}} \\] We now use this weight to ponderate and aggregate the feature maps to obtain our grad-cam attribution \\(\\phi\\) : \\[ \\phi = \\text{max}(0, \\sum_k w_k A^k) \\] Notice that \\(\\phi \\in \\mathbb{R}^{W \\times H}\\) and thus the size of the explanation depends on the size of the feature map ( \\(W, H\\) ) of the last feature map. In order to compare it to the original input \\(x\\) , we upsample \\(\\phi\\) using bicubic interpolation. Example \u00b6 from xplique.attributions import GradCAM # load images, labels and model # ... method = GradCAM ( model ) explanations = method . explain ( images , labels ) Notebooks \u00b6 Attribution Methods : Getting started GradCAM : Going Further GradCAM \u00b6 Used to compute the Grad-CAM visualization method. __init__( self , model : keras.engine.training.Model , output_layer : Union[str, int, None] = None , batch_size : Union[int, None] = 32 , operator : Union[xplique.commons.operators_operations.Tasks, str , Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float], None] = None, conv_layer : Union[str, int, None] = None) \u00b6 Parameters model : keras.engine.training.Model The model from which we want to obtain explanations output_layer : Union[str, int, None] = None Layer to target for the outputs (e.g logits or after softmax). If an int is provided it will be interpreted as a layer index. If a string is provided it will look for the layer name. Default to the last layer. It is recommended to use the layer before Softmax. batch_size : Union[int, None] = 32 Number of inputs to explain at once, if None compute all at once. operator : Union[xplique.commons.operators_operations.Tasks, str, Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float], None] = None Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y]. conv_layer : Union[str, int, None] = None Layer to target for Grad-CAM algorithm. If an int is provided it will be interpreted as a layer index. If a string is provided it will look for the layer name. explain( self , inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] , targets : Union[tf.Tensor, numpy.ndarray, None] = None) -> tf.Tensor \u00b6 Compute and resize explanations to match inputs shape. Accept Tensor, numpy array or tf.data.Dataset (in that case targets is None) Parameters inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] Dataset, Tensor or Array. Input samples to be explained. If Dataset, targets should not be provided (included in Dataset). Expected shape among (N, W), (N, T, W), (N, H, W, C). More information in the documentation. targets : Union[tf.Tensor, numpy.ndarray, None] = None Tensor or Array. One-hot encoding of the model's output from which an explanation is desired. One encoding per input and only one output at a time. Therefore, the expected shape is (N, output_size). More information in the documentation. Return grad_cam : tf.Tensor Grad-CAM explanations, same shape as the inputs except for the channels. Visual Explanations from Deep Networks via Gradient-based Localization (2016). \u21a9","title":"Grad-CAM"},{"location":"api/attributions/methods/grad_cam/#gradcam","text":"View colab tutorial | View source | \ud83d\udcf0 Paper Grad-CAM is a technique for producing visual explanations that can be used on Convolutional Neural Network (CNN) which uses both gradients and the feature maps of the last convolutional layer. Quote Grad-CAM uses the gradients of any target concept (say logits for \u201cdog\u201d or even a caption), flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. -- Visual Explanations from Deep Networks via Gradient-based Localization (2016). More precisely, to obtain the localization map for a prediction \\(f(x)\\) , we need to compute the weights \\(w_k\\) associated to each of the feature map channel \\(A^k \\in \\mathbb{R}^{W \\times H}\\) . As we use the last convolutionnal layer, \\(k\\) will be the number of filters, \\(Z\\) is the number of pixels in each feature map ( \\(Z = W \\times H\\) , e.g. 7x7 for ResNet50). \\[ w_k = \\frac{1}{Z} \\sum_i \\sum_j \\frac{\\partial f(x)}{\\partial A^k_{i,j}} \\] We now use this weight to ponderate and aggregate the feature maps to obtain our grad-cam attribution \\(\\phi\\) : \\[ \\phi = \\text{max}(0, \\sum_k w_k A^k) \\] Notice that \\(\\phi \\in \\mathbb{R}^{W \\times H}\\) and thus the size of the explanation depends on the size of the feature map ( \\(W, H\\) ) of the last feature map. In order to compare it to the original input \\(x\\) , we upsample \\(\\phi\\) using bicubic interpolation.","title":"GradCAM"},{"location":"api/attributions/methods/grad_cam/#example","text":"from xplique.attributions import GradCAM # load images, labels and model # ... method = GradCAM ( model ) explanations = method . explain ( images , labels )","title":"Example"},{"location":"api/attributions/methods/grad_cam/#notebooks","text":"Attribution Methods : Getting started GradCAM : Going Further","title":"Notebooks"},{"location":"api/attributions/methods/grad_cam/#GradCAM","text":"Used to compute the Grad-CAM visualization method.","title":"GradCAM"},{"location":"api/attributions/methods/grad_cam_pp/","text":"Grad-CAM ++ \u00b6 View colab tutorial | View source | \ud83d\udcf0 Paper Grad-CAM++ is a technique for producing visual explanations that can be used on Convolutional Neural Network (CNN) which uses both gradients and the feature maps of the last convolutional layer. More precisely, to obtain the localization map for a prediction \\(f(x)\\) , we need to compute the weights \\(w_k\\) associated to each of the feature map channel \\(A^k \\in \\mathbb{R}^{W \\times H}\\) . As we use the last convolutionnal layer, \\(k\\) will be the number of filters, \\(Z\\) is the number of pixels in each feature map ( \\(Z = W \\times H\\) , e.g. 7x7 for ResNet50). once this weights are obtained, we use them to ponderate and aggregate the feature maps to obtain our grad-cam++ attribution \\(\\phi\\) : \\[ \\phi = \\text{max}(0, \\sum_k w_k A^k) \\] Notice that \\(\\phi \\in \\mathbb{R}^{W \\times H}\\) and thus the size of the explanation depends on the size of the feature map ( \\(W, H\\) ) of the last feature map. In order to compare it to the original input \\(x\\) , we upsample \\(\\phi\\) using bicubic interpolation. Example \u00b6 from xplique.attributions import GradCAMPP # load images, labels and model # ... method = GradCAMPP ( model ) explanations = method . explain ( images , labels ) Notebooks \u00b6 Attribution Methods : Getting started GradCAMPP : Going Further GradCAMPP \u00b6 Used to compute the Grad-CAM++ visualization method. __init__( self , model : keras.engine.training.Model , output_layer : Union[str, int, None] = None , batch_size : Union[int, None] = 32 , operator : Union[xplique.commons.operators_operations.Tasks, str , Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float], None] = None, conv_layer : Union[str, int, None] = None) \u00b6 Parameters model : keras.engine.training.Model The model from which we want to obtain explanations output_layer : Union[str, int, None] = None Layer to target for the outputs (e.g logits or after softmax). If an int is provided it will be interpreted as a layer index. If a string is provided it will look for the layer name. Default to the last layer. It is recommended to use the layer before Softmax. batch_size : Union[int, None] = 32 Number of inputs to explain at once, if None compute all at once. operator : Union[xplique.commons.operators_operations.Tasks, str, Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float], None] = None Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y]. conv_layer : Union[str, int, None] = None Layer to target for Grad-CAM++ algorithm. If an int is provided it will be interpreted as a layer index. If a string is provided it will look for the layer name. explain( self , inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] , targets : Union[tf.Tensor, numpy.ndarray, None] = None) -> tf.Tensor \u00b6 Compute and resize explanations to match inputs shape. Accept Tensor, numpy array or tf.data.Dataset (in that case targets is None) Parameters inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] Dataset, Tensor or Array. Input samples to be explained. If Dataset, targets should not be provided (included in Dataset). Expected shape among (N, W), (N, T, W), (N, H, W, C). More information in the documentation. targets : Union[tf.Tensor, numpy.ndarray, None] = None Tensor or Array. One-hot encoding of the model's output from which an explanation is desired. One encoding per input and only one output at a time. Therefore, the expected shape is (N, output_size). More information in the documentation. Return grad_cam : tf.Tensor Grad-CAM explanations, same shape as the inputs except for the channels. Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks (2017). \u21a9","title":"Grad-CAM++"},{"location":"api/attributions/methods/grad_cam_pp/#grad-cam","text":"View colab tutorial | View source | \ud83d\udcf0 Paper Grad-CAM++ is a technique for producing visual explanations that can be used on Convolutional Neural Network (CNN) which uses both gradients and the feature maps of the last convolutional layer. More precisely, to obtain the localization map for a prediction \\(f(x)\\) , we need to compute the weights \\(w_k\\) associated to each of the feature map channel \\(A^k \\in \\mathbb{R}^{W \\times H}\\) . As we use the last convolutionnal layer, \\(k\\) will be the number of filters, \\(Z\\) is the number of pixels in each feature map ( \\(Z = W \\times H\\) , e.g. 7x7 for ResNet50). once this weights are obtained, we use them to ponderate and aggregate the feature maps to obtain our grad-cam++ attribution \\(\\phi\\) : \\[ \\phi = \\text{max}(0, \\sum_k w_k A^k) \\] Notice that \\(\\phi \\in \\mathbb{R}^{W \\times H}\\) and thus the size of the explanation depends on the size of the feature map ( \\(W, H\\) ) of the last feature map. In order to compare it to the original input \\(x\\) , we upsample \\(\\phi\\) using bicubic interpolation.","title":"Grad-CAM ++"},{"location":"api/attributions/methods/grad_cam_pp/#example","text":"from xplique.attributions import GradCAMPP # load images, labels and model # ... method = GradCAMPP ( model ) explanations = method . explain ( images , labels )","title":"Example"},{"location":"api/attributions/methods/grad_cam_pp/#notebooks","text":"Attribution Methods : Getting started GradCAMPP : Going Further","title":"Notebooks"},{"location":"api/attributions/methods/grad_cam_pp/#GradCAMPP","text":"Used to compute the Grad-CAM++ visualization method.","title":"GradCAMPP"},{"location":"api/attributions/methods/gradient_input/","text":"Gradient \\(\\odot\\) Input \u00b6 View colab tutorial | View source | \ud83d\udcf0 Paper Gradient \\(\\odot\\) Input is a visualization techniques based on the gradient of a class score relative to the input, element-wise with the input. This method was introduced by Shrikumar et al., 2016 1 , in an old version of their DeepLIFT paper 2 . Quote Gradient inputs was at first proposed as a technique to improve the sharpness of the attribution maps. The attribution is computed taking the (signed) partial derivatives of the output with respect to the input and multiplying them with the input itself. -- Towards better understanding of the gradient-based attribution methods for Deep Neural Networks (2017) 3 A theoretical analysis conducted by Ancona et al, 2018 3 showed that Gradient \\(\\odot\\) Input is equivalent to \\(\\epsilon\\) -LRP and DeepLift methods under certain conditions: using a baseline of zero, and with all biases to zero. More precisely, the explanation \\(\\phi\\) for an input \\(x\\) and a classifier \\(f\\) is defined as \\[ \\phi = x \\odot \\nabla_x f(x) \\] with \\(\\odot\\) the Hadamard product. Example \u00b6 from xplique.attributions import GradientInput # load images, labels and model # ... method = GradientInput ( model ) explanations = method . explain ( images , labels ) Notebooks \u00b6 Attribution Methods : Getting started Gradient \\(\\odot\\) Input : Going Further GradientInput \u00b6 Used to compute elementwise product between the saliency maps of Simonyan et al. and the input (Gradient x Input). __init__( self , model : keras.engine.training.Model , output_layer : Union[str, int, None] = None , batch_size : Union[int, None] = 64 , operator : Union[Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float], None] = None , reducer : Union[str, None] = 'mean') \u00b6 Parameters model : keras.engine.training.Model The model from which we want to obtain explanations output_layer : Union[str, int, None] = None Layer to target for the outputs (e.g logits or after softmax). If an int is provided it will be interpreted as a layer index. If a string is provided it will look for the layer name. Default to the last layer. It is recommended to use the layer before Softmax. batch_size : Union[int, None] = 64 Number of inputs to explain at once, if None compute all at once. operator : Union[Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float], None] = None Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y]. reducer : Union[str, None] = 'mean' String, name of the reducer to use. Either \"min\", \"mean\", \"max\", \"sum\", or None to ignore. Used only for images to obtain explanation with shape (n, h, w, 1). explain( self , inputs : Union[tf.Dataset, tf.Tensor, ] , targets : Union[tf.Tensor, , None] = None) -> tf.Tensor \u00b6 Compute the explanations of the given inputs. Accept Tensor, numpy array or tf.data.Dataset (in that case targets is None) Parameters inputs : Union[tf.Dataset, tf.Tensor, ] Dataset, Tensor or Array. Input samples to be explained. If Dataset, targets should not be provided (included in Dataset). Expected shape among (N, W), (N, T, W), (N, H, W, C). More information in the documentation. targets : Union[tf.Tensor, , None] = None Tensor or Array. One-hot encoding of the model's output from which an explanation is desired. One encoding per input and only one output at a time. Therefore, the expected shape is (N, output_size). More information in the documentation. Return explanations : tf.Tensor Explanation generated by the method. Not Just a Black Box: Learning Important Features Through Propagating Activation Differences \u21a9 Learning Important Features Through Propagating Activation Differences \u21a9 Towards better understanding of gradient-based attribution methods for Deep Neural Networks \u21a9 \u21a9","title":"Gradient Input"},{"location":"api/attributions/methods/gradient_input/#gradient-odot-input","text":"View colab tutorial | View source | \ud83d\udcf0 Paper Gradient \\(\\odot\\) Input is a visualization techniques based on the gradient of a class score relative to the input, element-wise with the input. This method was introduced by Shrikumar et al., 2016 1 , in an old version of their DeepLIFT paper 2 . Quote Gradient inputs was at first proposed as a technique to improve the sharpness of the attribution maps. The attribution is computed taking the (signed) partial derivatives of the output with respect to the input and multiplying them with the input itself. -- Towards better understanding of the gradient-based attribution methods for Deep Neural Networks (2017) 3 A theoretical analysis conducted by Ancona et al, 2018 3 showed that Gradient \\(\\odot\\) Input is equivalent to \\(\\epsilon\\) -LRP and DeepLift methods under certain conditions: using a baseline of zero, and with all biases to zero. More precisely, the explanation \\(\\phi\\) for an input \\(x\\) and a classifier \\(f\\) is defined as \\[ \\phi = x \\odot \\nabla_x f(x) \\] with \\(\\odot\\) the Hadamard product.","title":"Gradient \\(\\odot\\) Input"},{"location":"api/attributions/methods/gradient_input/#example","text":"from xplique.attributions import GradientInput # load images, labels and model # ... method = GradientInput ( model ) explanations = method . explain ( images , labels )","title":"Example"},{"location":"api/attributions/methods/gradient_input/#notebooks","text":"Attribution Methods : Getting started Gradient \\(\\odot\\) Input : Going Further","title":"Notebooks"},{"location":"api/attributions/methods/gradient_input/#GradientInput","text":"Used to compute elementwise product between the saliency maps of Simonyan et al. and the input (Gradient x Input).","title":"GradientInput"},{"location":"api/attributions/methods/guided_backpropagation/","text":"Guided Backpropagation \u00b6 View colab tutorial | View source | \ud83d\udcf0 Paper Guided-backprop is one of the first attribution method and was proposed in 2014. Its operation is similar to Saliency: it consists in backpropagating the output score with respect to the input, however, at each non-linearity (the ReLUs), only the positive gradient of positive activations are backpropagated. We can see this as a filter on the backprop. More precisely, with \\(f\\) our classifier and \\(f_l(x)\\) the activation at layer \\(l\\) , we usually have: \\[ \\frac{\\partial f(x)}{\\partial f_{l}(x)} = \\frac{\\partial f(x)}{\\partial \\text{ReLU}(f_{l}(x))} \\frac{\\partial \\text{ReLU}(f_l(x))}{\\partial f_{l}(x)} = \\frac{\\partial f(x)}{\\partial \\text{ReLU}(f_{l}(x))} \\odot \\mathbb{1}(f_{l}(x)) \\] with \\(\\mathbb{1}(.)\\) the indicator function. With Guided-backprop, the backpropagation is modified such that : \\[ \\frac{\\partial f(x)}{\\partial f_{l}(x)} = \\frac{\\partial f(x)}{\\partial \\text{ReLU}(f_{l}(x))} \\odot \\mathbb{1}(f_{l}(x)) \\odot \\mathbb{1}(\\frac{\\partial f(x)}{\\partial \\text{ReLU}(f_{l}(x))}) \\] Example \u00b6 from xplique.attributions import GuidedBackprop # load images, labels and model # ... method = GuidedBackprop ( model ) explanations = method . explain ( images , labels ) Notebooks \u00b6 Attribution Methods : Getting started Guided Backprop : Going Further GuidedBackprop \u00b6 Used to compute the Guided Backpropagation, which modifies the classic Saliency procedure on ReLU's non linearities, allowing only the positive gradients from positive activations to pass through. __init__( self , model : keras.engine.training.Model , output_layer : Union[str, int, None] = None , batch_size : Union[int, None] = 32 , operator : Union[xplique.commons.operators_operations.Tasks, str , Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float], None] = None, reducer : Union[str, None] = 'mean') \u00b6 Parameters model : keras.engine.training.Model The model from which we want to obtain explanations output_layer : Union[str, int, None] = None Layer to target for the outputs (e.g logits or after softmax). If an int is provided it will be interpreted as a layer index. If a string is provided it will look for the layer name. Default to the last layer. It is recommended to use the layer before Softmax. batch_size : Union[int, None] = 32 Number of inputs to explain at once, if None compute all at once. operator : Union[xplique.commons.operators_operations.Tasks, str, Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float], None] = None Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y]. reducer : Union[str, None] = 'mean' String, name of the reducer to use. Either \"min\", \"mean\", \"max\", \"sum\", or None to ignore. Used only for images to obtain explanation with shape (n, h, w, 1). explain( self , inputs : Union[tf.Dataset, tf.Tensor, ] , targets : Union[tf.Tensor, , None] = None) -> tf.Tensor \u00b6 Compute the explanations of the given inputs. Accept Tensor, numpy array or tf.data.Dataset (in that case targets is None) Parameters inputs : Union[tf.Dataset, tf.Tensor, ] Dataset, Tensor or Array. Input samples to be explained. If Dataset, targets should not be provided (included in Dataset). Expected shape among (N, W), (N, T, W), (N, H, W, C). More information in the documentation. targets : Union[tf.Tensor, , None] = None Tensor or Array. One-hot encoding of the model's output from which an explanation is desired. One encoding per input and only one output at a time. Therefore, the expected shape is (N, output_size). More information in the documentation. Return explanations : tf.Tensor Explanation generated by the method.","title":"Guided Backprop"},{"location":"api/attributions/methods/guided_backpropagation/#guided-backpropagation","text":"View colab tutorial | View source | \ud83d\udcf0 Paper Guided-backprop is one of the first attribution method and was proposed in 2014. Its operation is similar to Saliency: it consists in backpropagating the output score with respect to the input, however, at each non-linearity (the ReLUs), only the positive gradient of positive activations are backpropagated. We can see this as a filter on the backprop. More precisely, with \\(f\\) our classifier and \\(f_l(x)\\) the activation at layer \\(l\\) , we usually have: \\[ \\frac{\\partial f(x)}{\\partial f_{l}(x)} = \\frac{\\partial f(x)}{\\partial \\text{ReLU}(f_{l}(x))} \\frac{\\partial \\text{ReLU}(f_l(x))}{\\partial f_{l}(x)} = \\frac{\\partial f(x)}{\\partial \\text{ReLU}(f_{l}(x))} \\odot \\mathbb{1}(f_{l}(x)) \\] with \\(\\mathbb{1}(.)\\) the indicator function. With Guided-backprop, the backpropagation is modified such that : \\[ \\frac{\\partial f(x)}{\\partial f_{l}(x)} = \\frac{\\partial f(x)}{\\partial \\text{ReLU}(f_{l}(x))} \\odot \\mathbb{1}(f_{l}(x)) \\odot \\mathbb{1}(\\frac{\\partial f(x)}{\\partial \\text{ReLU}(f_{l}(x))}) \\]","title":"Guided Backpropagation"},{"location":"api/attributions/methods/guided_backpropagation/#example","text":"from xplique.attributions import GuidedBackprop # load images, labels and model # ... method = GuidedBackprop ( model ) explanations = method . explain ( images , labels )","title":"Example"},{"location":"api/attributions/methods/guided_backpropagation/#notebooks","text":"Attribution Methods : Getting started Guided Backprop : Going Further","title":"Notebooks"},{"location":"api/attributions/methods/guided_backpropagation/#GuidedBackprop","text":"Used to compute the Guided Backpropagation, which modifies the classic Saliency procedure on ReLU's non linearities, allowing only the positive gradients from positive activations to pass through.","title":"GuidedBackprop"},{"location":"api/attributions/methods/hsic/","text":"Hsic Attribution Method \u00b6 View colab tutorial | View source | \ud83d\udcf0 Paper The Hsic attribution method from Novello, Fel, Vigouroux 1 explains a neural network's prediction for a given input image by assessing the dependence between the output and patches of the input. Thanks to the sample efficiency of HSIC Estimator, this black box method requires fewer forward passes to produce relevant explanations. Let's consider two random variables which are the perturbation associated with each patch of the input image, \\(X_i, i \\in \\{1,...d\\}\\) with \\(d= \\text{grid_size}^2\\) image patches and the output \\(Y\\) . Let \\(X^1_i,...,X^p_i\\) and \\(Y^1,...,Y^p\\) be \\(p\\) samples of \\(X_i\\) and \\(Y\\) . HSIC attribution method requires selecting a kernel for the input and the output to construct an RKHS on which is computed the Maximum Mean Discrepancy, a dissimilarity metric between distributions. Let \\(k:\\mathbb{R}^2 \\rightarrow \\mathbb{R}\\) and \\(l:\\mathbb{R}^2 \\rightarrow \\mathbb{R}\\) the kernels selected for \\(X_i\\) and \\(Y\\) , HSIC is estimated with an error \\(\\mathcal{O}(1/\\sqrt{p})\\) using the estimator $$ \\mathcal{H}^p_{X_i, Y} = \\frac{1}{(p-1)^2} \\operatorname{tr} (KHLH), $$ where \\(H, L, K \\in \\mathbb{R}^{p \\times p}\\) and \\(K_{ij} = k(x_i, x_j), L_{i,j} = l(y_i, y_j)\\) and \\(H_{ij} = \\delta(i=j) - p^{-1}\\) where \\(\\delta(i=j) = 1\\) if \\(i=j\\) and \\(0\\) otherwise. In the paper Making Sense of Dependence: Efficient Black-box Explanations Using Dependence Measure , the sampler LatinHypercube is used to sample the perturbations. Note however that the present implementation uses TFSobolSequence as default sampler because LatinHypercube requires scipy \\(\\geq\\) 1.7.0 . you can nevertheless use this sampler -- which is included in the library -- by specifying it during the init of your explainer. For the kernel \\(k\\) applied on \\(X_i\\) , a modified Dirac kernel is used to enable an ANOVA-like decomposition property that allows assessing pairwise patch interactions (see the paper for more details). For the kernel \\(l\\) of output \\(Y\\) , a Radial Basis Function (RBF) is used. Tip We recommend using a grid size of \\(7 \\times 7\\) to define the image patches. The paper uses a number of forwards of \\(1500\\) to obtain the most faithful explanations and \\(750\\) for a more budget - but still faithful - version. Info To explain small objects in images, it may be necessary to increase the grid_size , which also requires an increase in nb_design . However, increasing both may impact the memory usage and result in out of memory errors, hence, setting estimator_batch_size parameter enables a limited usage of the memory. Note that the classical batch_size correspond to the batch_size used in the model call, here estimator_batch_size is intern to the method estimator. Example \u00b6 Low budget version from xplique.attributions import HsicAttributionMethod # load images, labels and model # ... explainer = HsicAttributionMethod ( model , grid_size = 7 , nb_design = 750 ) explanations = explainer ( images , labels ) High budget version from xplique.attributions import HsicAttributionMethod # load images, labels and model # ... explainer = HsicAttributionMethod ( model , grid_size = 7 , nb_design = 1500 ) explanations = explainer ( images , labels ) Recommended version, (you need scipy \\(\\geq\\) 1.7.0 ) from xplique.attributions import HsicAttributionMethod from xplique.attributions.global_sensitivity_analysis import LatinHypercube # load images, labels and model # ... explainer = HsicAttributionMethod ( model , grid_size = 7 , nb_design = 1500 , sampler = LatinHypercube ( binary = True )) explanations = explainer ( images , labels ) Notebooks \u00b6 Attribution Methods : Getting started HsicAttributionMethod \u00b6 HSIC Attribution Method. Compute the dependance of each input dimension wrt the output using Hilbert-Schmidt Independance Criterion, a perturbation function on a grid and an adapted sampling as described in the original paper. __init__( self , model, grid_size : int = 8 , nb_design : int = 500 , sampler : Union[xplique.attributions.global_sensitivity_analysis.samplers.Sampler, None] = None , estimator : Union[xplique.attributions.global_sensitivity_analysis.hsic_estimators.HsicEstimator, None] = None , perturbation_function : Union[Callable, str, None] = 'inpainting' , batch_size : int = 256 , estimator_batch_size : int = None , operator : Union[xplique.commons.operators_operations.Tasks, str , Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float], None] = None) \u00b6 Parameters model : model Model used for computing explanations. grid_size : int = 8 Cut the image in a grid of (grid_size, grid_size) to estimate an indice per cell. nb_design : int = 500 Number of design for the sampler. sampler : Union[xplique.attributions.global_sensitivity_analysis.samplers.Sampler, None] = None Sampler used to generate the (quasi-)monte carlo samples, LHS or QMC. For more option, see the sampler module. Note that the original paper uses LHS but here the default sampler is TFSobolSequence as LHS requires scipy 1.7.0. estimator : Union[xplique.attributions.global_sensitivity_analysis.hsic_estimators.HsicEstimator, None] = None Estimator used to compute the HSIC score. perturbation_function : Union[Callable, str, None] = 'inpainting' Function to call to apply the perturbation on the input. Can also be string: 'inpainting', 'blurring', or 'amplitude'. batch_size : int = 256 Batch size to use for the forwards. estimator_batch_size : int = None Batch size to use in the estimator. It should only be set if HSIC exceeds the memory. By default, a tensor of grid_size \u00b2 * nb_design \u00b2 is created. estimator_batch_size is used over the nb_design \u00b2 dimension. operator : Union[xplique.commons.operators_operations.Tasks, str, Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float], None] = None Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y]. explain( self , inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] , targets : Union[tf.Tensor, numpy.ndarray, None] = None) -> tf.Tensor \u00b6 Compute the total Sobol' indices according to the explainer parameter (perturbation function, grid size...). Accept Tensor, numpy array or tf.data.Dataset (in that case targets is None). Parameters inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] Images to be explained, either tf.dataset, Tensor or numpy array. If Dataset, targets should not be provided (included in Dataset). Expected shape (N, W, H, C) or (N, W, H). targets : Union[tf.Tensor, numpy.ndarray, None] = None One-hot encoding for classification or direction {-1, +1} for regression. Tensor or numpy array. Expected shape (N, C) or (N). Return attributions_maps : tf.Tensor GSA Attribution Method explanations, same shape as the inputs except for the channels. Making Sense of Dependence: Efficient Black-box Explanations Using Dependence Measure (2022) \u21a9","title":"Hsic Attribution Method"},{"location":"api/attributions/methods/hsic/#hsic-attribution-method","text":"View colab tutorial | View source | \ud83d\udcf0 Paper The Hsic attribution method from Novello, Fel, Vigouroux 1 explains a neural network's prediction for a given input image by assessing the dependence between the output and patches of the input. Thanks to the sample efficiency of HSIC Estimator, this black box method requires fewer forward passes to produce relevant explanations. Let's consider two random variables which are the perturbation associated with each patch of the input image, \\(X_i, i \\in \\{1,...d\\}\\) with \\(d= \\text{grid_size}^2\\) image patches and the output \\(Y\\) . Let \\(X^1_i,...,X^p_i\\) and \\(Y^1,...,Y^p\\) be \\(p\\) samples of \\(X_i\\) and \\(Y\\) . HSIC attribution method requires selecting a kernel for the input and the output to construct an RKHS on which is computed the Maximum Mean Discrepancy, a dissimilarity metric between distributions. Let \\(k:\\mathbb{R}^2 \\rightarrow \\mathbb{R}\\) and \\(l:\\mathbb{R}^2 \\rightarrow \\mathbb{R}\\) the kernels selected for \\(X_i\\) and \\(Y\\) , HSIC is estimated with an error \\(\\mathcal{O}(1/\\sqrt{p})\\) using the estimator $$ \\mathcal{H}^p_{X_i, Y} = \\frac{1}{(p-1)^2} \\operatorname{tr} (KHLH), $$ where \\(H, L, K \\in \\mathbb{R}^{p \\times p}\\) and \\(K_{ij} = k(x_i, x_j), L_{i,j} = l(y_i, y_j)\\) and \\(H_{ij} = \\delta(i=j) - p^{-1}\\) where \\(\\delta(i=j) = 1\\) if \\(i=j\\) and \\(0\\) otherwise. In the paper Making Sense of Dependence: Efficient Black-box Explanations Using Dependence Measure , the sampler LatinHypercube is used to sample the perturbations. Note however that the present implementation uses TFSobolSequence as default sampler because LatinHypercube requires scipy \\(\\geq\\) 1.7.0 . you can nevertheless use this sampler -- which is included in the library -- by specifying it during the init of your explainer. For the kernel \\(k\\) applied on \\(X_i\\) , a modified Dirac kernel is used to enable an ANOVA-like decomposition property that allows assessing pairwise patch interactions (see the paper for more details). For the kernel \\(l\\) of output \\(Y\\) , a Radial Basis Function (RBF) is used. Tip We recommend using a grid size of \\(7 \\times 7\\) to define the image patches. The paper uses a number of forwards of \\(1500\\) to obtain the most faithful explanations and \\(750\\) for a more budget - but still faithful - version. Info To explain small objects in images, it may be necessary to increase the grid_size , which also requires an increase in nb_design . However, increasing both may impact the memory usage and result in out of memory errors, hence, setting estimator_batch_size parameter enables a limited usage of the memory. Note that the classical batch_size correspond to the batch_size used in the model call, here estimator_batch_size is intern to the method estimator.","title":"Hsic Attribution Method"},{"location":"api/attributions/methods/hsic/#example","text":"Low budget version from xplique.attributions import HsicAttributionMethod # load images, labels and model # ... explainer = HsicAttributionMethod ( model , grid_size = 7 , nb_design = 750 ) explanations = explainer ( images , labels ) High budget version from xplique.attributions import HsicAttributionMethod # load images, labels and model # ... explainer = HsicAttributionMethod ( model , grid_size = 7 , nb_design = 1500 ) explanations = explainer ( images , labels ) Recommended version, (you need scipy \\(\\geq\\) 1.7.0 ) from xplique.attributions import HsicAttributionMethod from xplique.attributions.global_sensitivity_analysis import LatinHypercube # load images, labels and model # ... explainer = HsicAttributionMethod ( model , grid_size = 7 , nb_design = 1500 , sampler = LatinHypercube ( binary = True )) explanations = explainer ( images , labels )","title":"Example"},{"location":"api/attributions/methods/hsic/#notebooks","text":"Attribution Methods : Getting started","title":"Notebooks"},{"location":"api/attributions/methods/hsic/#HsicAttributionMethod","text":"HSIC Attribution Method. Compute the dependance of each input dimension wrt the output using Hilbert-Schmidt Independance Criterion, a perturbation function on a grid and an adapted sampling as described in the original paper.","title":"HsicAttributionMethod"},{"location":"api/attributions/methods/integrated_gradients/","text":"Integrated Gradients \u00b6 View colab tutorial | View source | \ud83d\udcf0 Paper Integrated Gradients is a visualization technique resulting of a theoretical search for an explanatory method that satisfies two axioms, Sensitivity and Implementation Invariance (Sundararajan et al 1 ). Quote We consider the straightline path (in \\(R^n\\) ) from the baseline \\(\\bar{x}\\) to the input \\(x\\) , and compute the gradients at all points along the path. Integrated gradients are obtained by cumulating these gradients. -- Axiomatic Attribution for Deep Networks (2017) 1 Rather than calculating only the gradient relative to the image, the method consists of averaging the gradient values along the path from a baseline state to the current value. The baseline state is often set to zero, representing the complete absence of features. More precisely, with \\(x_0\\) the baseline state, \\(x\\) the image and \\(f\\) our classifier, the Integrated Gradient attribution is defined as \\[\\phi = (x - x_0) \\cdot \\int_0^1{ \\nabla_x f(x_0 + \\alpha (x - x_0) ) d\\alpha }\\] In order to approximate from a finite number of steps, the implementation here use the Trapezoidal rule 3 and not a left-Riemann summation, which allows for more accurate results and improved performance. (see the paper below for a comparison of the methods 2 ). Example \u00b6 from xplique.attributions import IntegratedGradients # load images, labels and model # ... method = IntegratedGradients ( model , steps = 50 , baseline_value = 0.0 ) explanations = method . explain ( images , labels ) Notebooks \u00b6 Attribution Methods : Getting started Integrated Gradients : Going Further IntegratedGradients \u00b6 Used to compute the Integrated Gradients, by cumulating the gradients along a path from a baseline to the desired point. __init__( self , model : keras.engine.training.Model , output_layer : Union[str, int, None] = None , batch_size : Union[int, None] = 32 , operator : Union[xplique.commons.operators_operations.Tasks, str , Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float], None] = None, reducer : Union[str, None] = 'mean' , steps : int = 50 , baseline_value : float = 0.0) \u00b6 Parameters model : keras.engine.training.Model The model from which we want to obtain explanations output_layer : Union[str, int, None] = None Layer to target for the outputs (e.g logits or after softmax). If an int is provided it will be interpreted as a layer index. If a string is provided it will look for the layer name. Default to the last layer. It is recommended to use the layer before Softmax. batch_size : Union[int, None] = 32 Number of inputs to explain at once, if None compute all at once. operator : Union[xplique.commons.operators_operations.Tasks, str, Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float], None] = None Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y]. reducer : Union[str, None] = 'mean' String, name of the reducer to use. Either \"min\", \"mean\", \"max\", \"sum\", or None to ignore. Used only for images to obtain explanation with shape (n, h, w, 1). steps : int = 50 Number of points to interpolate between the baseline and the desired point. baseline_value : float = 0.0 Scalar used to create the the baseline point. explain( self , inputs : Union[tf.Dataset, tf.Tensor, ] , targets : Union[tf.Tensor, , None] = None) -> tf.Tensor \u00b6 Compute the explanations of the given inputs. Accept Tensor, numpy array or tf.data.Dataset (in that case targets is None) Parameters inputs : Union[tf.Dataset, tf.Tensor, ] Dataset, Tensor or Array. Input samples to be explained. If Dataset, targets should not be provided (included in Dataset). Expected shape among (N, W), (N, T, W), (N, H, W, C). More information in the documentation. targets : Union[tf.Tensor, , None] = None Tensor or Array. One-hot encoding of the model's output from which an explanation is desired. One encoding per input and only one output at a time. Therefore, the expected shape is (N, output_size). More information in the documentation. Return explanations : tf.Tensor Explanation generated by the method. Axiomatic Attribution for Deep Networks \u21a9 \u21a9 Computing Linear Restrictions of Neural Networks \u21a9 Trapezoidal rule \u21a9","title":"Integrated Gradient"},{"location":"api/attributions/methods/integrated_gradients/#integrated-gradients","text":"View colab tutorial | View source | \ud83d\udcf0 Paper Integrated Gradients is a visualization technique resulting of a theoretical search for an explanatory method that satisfies two axioms, Sensitivity and Implementation Invariance (Sundararajan et al 1 ). Quote We consider the straightline path (in \\(R^n\\) ) from the baseline \\(\\bar{x}\\) to the input \\(x\\) , and compute the gradients at all points along the path. Integrated gradients are obtained by cumulating these gradients. -- Axiomatic Attribution for Deep Networks (2017) 1 Rather than calculating only the gradient relative to the image, the method consists of averaging the gradient values along the path from a baseline state to the current value. The baseline state is often set to zero, representing the complete absence of features. More precisely, with \\(x_0\\) the baseline state, \\(x\\) the image and \\(f\\) our classifier, the Integrated Gradient attribution is defined as \\[\\phi = (x - x_0) \\cdot \\int_0^1{ \\nabla_x f(x_0 + \\alpha (x - x_0) ) d\\alpha }\\] In order to approximate from a finite number of steps, the implementation here use the Trapezoidal rule 3 and not a left-Riemann summation, which allows for more accurate results and improved performance. (see the paper below for a comparison of the methods 2 ).","title":"Integrated Gradients"},{"location":"api/attributions/methods/integrated_gradients/#example","text":"from xplique.attributions import IntegratedGradients # load images, labels and model # ... method = IntegratedGradients ( model , steps = 50 , baseline_value = 0.0 ) explanations = method . explain ( images , labels )","title":"Example"},{"location":"api/attributions/methods/integrated_gradients/#notebooks","text":"Attribution Methods : Getting started Integrated Gradients : Going Further","title":"Notebooks"},{"location":"api/attributions/methods/integrated_gradients/#IntegratedGradients","text":"Used to compute the Integrated Gradients, by cumulating the gradients along a path from a baseline to the desired point.","title":"IntegratedGradients"},{"location":"api/attributions/methods/kernel_shap/","text":"Kernel Shap \u00b6 View colab tutorial | View source By setting appropriately the perturbation function, the similarity kernel and the interpretable model in the LIME framework we can theoretically obtain the Shapley Values more efficiently. Therefore, KernelShap is a method based on LIME with specific attributes. Quote The exact computation of SHAP values is challenging. However, by combining insights from current additive feature attribution methods, we can approximate them. We describe two model-agnostic approximation methods, [...] and another that is novel (Kernel SHAP) -- A Unified Approach to Interpreting Model Predictions 1 Example \u00b6 from xplique.attributions import KernelShap # load images, labels and model # define a custom map_to_interpret_space function # ... method = KernelShap ( model , map_to_interpret_space = custom_map ) explanations = method . explain ( images , labels ) The choice of the map function will have a great deal toward the quality of explanation. By default, the map function use the quickshift segmentation of scikit-images Notebooks \u00b6 Attribution Methods : Getting started KernelShap : Going Further KernelShap \u00b6 By setting appropriately the perturbation function, the similarity kernel and the interpretable model in the LIME framework we can theoretically obtain the Shapley Values more efficiently. Therefore, KernelShap is a method based on LIME with specific attributes. __init__( self , model : Callable , batch_size : int = 64 , operator : Union[xplique.commons.operators_operations.Tasks, str , Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float], None] = None, map_to_interpret_space : Union[Callable, None] = None , nb_samples : int = 800 , ref_value : Union[numpy.ndarray, None] = None) \u00b6 Parameters model : Callable The model from which we want to obtain explanations. batch_size : int = 64 Number of perturbed samples to process at once, mandatory when nb_samples is huge. Notice, it is different compare to WhiteBox explainers which batch the inputs. Here inputs are process one by one. operator : Union[xplique.commons.operators_operations.Tasks, str, Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float], None] = None Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y]. map_to_interpret_space : Union[Callable, None] = None Function which group features of an input corresponding to the same interpretable feature (e.g super-pixel). It allows to transpose from (resp. to) the original input space to (resp. from) the interpretable space. nb_samples : int = 800 The number of perturbed samples you want to generate for each input sample. Default to 800. ref_value : Union[numpy.ndarray, None] = None It defines reference value which replaces each feature when the corresponding interpretable feature is set to 0. It should be provided as: a ndarray of shape (1) if there is no channels in your input and (C,) otherwise. The default ref value is set to (0.5,0.5,0.5) for inputs with 3 channels (corresponding to a grey pixel when inputs are normalized by 255) and to 0 otherwise. explain( self , inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] , targets : Union[tf.Tensor, numpy.ndarray, None] = None) -> tf.Tensor \u00b6 This method attributes the output of the model with given targets to the inputs of the model using the approach described above, training an interpretable model and returning a representation of the interpretable model. Parameters inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] Dataset, Tensor or Array. Input samples to be explained. If Dataset, targets should not be provided (included in Dataset). Expected shape among (N, W), (N, T, W), (N, H, W, C). More information in the documentation. targets : Union[tf.Tensor, numpy.ndarray, None] = None Tensor or Array. One-hot encoding of the model's output from which an explanation is desired. One encoding per input and only one output at a time. Therefore, the expected shape is (N, output_size). More information in the documentation. Return explanations : tf.Tensor Interpretable coefficients, same shape as the inputs, except for the channels. Coefficients of the interpretable model. Those coefficients having the size of the interpretable space will be given the same value to coefficient which were grouped together (e.g belonging to the same super-pixel). Warning The computation time might be very long depending on the hyperparameters settings. A huge number of perturbed samples and a fine-grained mapping may lead to better results but it is long to compute. Parameters in-depth \u00b6 map_to_interpret_space : \u00b6 Function which group features of an input corresponding to the same interpretable feature (e.g super-pixel). It allows to transpose from (resp. to) the original input space to (resp. from) the interpretable space. The default mappings are: - the quickshift segmentation algorithm for inputs with \\((N, W, H, C)\\) shape, we assume here such shape is used to represent \\((W, H, C)\\) images. - the felzenszwalb segmentation algorithm for inputs with \\((N, W, H)\\) shape, we assume here such shape is used to represent \\((W, H)\\) images. - an identity mapping if inputs has shape \\((N, W)\\) or \\((N, T, W)\\) , we assume here your inputs are tabular data or time-series data. To use your own custom map function you should use the following scheme: def custom_map_to_interpret_space ( inputs : tf . tensor ) -> tf . tensor : ** some grouping techniques ** return mappings mappings should have the same dimension as input except for channels . For instance you can use the scikit-image (as we did for the quickshift algorithm) library to defines super pixels on your images. Info The quality of your explanation relies strongly on this mapping. Warning Depending on the mapping you might have a huge number of interpretable_features (e.g you map pixels 2 by 2 on a 299x299 image). Thus, the compuation time might be very long! A Unified Approach to Interpreting Model Predictions \u21a9","title":"KernelSHAP"},{"location":"api/attributions/methods/kernel_shap/#kernel-shap","text":"View colab tutorial | View source By setting appropriately the perturbation function, the similarity kernel and the interpretable model in the LIME framework we can theoretically obtain the Shapley Values more efficiently. Therefore, KernelShap is a method based on LIME with specific attributes. Quote The exact computation of SHAP values is challenging. However, by combining insights from current additive feature attribution methods, we can approximate them. We describe two model-agnostic approximation methods, [...] and another that is novel (Kernel SHAP) -- A Unified Approach to Interpreting Model Predictions 1","title":"Kernel Shap"},{"location":"api/attributions/methods/kernel_shap/#example","text":"from xplique.attributions import KernelShap # load images, labels and model # define a custom map_to_interpret_space function # ... method = KernelShap ( model , map_to_interpret_space = custom_map ) explanations = method . explain ( images , labels ) The choice of the map function will have a great deal toward the quality of explanation. By default, the map function use the quickshift segmentation of scikit-images","title":"Example"},{"location":"api/attributions/methods/kernel_shap/#notebooks","text":"Attribution Methods : Getting started KernelShap : Going Further","title":"Notebooks"},{"location":"api/attributions/methods/kernel_shap/#KernelShap","text":"By setting appropriately the perturbation function, the similarity kernel and the interpretable model in the LIME framework we can theoretically obtain the Shapley Values more efficiently. Therefore, KernelShap is a method based on LIME with specific attributes.","title":"KernelShap"},{"location":"api/attributions/methods/kernel_shap/#parameters-in-depth","text":"","title":"Parameters in-depth"},{"location":"api/attributions/methods/lime/","text":"LIME \u00b6 View colab tutorial | View source The Lime method use an interpretable model to provide an explanation. More specifically, you map inputs ( \\(x \\in R^d\\) ) to an interpretable space (e.g super-pixels) of size num_interpetable_features. From there you generate perturbed interpretable samples ( \\(z' \\in \\{0,1\\}^{num\\_interpretable\\_samples}\\) where \\(1\\) means we keep this specific interpretable feature). Once you have your interpretable samples you can map them back to their original space (the perturbed samples \\(z \\in R^d\\) ) and obtain the label prediction of your model for each perturbed samples. In the Lime method you define a similarity kernel which compute the similarity between an input and its perturbed representations (either in the original input space or in the interpretable space): \\(\\pi_x(z',z)\\) . Finally, you train an interpretable model per input, using interpretable samples along the corresponding perturbed labels and it will draw interpretable samples weighted by the similarity kernel. Thus, you will have an interpretable explanation (i.e in the interpretable space) which can be broadcasted afterwards to the original space considering the mapping you used. Quote The overall goal of LIME is to identify an interpretable model over the interpretable representation that is locally faithful to the classifier. -- \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier. 1 Example \u00b6 from xplique.attributions import Lime # load images, labels and model # define a custom map_to_interpret_space function # ... method = Lime ( model , map_to_interpret_space = custom_map ) explanations = method . explain ( images , labels ) The choice of the interpretable model and the map function will have a great deal toward the quality of explanation. By default, the map function use the quickshift segmentation of scikit-images Notebooks \u00b6 Attribution Methods : Getting started LIME : Going Further Lime \u00b6 Used to compute the LIME method. __init__( self , model : Callable , batch_size : Union[int, None] = None , operator : Union[xplique.commons.operators_operations.Tasks, str , Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float], None] = None, interpretable_model : Any = Ridge(alpha=2) , similarity_kernel : Union[Callable[[tf.Tensor, tf.Tensor, tf.Tensor], tf.Tensor], None] = None , pertub_func : Union[Callable[[Union[int, tf.Tensor], int], tf.Tensor], None] = None , map_to_interpret_space : Union[Callable[[tf.Tensor], tf.Tensor], None] = None , ref_value : Union[numpy.ndarray, None] = None , nb_samples : int = 150 , distance_mode : str = 'euclidean' , kernel_width : float = 45.0 , prob : float = 0.5) \u00b6 Parameters model : Callable The model from which we want to obtain explanations batch_size : Union[int, None] = None Number of perturbed samples to process at once, mandatory when nb_samples is huge. Notice, it is different compare to WhiteBox explainers which batch the inputs. Here inputs are process one by one. operator : Union[xplique.commons.operators_operations.Tasks, str, Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float], None] = None Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y]. interpretable_model : Any = Ridge(alpha=2) Model object to train interpretable model. See the documentation for more information. similarity_kernel : Union[Callable[[tf.Tensor, tf.Tensor, tf.Tensor], tf.Tensor], None] = None Function which considering an input, perturbed instances of these input and the interpretable version of those perturbed samples compute the similarities between the input and the perturbed samples. See the documentation for more information. pertub_func : Union[Callable[[Union[int, tf.Tensor], int], tf.Tensor], None] = None Function which generate perturbed interpretable samples in the interpretation space from the number of interpretable features (e.g nb of super pixel) and the number of perturbed samples you want per original input. See the documentation for more information. ref_value : Union[numpy.ndarray, None] = None It defines reference value which replaces each feature when the corresponding interpretable feature is set to 0. It should be provided as: a ndarray of shape (1) if there is no channels in your input and (C,) otherwise The default ref value is set to (0.5,0.5,0.5) for inputs with 3 channels (corresponding to a grey pixel when inputs are normalized by 255) and to 0 otherwise. map_to_interpret_space : Union[Callable[[tf.Tensor], tf.Tensor], None] = None Function which group features of an input corresponding to the same interpretable feature (e.g super-pixel). It allows to transpose from (resp. to) the original input space to (resp. from) the interpretable space. See the documentation for more information. nb_samples : int = 150 The number of perturbed samples you want to generate for each input sample. Default to 150. prob : float = 0.5 The probability argument for the default pertub function. distance_mode : str = 'euclidean' The distance mode used in the default similarity kernel, you can choose either \"euclidean\" or \"cosine\" (will compute cosine similarity). Default value set to \"euclidean\". kernel_width : float = 45.0 Width of your kernel. It is important to make it evolving depending on your inputs size otherwise you will get all similarity close to 0 leading to poor performance or NaN values. Default to 45 (i.e adapted for RGB images). explain( self , inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] , targets : Union[tf.Tensor, numpy.ndarray, None] = None) -> tf.Tensor \u00b6 This method attributes the output of the model with given targets to the inputs of the model using the approach described above, training an interpretable model and returning a representation of the interpretable model. Parameters inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] Dataset, Tensor or Array. Input samples to be explained. If Dataset, targets should not be provided (included in Dataset). Expected shape among (N, W), (N, T, W), (N, H, W, C). More information in the documentation. targets : Union[tf.Tensor, numpy.ndarray, None] = None Tensor or Array. One-hot encoding of the model's output from which an explanation is desired. One encoding per input and only one output at a time. Therefore, the expected shape is (N, output_size). More information in the documentation. Return explanations : tf.Tensor Interpretable coefficients, same shape as the inputs, except for the channels. Coefficients of the interpretable model. Those coefficients having the size of the interpretable space will be given the same value to coefficient which were grouped together (e.g belonging to the same super-pixel). Warning The computation time might be very long depending on the hyperparameters settings. A huge number of perturbed samples and a fine-grained mapping may lead to better results but it is long to compute. Parameters in-depth \u00b6 interpretable_model : \u00b6 A Model object providing a fit method that train the model with the following inputs: - interpretable_inputs : 2D ndarray of shape ( \\(nb\\_samples\\) x \\(num\\_interp\\_features\\) ), - expected_outputs : 1D ndarray of shape ( \\(nb\\_samples\\) ), - weights : 1D ndarray of shape ( \\(nb\\_samples\\) ) The model object should also provide a predict and fit method. It should also have a coef_ attributes (the interpretable explanations) at least once fit is called. As interpretable model you can use linear models from scikit-learn. Warning Note that here nb_samples doesn't indicates the length of inputs but the number of perturbed samples we want to generate for each input. similarity_kernel : \u00b6 Function which considering an input, perturbed instances of these input and the interpretable version of those perturbed samples compute the similarities between the input and the perturbed samples. Info The similarities can be computed in the original input space or in the interpretable space. You can provide a custom function. Note that to use a custom function, you have to follow the following scheme: def custom_similarity ( original_input , interpret_samples , perturbed_samples ) -> tf . tensor ( shape = ( nb_samples ,), dtype = tf . float32 ): ** some tf actions ** return similarities where: - original_input has shape among \\((W)\\) , \\((W, H)\\) , \\((W, H, C)\\) - interpret_samples is a tf.tensor of shape \\((nb\\_samples, num\\_interp\\_features)\\) - perturbed_samples is a tf.tensor of shape \\((nb\\_samples, *original\\_input.shape)\\) If it is possible you can add the @tf.function decorator. Warning Note that here nb_samples doesn't indicates the length of inputs but the number of perturbed samples we want to generate for each input. Info The default similarity kernel use the euclidean distance between the original input and the perturbed samples in the input space. pertub_func : \u00b6 Function which generate perturbed interpretable samples in the interpretation space from the number of interpretable features (e.g nb of super pixel) and the number of perturbed samples you want per original input. The generated interp_samples belong to \\(\\{0,1\\}^{num\\_features}\\) . Where \\(1\\) indicates that we keep the corresponding feature (e.g super pixel) in the mapping. To use your own custom pertub function you should use the following scheme: @tf . function def custom_pertub_function ( num_features , nb_samples ) -> tf . tensor ( shape = ( nb_samples , num_interp_features ), dtype = tf . int32 ): ** some tf actions ** return perturbed_sample Info The default pertub function provided keep a feature (e.g super pixel) with a probability 0.5. If you want to change it, define the prob value when initiating the explainer or define your own function. map_to_interpret_space : \u00b6 Function which group features of an input corresponding to the same interpretable feature (e.g super-pixel). It allows to transpose from (resp. to) the original input space to (resp. from) the interpretable space. The default mappings are: - the quickshift segmentation algorithm for inputs with \\((N, W, H, C)\\) shape, we assume here such shape is used to represent \\((W, H, C)\\) images. - the felzenszwalb segmentation algorithm for inputs with \\((N, W, H)\\) shape, we assume here such shape is used to represent \\((W, H)\\) images. - an identity mapping if inputs has shape \\((N, W)\\) or \\((N, T, W)\\) , we assume here your inputs are tabular data or time-series data. To use your own custom map function you should use the following scheme: def custom_map_to_interpret_space ( single_inp : tf . tensor ) -> tf . tensor : ** some grouping techniques ** return mapping mapping should have the same dimension as single input except for channels . For instance you can use the scikit-image (as we did for the quickshift algorithm) library to defines super pixels on your images. Info The quality of your explanation relies strongly on this mapping. Warning Depending on the mapping you might have a huge number of interpretable_features (e.g you map pixels 2 by 2 on a 299x299 image). Thus, the compuation time might be very long! \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier. \u21a9","title":"Lime"},{"location":"api/attributions/methods/lime/#lime","text":"View colab tutorial | View source The Lime method use an interpretable model to provide an explanation. More specifically, you map inputs ( \\(x \\in R^d\\) ) to an interpretable space (e.g super-pixels) of size num_interpetable_features. From there you generate perturbed interpretable samples ( \\(z' \\in \\{0,1\\}^{num\\_interpretable\\_samples}\\) where \\(1\\) means we keep this specific interpretable feature). Once you have your interpretable samples you can map them back to their original space (the perturbed samples \\(z \\in R^d\\) ) and obtain the label prediction of your model for each perturbed samples. In the Lime method you define a similarity kernel which compute the similarity between an input and its perturbed representations (either in the original input space or in the interpretable space): \\(\\pi_x(z',z)\\) . Finally, you train an interpretable model per input, using interpretable samples along the corresponding perturbed labels and it will draw interpretable samples weighted by the similarity kernel. Thus, you will have an interpretable explanation (i.e in the interpretable space) which can be broadcasted afterwards to the original space considering the mapping you used. Quote The overall goal of LIME is to identify an interpretable model over the interpretable representation that is locally faithful to the classifier. -- \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier. 1","title":"LIME"},{"location":"api/attributions/methods/lime/#example","text":"from xplique.attributions import Lime # load images, labels and model # define a custom map_to_interpret_space function # ... method = Lime ( model , map_to_interpret_space = custom_map ) explanations = method . explain ( images , labels ) The choice of the interpretable model and the map function will have a great deal toward the quality of explanation. By default, the map function use the quickshift segmentation of scikit-images","title":"Example"},{"location":"api/attributions/methods/lime/#notebooks","text":"Attribution Methods : Getting started LIME : Going Further","title":"Notebooks"},{"location":"api/attributions/methods/lime/#Lime","text":"Used to compute the LIME method.","title":"Lime"},{"location":"api/attributions/methods/lime/#parameters-in-depth","text":"","title":"Parameters in-depth"},{"location":"api/attributions/methods/occlusion/","text":"Occlusion sensitivity \u00b6 View colab tutorial | View source The Occlusion sensitivity method sweep a patch that occludes pixels over the images, and use the variations of the model prediction to deduce critical areas. 1 Quote [...] this method, referred to as Occlusion, replacing one feature \\(x_i\\) at the time with a baseline and measuring the effect of this perturbation on the target output. -- Towards better understanding of the gradient-based attribution methods for Deep Neural Networks (2017) 2 with \\(S_c\\) the unormalized class score (layer before softmax) and \\(\\bar{x}\\) a baseline, the Occlusion sensitivity map \\(\\phi\\) is defined as : \\[ \\phi_i = S_c(x) - S_c(x_{[x_i = \\bar{x}]}) \\] Example \u00b6 from xplique.attributions import Occlusion # load images, labels and model # ... method = Occlusion ( model , patch_size = ( 10 , 10 ), patch_stride = ( 2 , 2 ), occlusion_value = 0.5 ) explanations = method . explain ( images , labels ) Notebooks \u00b6 Attribution Methods : Getting started Occlusion : Going Further Occlusion \u00b6 Used to compute the Occlusion sensitivity method, sweep a patch that occludes pixels over the images and use the variations of the model prediction to deduce critical areas. __init__( self , model : Callable , batch_size : Union[int, None] = 32 , operator : Union[xplique.commons.operators_operations.Tasks, str , Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float], None] = None, patch_size : Union[int, Tuple[int, int]] = 3 , patch_stride : Union[int, Tuple[int, int]] = 3 , occlusion_value : float = 0.0) \u00b6 Parameters model : Callable The model from which we want to obtain explanations batch_size : Union[int, None] = 32 Number of pertubed samples to explain at once. Default to 32. operator : Union[xplique.commons.operators_operations.Tasks, str, Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float], None] = None Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y]. patch_size : Union[int, Tuple[int, int]] = 3 Size of the patches to apply, if integer then assume an hypercube. patch_stride : Union[int, Tuple[int, int]] = 3 Stride between two patches, if integer then assume an hypercube. occlusion_value : float = 0.0 Value used as occlusion. explain( self , inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] , targets : Union[tf.Tensor, numpy.ndarray, None] = None) -> tf.Tensor \u00b6 Compute Occlusion sensitivity for a batch of samples. Parameters inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] Dataset, Tensor or Array. Input samples to be explained. If Dataset, targets should not be provided (included in Dataset). Expected shape among (N, W), (N, T, W), (N, H, W, C). More information in the documentation. targets : Union[tf.Tensor, numpy.ndarray, None] = None Tensor or Array. One-hot encoding of the model's output from which an explanation is desired. One encoding per input and only one output at a time. Therefore, the expected shape is (N, output_size). More information in the documentation. Return explanations : tf.Tensor Occlusion sensitivity, same shape as the inputs, except for the channels. Info patch_size and patch_stride will define patch to apply to the original input. Thus, a combination of patches will generate pertubed samples of the original input (masked by patches with occlusion_value value). Consequently, the number of pertubed instances of an input depend on those parameters. Too little value of those two arguments on large image might lead to an incredible amount of pertubed samples and increase compuation time. On another hand too huge values might not be accurate enough. Visualizing and Understanding Convolutional Networks (2014). \u21a9 Towards better understanding of gradient-based attribution methods for Deep Neural Networks \u21a9","title":"Occlusion sensitivity"},{"location":"api/attributions/methods/occlusion/#occlusion-sensitivity","text":"View colab tutorial | View source The Occlusion sensitivity method sweep a patch that occludes pixels over the images, and use the variations of the model prediction to deduce critical areas. 1 Quote [...] this method, referred to as Occlusion, replacing one feature \\(x_i\\) at the time with a baseline and measuring the effect of this perturbation on the target output. -- Towards better understanding of the gradient-based attribution methods for Deep Neural Networks (2017) 2 with \\(S_c\\) the unormalized class score (layer before softmax) and \\(\\bar{x}\\) a baseline, the Occlusion sensitivity map \\(\\phi\\) is defined as : \\[ \\phi_i = S_c(x) - S_c(x_{[x_i = \\bar{x}]}) \\]","title":"Occlusion sensitivity"},{"location":"api/attributions/methods/occlusion/#example","text":"from xplique.attributions import Occlusion # load images, labels and model # ... method = Occlusion ( model , patch_size = ( 10 , 10 ), patch_stride = ( 2 , 2 ), occlusion_value = 0.5 ) explanations = method . explain ( images , labels )","title":"Example"},{"location":"api/attributions/methods/occlusion/#notebooks","text":"Attribution Methods : Getting started Occlusion : Going Further","title":"Notebooks"},{"location":"api/attributions/methods/occlusion/#Occlusion","text":"Used to compute the Occlusion sensitivity method, sweep a patch that occludes pixels over the images and use the variations of the model prediction to deduce critical areas.","title":"Occlusion"},{"location":"api/attributions/methods/rise/","text":"RISE \u00b6 View colab tutorial | View source | \ud83d\udcf0 Paper The RISE method consist of probing the model with randomly masked versions of the input image and obtaining the corresponding outputs to deduce critical areas. Quote [...] we estimate the importance of pixels by dimming them in random combinations, reducing their intensities down to zero. We model this by multiplying an image with a [0,1] valued mask. -- RISE: Randomized Input Sampling for Explanation of Black-box Models (2018) 1 with \\(f(x)\\) the prediction of a classifier, for an input \\(x\\) and \\(m \\sim \\mathcal{M}\\) a mask with value in \\([0,1]\\) created from a low dimension ( \\(m\\) is in \\({0, 1}^{w \\times h}\\) with \\(w \\ll W\\) and \\(h \\ll H\\) then upsampled, see the paper for more details). The RISE importance estimator is defined as: \\[ \\phi_i = \\mathbb{E}( f(x \\odot m) | m_i = 1) \\approx \\frac{1}{\\mathbb{E}(\\mathcal{M}) N} \\sum_{i=1}^N f(x \\odot m_i) m_i \\] The most important parameters here are (1) the grid_size that control \\(w\\) and \\(h\\) and (2) nb_samples that control \\(N\\) . The pourcentage of visible pixels \\(\\mathbb{E}(\\mathcal{M})\\) is controlled using the preservation_probability parameter. Example \u00b6 from xplique.attributions import Rise # load images, labels and model # ... method = Rise ( model , nb_samples = 4000 , grid_size = 7 , preservation_probability = 0.5 ) explanations = method . explain ( images , labels ) Notebooks \u00b6 Attribution Methods : Getting started RISE : Going Further Rise \u00b6 Used to compute the RISE method, by probing the model with randomly masked versions of the input image and obtaining the corresponding outputs to deduce critical areas. __init__( self , model : Callable , batch_size : Union[int, None] = 32 , operator : Union[xplique.commons.operators_operations.Tasks, str , Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float], None] = None, nb_samples : int = 4000 , grid_size : Union[int, Tuple[int]] = 7 , preservation_probability : float = 0.5 , mask_value : float = 0.0) \u00b6 Parameters model : Callable The model from which we want to obtain explanations batch_size : Union[int, None] = 32 Number of pertubed samples to explain at once. Default to 32. operator : Union[xplique.commons.operators_operations.Tasks, str, Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float], None] = None Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y]. nb_samples : int = 4000 Number of masks generated for Monte Carlo sampling. grid_size : Union[int, Tuple[int]] = 7 Size of the grid used to generate the scaled-down masks. Masks are then rescale to and cropped to input_size. Can be a tuple for different cutting depending on the dimension. Ignored for tabular data. preservation_probability : float = 0.5 Probability of preservation for each pixel (or the percentage of non-masked pixels in each masks), also the expectation value of the mask. mask_value : float = 0.0 Value used as when applying masks. explain( self , inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] , targets : Union[tf.Tensor, numpy.ndarray, None] = None) -> tf.Tensor \u00b6 Compute RISE for a batch of samples. Parameters inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] Dataset, Tensor or Array. Input samples to be explained. If Dataset, targets should not be provided (included in Dataset). Expected shape among (N, W), (N, T, W), (N, H, W, C). More information in the documentation. targets : Union[tf.Tensor, numpy.ndarray, None] = None Tensor or Array. One-hot encoding of the model's output from which an explanation is desired. One encoding per input and only one output at a time. Therefore, the expected shape is (N, output_size). More information in the documentation. Return explanations : tf.Tensor RISE maps, same shape as the inputs, except for the channels. RISE: Randomized Input Sampling for Explanation of Black-box Models (2018) \u21a9","title":"Rise"},{"location":"api/attributions/methods/rise/#rise","text":"View colab tutorial | View source | \ud83d\udcf0 Paper The RISE method consist of probing the model with randomly masked versions of the input image and obtaining the corresponding outputs to deduce critical areas. Quote [...] we estimate the importance of pixels by dimming them in random combinations, reducing their intensities down to zero. We model this by multiplying an image with a [0,1] valued mask. -- RISE: Randomized Input Sampling for Explanation of Black-box Models (2018) 1 with \\(f(x)\\) the prediction of a classifier, for an input \\(x\\) and \\(m \\sim \\mathcal{M}\\) a mask with value in \\([0,1]\\) created from a low dimension ( \\(m\\) is in \\({0, 1}^{w \\times h}\\) with \\(w \\ll W\\) and \\(h \\ll H\\) then upsampled, see the paper for more details). The RISE importance estimator is defined as: \\[ \\phi_i = \\mathbb{E}( f(x \\odot m) | m_i = 1) \\approx \\frac{1}{\\mathbb{E}(\\mathcal{M}) N} \\sum_{i=1}^N f(x \\odot m_i) m_i \\] The most important parameters here are (1) the grid_size that control \\(w\\) and \\(h\\) and (2) nb_samples that control \\(N\\) . The pourcentage of visible pixels \\(\\mathbb{E}(\\mathcal{M})\\) is controlled using the preservation_probability parameter.","title":"RISE"},{"location":"api/attributions/methods/rise/#example","text":"from xplique.attributions import Rise # load images, labels and model # ... method = Rise ( model , nb_samples = 4000 , grid_size = 7 , preservation_probability = 0.5 ) explanations = method . explain ( images , labels )","title":"Example"},{"location":"api/attributions/methods/rise/#notebooks","text":"Attribution Methods : Getting started RISE : Going Further","title":"Notebooks"},{"location":"api/attributions/methods/rise/#Rise","text":"Used to compute the RISE method, by probing the model with randomly masked versions of the input image and obtaining the corresponding outputs to deduce critical areas.","title":"Rise"},{"location":"api/attributions/methods/saliency/","text":"Saliency Maps \u00b6 View colab tutorial | View source | \ud83d\udcf0 Paper Saliency is one of the most easy explanation method based on the gradient of a class score relative to the input. Quote An interpretation of computing the image-specific class saliency using the class score derivative is that the magnitude of the derivative indicates which pixels need to be changed the least to affect the class score the most. One can expect that such pixels correspond to the object location in the image. -- Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps (2013) 1 More precisely, for an image \\(x\\) the importance map \\(\\phi\\) according to a classifier \\(f\\) is defined as: \\[ \\phi = | \\nabla_{x} f(x) | \\] more precisely, in the image case, Xplique is faithful to the original method and returns the max on the axis of channels, with \\(\\phi_i \\in \\mathbb{R}^3\\) for RGB, the importance for the pixel \\(i\\) is given by \\(||\\phi_i||_{\\infty}\\) Example \u00b6 from xplique.attributions import Saliency # load images, labels and model # ... method = Saliency ( model ) explanations = method . explain ( images , labels ) Notebooks \u00b6 Attribution Methods : Getting started Saliency : Going Further Saliency \u00b6 Used to compute the absolute gradient of the output relative to the input. __init__( self , model : keras.engine.training.Model , output_layer : Union[str, int, None] = None , batch_size : Union[int, None] = 32 , operator : Union[xplique.commons.operators_operations.Tasks, str , Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float], None] = None, reducer : Union[str, None] = 'max') \u00b6 Parameters model : keras.engine.training.Model The model from which we want to obtain explanations output_layer : Union[str, int, None] = None Layer to target for the outputs (e.g logits or after softmax). If an int is provided it will be interpreted as a layer index. If a string is provided it will look for the layer name. Default to the last layer. It is recommended to use the layer before Softmax. batch_size : Union[int, None] = 32 Number of inputs to explain at once, if None compute all at once. operator : Union[xplique.commons.operators_operations.Tasks, str, Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float], None] = None Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y]. reducer : Union[str, None] = 'max' String, name of the reducer to use. Either \"min\", \"mean\", \"max\", \"sum\", or None to ignore. Used only for images to obtain explanation with shape (n, h, w, 1). Maximum is taking by default to match the initial paper. explain( self , inputs : Union[tf.Dataset, tf.Tensor, ] , targets : Union[tf.Tensor, , None] = None) -> tf.Tensor \u00b6 Compute the explanations of the given inputs. Accept Tensor, numpy array or tf.data.Dataset (in that case targets is None) Parameters inputs : Union[tf.Dataset, tf.Tensor, ] Dataset, Tensor or Array. Input samples to be explained. If Dataset, targets should not be provided (included in Dataset). Expected shape among (N, W), (N, T, W), (N, H, W, C). More information in the documentation. targets : Union[tf.Tensor, , None] = None Tensor or Array. One-hot encoding of the model's output from which an explanation is desired. One encoding per input and only one output at a time. Therefore, the expected shape is (N, output_size). More information in the documentation. Return explanations : tf.Tensor Explanation generated by the method. Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps \u21a9","title":"Saliency"},{"location":"api/attributions/methods/saliency/#saliency-maps","text":"View colab tutorial | View source | \ud83d\udcf0 Paper Saliency is one of the most easy explanation method based on the gradient of a class score relative to the input. Quote An interpretation of computing the image-specific class saliency using the class score derivative is that the magnitude of the derivative indicates which pixels need to be changed the least to affect the class score the most. One can expect that such pixels correspond to the object location in the image. -- Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps (2013) 1 More precisely, for an image \\(x\\) the importance map \\(\\phi\\) according to a classifier \\(f\\) is defined as: \\[ \\phi = | \\nabla_{x} f(x) | \\] more precisely, in the image case, Xplique is faithful to the original method and returns the max on the axis of channels, with \\(\\phi_i \\in \\mathbb{R}^3\\) for RGB, the importance for the pixel \\(i\\) is given by \\(||\\phi_i||_{\\infty}\\)","title":"Saliency Maps"},{"location":"api/attributions/methods/saliency/#example","text":"from xplique.attributions import Saliency # load images, labels and model # ... method = Saliency ( model ) explanations = method . explain ( images , labels )","title":"Example"},{"location":"api/attributions/methods/saliency/#notebooks","text":"Attribution Methods : Getting started Saliency : Going Further","title":"Notebooks"},{"location":"api/attributions/methods/saliency/#Saliency","text":"Used to compute the absolute gradient of the output relative to the input.","title":"Saliency"},{"location":"api/attributions/methods/smoothgrad/","text":"SmoothGrad \u00b6 View colab tutorial | View source | \ud83d\udcf0 Paper SmoothGrad is a gradient-based explanation method, which, as the name suggests, averages the gradient at several points corresponding to small perturbations around the point of interest. The smoothing effect induced by the average help reducing the visual noise, and hence improve the explanations. Quote [...] The gradient at any given point will be less meaningful than a local average of gradient values. This suggests a new way to create improved sensitivity maps: instead of basing a visualization directly on the gradient, we could base it on a smoothing of the gradients with a Gaussian kernel. -- SmoothGrad: removing noise by adding noise (2017) 1 More precisely, the explanation \\(\\phi\\) for an input \\(x\\) and a classifier \\(f\\) is defined as \\[ \\phi = \\mathbb{E}_{\\delta ~\\sim~ \\mathcal{N}(0, \\sigma^2) }( \\nabla_x f(x + \\delta) ) \\approx \\frac{1}{N} \\sum_{i=0}^N \\nabla_x f(x + \\delta_i) \\] The \\(\\sigma\\) in the formula is controlled using the noise parameter, and the expectation is estimated using \\(N\\) samples controlled by the nb_samples parameter. Tip It is recommended to have a noise level \\(\\sigma\\) at about 20% of the range of your inputs, i.e. \\(\\sigma=0.2\\) if your inputs are between \\([0, 1]\\) or \\(\\sigma=0.4\\) if your inputs are between \\([-1, 1]\\) . Example \u00b6 from xplique.attributions import SmoothGrad # load images, labels and model # ... method = SmoothGrad ( model , nb_samples = 50 , noise = 0.15 ) explanations = method . explain ( images , labels ) Notebooks \u00b6 Attribution Methods : Getting started SmoothGrad : Going Further SmoothGrad \u00b6 Used to compute the SmoothGrad, by averaging Saliency maps of noisy samples centered on the original sample. __init__( self , model : keras.engine.training.Model , output_layer : Union[str, int, None] = None , batch_size : Union[int, None] = 32 , operator : Union[xplique.commons.operators_operations.Tasks, str , Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float], None] = None, reducer : Union[str, None] = 'mean' , nb_samples : int = 50 , noise : float = 0.2) \u00b6 Parameters model : keras.engine.training.Model The model from which we want to obtain explanations output_layer : Union[str, int, None] = None Layer to target for the outputs (e.g logits or after softmax). If an int is provided it will be interpreted as a layer index. If a string is provided it will look for the layer name. Default to the last layer. It is recommended to use the layer before Softmax. batch_size : Union[int, None] = 32 Number of inputs to explain at once, if None compute all at once. operator : Union[xplique.commons.operators_operations.Tasks, str, Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float], None] = None Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y]. reducer : Union[str, None] = 'mean' String, name of the reducer to use. Either \"min\", \"mean\", \"max\", \"sum\", or None to ignore. Used only for images to obtain explanation with shape (n, h, w, 1). nb_samples : int = 50 Number of noisy samples generated for the smoothing procedure. noise : float = 0.2 Scalar, noise used as standard deviation of a normal law centered on zero. explain( self , inputs : Union[tf.Dataset, tf.Tensor, ] , targets : Union[tf.Tensor, , None] = None) -> tf.Tensor \u00b6 Compute the explanations of the given inputs. Accept Tensor, numpy array or tf.data.Dataset (in that case targets is None) Parameters inputs : Union[tf.Dataset, tf.Tensor, ] Dataset, Tensor or Array. Input samples to be explained. If Dataset, targets should not be provided (included in Dataset). Expected shape among (N, W), (N, T, W), (N, H, W, C). More information in the documentation. targets : Union[tf.Tensor, , None] = None Tensor or Array. One-hot encoding of the model's output from which an explanation is desired. One encoding per input and only one output at a time. Therefore, the expected shape is (N, output_size). More information in the documentation. Return explanations : tf.Tensor Explanation generated by the method. SmoothGrad: removing noise by adding noise (2017) \u21a9","title":"SmoothGrad"},{"location":"api/attributions/methods/smoothgrad/#smoothgrad","text":"View colab tutorial | View source | \ud83d\udcf0 Paper SmoothGrad is a gradient-based explanation method, which, as the name suggests, averages the gradient at several points corresponding to small perturbations around the point of interest. The smoothing effect induced by the average help reducing the visual noise, and hence improve the explanations. Quote [...] The gradient at any given point will be less meaningful than a local average of gradient values. This suggests a new way to create improved sensitivity maps: instead of basing a visualization directly on the gradient, we could base it on a smoothing of the gradients with a Gaussian kernel. -- SmoothGrad: removing noise by adding noise (2017) 1 More precisely, the explanation \\(\\phi\\) for an input \\(x\\) and a classifier \\(f\\) is defined as \\[ \\phi = \\mathbb{E}_{\\delta ~\\sim~ \\mathcal{N}(0, \\sigma^2) }( \\nabla_x f(x + \\delta) ) \\approx \\frac{1}{N} \\sum_{i=0}^N \\nabla_x f(x + \\delta_i) \\] The \\(\\sigma\\) in the formula is controlled using the noise parameter, and the expectation is estimated using \\(N\\) samples controlled by the nb_samples parameter. Tip It is recommended to have a noise level \\(\\sigma\\) at about 20% of the range of your inputs, i.e. \\(\\sigma=0.2\\) if your inputs are between \\([0, 1]\\) or \\(\\sigma=0.4\\) if your inputs are between \\([-1, 1]\\) .","title":"SmoothGrad"},{"location":"api/attributions/methods/smoothgrad/#example","text":"from xplique.attributions import SmoothGrad # load images, labels and model # ... method = SmoothGrad ( model , nb_samples = 50 , noise = 0.15 ) explanations = method . explain ( images , labels )","title":"Example"},{"location":"api/attributions/methods/smoothgrad/#notebooks","text":"Attribution Methods : Getting started SmoothGrad : Going Further","title":"Notebooks"},{"location":"api/attributions/methods/smoothgrad/#SmoothGrad","text":"Used to compute the SmoothGrad, by averaging Saliency maps of noisy samples centered on the original sample.","title":"SmoothGrad"},{"location":"api/attributions/methods/sobol/","text":"Sobol Attribution Method \u00b6 View colab tutorial | View source | \ud83d\udcf0 Paper The Sobol attribution method from Fel, Cad\u00e8ne & al. 1 is an attribution method grounded in Sensitivity Analysis. Beyond modeling the individual contributions of image regions, Sobol indices provide an efficient way to capture higher-order interactions between image regions and their contributions to a neural network\u2019s prediction through the lens of variance. Quote The total Sobol index \\(ST_i\\) which measures the contribution of the variable \\(X_i\\) as well as its interactions of any order with any other input variables to the model output variance. -- Look at the Variance! Efficient Black-box Explanations with Sobol-based Sensitivity Analysis (2021) 1 More precisely, the attribution score \\(\\phi_i\\) for an input variable \\(x_i\\) , is defined as \\[ \\phi_i = \\frac{\\mathbb{E}_{X \\sim i}(Var_{X_i}(f(x) | X_{\\sim i}))} {Var (f(X ))} \\] Where \\(\\mathbb{E}_{X \\sim i}(Var_{X_i}(f(x) | X_{\\sim i}))\\) is the expected variance that would be left if all variables but \\(X_{\\sim i}\\) were to be fixed. In order to generate stochasticity( \\(X_i\\) ), a perturbation function is used and uses perturbation masks to modulate the generated perturbation. The perturbation functions available are inpainting that modulates pixel regions to a baseline state, amplitude and blurring. The calculation of the indices also requires an estimator -- in practice this parameter does not change the results much -- JansenEstimator being recommended. Finally the exploration of the manifold exploration is made using a sampling method, several samplers are proposed: Quasi-Monte Carlo ( ScipySobolSequence , recommended) using Scipy's sobol sequence, Latin hypercubes -- LHSAmpler -- or Halton's sequences HaltonSequence . Tip For quick a faithful explanations, we recommend to use grid_size in \\([7, 12)\\) , nb_design in \\(\\{16, 32, 64\\}\\) (more is useless), and a QMC sampler. (see SobolAttributionMethod documentation below for detail on those parameters). Example \u00b6 from xplique.attributions import SobolAttributionMethod from xplique.attributions.global_sensitivity_analysis import ( JansenEstimator , GlenEstimator , LHSampler , ScipySobolSequence , HaltonSequence ) # load images, labels and model # ... # default explainer (recommended) explainer = SobolAttributionMethod ( model , grid_size = 8 , nb_design = 32 ) explanations = method ( images , labels ) # one-hot encoded labels If you want to change the estimator or the sampling: from xplique.attributions import SobolAttributionMethod from xplique.attributions.global_sensitivity_analysis import ( JansenEstimator , GlenEstimator , LHSampler , ScipySobolSequence , HaltonSequence ) # load images, labels and model # ... explainer_lhs = SobolAttributionMethod ( model , grid_size = 8 , nb_design = 32 , sampler = LHSampler (), estimator = GlenEstimator ()) explanations_lhs = explainer_lhs ( images , labels ) Notebooks \u00b6 Attribution Methods : Getting started SobolAttributionMethod \u00b6 Sobol' Attribution Method. Compute the total order Sobol' indices using a perturbation function on a grid and an adapted sampling as described in the original paper. __init__( self , model, grid_size : int = 8 , nb_design : int = 32 , sampler : Union[xplique.attributions.global_sensitivity_analysis.replicated_designs.ReplicatedSampler, None] = None , estimator : Union[xplique.attributions.global_sensitivity_analysis.sobol_estimators.SobolEstimator, None] = None , perturbation_function : Union[Callable, str, None] = 'inpainting' , batch_size=256, operator : Union[xplique.commons.operators_operations.Tasks, str , Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float], None] = None) \u00b6 Parameters model : model Model used for computing explanations. grid_size : int = 8 Cut the image in a grid of (grid_size, grid_size) to estimate an indice per cell. nb_design : int = 32 Must be a power of two. Number of design, the number of forward will be: nb_design * (grid_size**2 + 2). Generally not above 32. sampler : Union[xplique.attributions.global_sensitivity_analysis.replicated_designs.ReplicatedSampler, None] = None Sampler used to generate the (quasi-)monte carlo samples, QMC (sobol sequence recommended). For more option, see the sampler module. estimator : Union[xplique.attributions.global_sensitivity_analysis.sobol_estimators.SobolEstimator, None] = None Estimator used to compute the total order sobol' indices, Jansen recommended. For more option, see the estimator module. perturbation_function : Union[Callable, str, None] = 'inpainting' Function to call to apply the perturbation on the input. Can also be string: 'inpainting', 'blurring', or 'amplitude'. batch_size : batch_size=256 Batch size to use for the forwards. operator : Union[xplique.commons.operators_operations.Tasks, str, Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float], None] = None Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y]. explain( self , inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] , targets : Union[tf.Tensor, numpy.ndarray, None] = None) -> tf.Tensor \u00b6 Compute the total Sobol' indices according to the explainer parameter (perturbation function, grid size...). Accept Tensor, numpy array or tf.data.Dataset (in that case targets is None). Parameters inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] Images to be explained, either tf.dataset, Tensor or numpy array. If Dataset, targets should not be provided (included in Dataset). Expected shape (N, W, H, C) or (N, W, H). targets : Union[tf.Tensor, numpy.ndarray, None] = None One-hot encoding for classification or direction {-1, +1} for regression. Tensor or numpy array. Expected shape (N, C) or (N). Return attributions_maps : tf.Tensor GSA Attribution Method explanations, same shape as the inputs except for the channels. Look at the Variance! Efficient Black-box Explanations with Sobol-based Sensitivity Analysis (2021) \u21a9 \u21a9","title":"Sobol Attribution Method"},{"location":"api/attributions/methods/sobol/#sobol-attribution-method","text":"View colab tutorial | View source | \ud83d\udcf0 Paper The Sobol attribution method from Fel, Cad\u00e8ne & al. 1 is an attribution method grounded in Sensitivity Analysis. Beyond modeling the individual contributions of image regions, Sobol indices provide an efficient way to capture higher-order interactions between image regions and their contributions to a neural network\u2019s prediction through the lens of variance. Quote The total Sobol index \\(ST_i\\) which measures the contribution of the variable \\(X_i\\) as well as its interactions of any order with any other input variables to the model output variance. -- Look at the Variance! Efficient Black-box Explanations with Sobol-based Sensitivity Analysis (2021) 1 More precisely, the attribution score \\(\\phi_i\\) for an input variable \\(x_i\\) , is defined as \\[ \\phi_i = \\frac{\\mathbb{E}_{X \\sim i}(Var_{X_i}(f(x) | X_{\\sim i}))} {Var (f(X ))} \\] Where \\(\\mathbb{E}_{X \\sim i}(Var_{X_i}(f(x) | X_{\\sim i}))\\) is the expected variance that would be left if all variables but \\(X_{\\sim i}\\) were to be fixed. In order to generate stochasticity( \\(X_i\\) ), a perturbation function is used and uses perturbation masks to modulate the generated perturbation. The perturbation functions available are inpainting that modulates pixel regions to a baseline state, amplitude and blurring. The calculation of the indices also requires an estimator -- in practice this parameter does not change the results much -- JansenEstimator being recommended. Finally the exploration of the manifold exploration is made using a sampling method, several samplers are proposed: Quasi-Monte Carlo ( ScipySobolSequence , recommended) using Scipy's sobol sequence, Latin hypercubes -- LHSAmpler -- or Halton's sequences HaltonSequence . Tip For quick a faithful explanations, we recommend to use grid_size in \\([7, 12)\\) , nb_design in \\(\\{16, 32, 64\\}\\) (more is useless), and a QMC sampler. (see SobolAttributionMethod documentation below for detail on those parameters).","title":"Sobol Attribution Method"},{"location":"api/attributions/methods/sobol/#example","text":"from xplique.attributions import SobolAttributionMethod from xplique.attributions.global_sensitivity_analysis import ( JansenEstimator , GlenEstimator , LHSampler , ScipySobolSequence , HaltonSequence ) # load images, labels and model # ... # default explainer (recommended) explainer = SobolAttributionMethod ( model , grid_size = 8 , nb_design = 32 ) explanations = method ( images , labels ) # one-hot encoded labels If you want to change the estimator or the sampling: from xplique.attributions import SobolAttributionMethod from xplique.attributions.global_sensitivity_analysis import ( JansenEstimator , GlenEstimator , LHSampler , ScipySobolSequence , HaltonSequence ) # load images, labels and model # ... explainer_lhs = SobolAttributionMethod ( model , grid_size = 8 , nb_design = 32 , sampler = LHSampler (), estimator = GlenEstimator ()) explanations_lhs = explainer_lhs ( images , labels )","title":"Example"},{"location":"api/attributions/methods/sobol/#notebooks","text":"Attribution Methods : Getting started","title":"Notebooks"},{"location":"api/attributions/methods/sobol/#SobolAttributionMethod","text":"Sobol' Attribution Method. Compute the total order Sobol' indices using a perturbation function on a grid and an adapted sampling as described in the original paper.","title":"SobolAttributionMethod"},{"location":"api/attributions/methods/square_grad/","text":"Square Grad \u00b6 View colab tutorial | View source | \ud83d\udcf0 Paper Similar to SmoothGrad, Square-Grad is a gradient-based explanation method, which, as the name suggests, averages the square of the gradient at several points corresponding to small perturbations around the point of interest. The smoothing effect induced by the average help reducing the visual noise, and hence improve the explanations. More precisely, the explanation \\(\\phi\\) for an input \\(x\\) and a classifier \\(f\\) is defined as \\[ \\phi = \\mathbb{E}_{\\delta ~\\sim~ \\mathcal{N}(0, \\sigma^2) }( (\\nabla_x f(x + \\delta))^2 ) \\approx \\frac{1}{N} \\sum_{i=0}^N (\\nabla_x f(x + \\delta_i))^2 \\] The \\(\\sigma\\) in the formula is controlled using the noise parameter, and the expectation is estimated using \\(N\\) samples controlled by the nb_samples parameter. Tip It is recommended to have a noise level \\(\\sigma\\) at about 20% of the range of your inputs, i.e. \\(\\sigma=0.2\\) if your inputs are between \\([0, 1]\\) or \\(\\sigma=0.4\\) if your inputs are between \\([-1, 1]\\) . Example \u00b6 from xplique.attributions import SquareGrad # load images, labels and model # ... method = SquareGrad ( model , nb_samples = 50 , noise = 0.15 ) explanations = method . explain ( images , labels ) Notebooks \u00b6 Attribution Methods : Getting started SquareGrad : Going Further SquareGrad \u00b6 SquareGrad (or SmoothGrad^2) is an unpublished variant of classic SmoothGrad which squares each gradients of the noisy inputs before averaging. __init__( self , model : keras.engine.training.Model , output_layer : Union[str, int, None] = None , batch_size : Union[int, None] = 32 , operator : Union[xplique.commons.operators_operations.Tasks, str , Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float], None] = None, reducer : Union[str, None] = 'mean' , nb_samples : int = 50 , noise : float = 0.2) \u00b6 Parameters model : keras.engine.training.Model The model from which we want to obtain explanations output_layer : Union[str, int, None] = None Layer to target for the outputs (e.g logits or after softmax). If an int is provided it will be interpreted as a layer index. If a string is provided it will look for the layer name. Default to the last layer. It is recommended to use the layer before Softmax. batch_size : Union[int, None] = 32 Number of inputs to explain at once, if None compute all at once. operator : Union[xplique.commons.operators_operations.Tasks, str, Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float], None] = None Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y]. reducer : Union[str, None] = 'mean' String, name of the reducer to use. Either \"min\", \"mean\", \"max\", \"sum\", or None to ignore. Used only for images to obtain explanation with shape (n, h, w, 1). nb_samples : int = 50 Number of noisy samples generated for the smoothing procedure. noise : float = 0.2 Scalar, noise used as standard deviation of a normal law centered on zero. explain( self , inputs : Union[tf.Dataset, tf.Tensor, ] , targets : Union[tf.Tensor, , None] = None) -> tf.Tensor \u00b6 Compute the explanations of the given inputs. Accept Tensor, numpy array or tf.data.Dataset (in that case targets is None) Parameters inputs : Union[tf.Dataset, tf.Tensor, ] Dataset, Tensor or Array. Input samples to be explained. If Dataset, targets should not be provided (included in Dataset). Expected shape among (N, W), (N, T, W), (N, H, W, C). More information in the documentation. targets : Union[tf.Tensor, , None] = None Tensor or Array. One-hot encoding of the model's output from which an explanation is desired. One encoding per input and only one output at a time. Therefore, the expected shape is (N, output_size). More information in the documentation. Return explanations : tf.Tensor Explanation generated by the method.","title":"SquareGrad"},{"location":"api/attributions/methods/square_grad/#square-grad","text":"View colab tutorial | View source | \ud83d\udcf0 Paper Similar to SmoothGrad, Square-Grad is a gradient-based explanation method, which, as the name suggests, averages the square of the gradient at several points corresponding to small perturbations around the point of interest. The smoothing effect induced by the average help reducing the visual noise, and hence improve the explanations. More precisely, the explanation \\(\\phi\\) for an input \\(x\\) and a classifier \\(f\\) is defined as \\[ \\phi = \\mathbb{E}_{\\delta ~\\sim~ \\mathcal{N}(0, \\sigma^2) }( (\\nabla_x f(x + \\delta))^2 ) \\approx \\frac{1}{N} \\sum_{i=0}^N (\\nabla_x f(x + \\delta_i))^2 \\] The \\(\\sigma\\) in the formula is controlled using the noise parameter, and the expectation is estimated using \\(N\\) samples controlled by the nb_samples parameter. Tip It is recommended to have a noise level \\(\\sigma\\) at about 20% of the range of your inputs, i.e. \\(\\sigma=0.2\\) if your inputs are between \\([0, 1]\\) or \\(\\sigma=0.4\\) if your inputs are between \\([-1, 1]\\) .","title":"Square Grad"},{"location":"api/attributions/methods/square_grad/#example","text":"from xplique.attributions import SquareGrad # load images, labels and model # ... method = SquareGrad ( model , nb_samples = 50 , noise = 0.15 ) explanations = method . explain ( images , labels )","title":"Example"},{"location":"api/attributions/methods/square_grad/#notebooks","text":"Attribution Methods : Getting started SquareGrad : Going Further","title":"Notebooks"},{"location":"api/attributions/methods/square_grad/#SquareGrad","text":"SquareGrad (or SmoothGrad^2) is an unpublished variant of classic SmoothGrad which squares each gradients of the noisy inputs before averaging.","title":"SquareGrad"},{"location":"api/attributions/methods/vargrad/","text":"VarGrad \u00b6 View colab tutorial | View source | \ud83d\udcf0 Paper Similar to SmoothGrad, VarGrad is a gradient-based explanation method, which, as the name suggests, return the variance of the gradient at several points corresponding to small perturbations around the point of interest. The smoothing effect induced by the average help reducing the visual noise, and hence improve the explanations. More precisely, the explanation \\(\\phi\\) for an input \\(x\\) and a classifier \\(f\\) is defined as \\[ \\phi = \\mathbb{V}_{\\delta ~\\sim~ \\mathcal{N}(0, \\sigma^2) }( \\nabla_x f(x + \\delta) ) \\approx \\frac{1}{N-1} \\sum_{i=0}^N (\\nabla_x f(x + \\delta_i) - \\hat{\\mu})^2 \\] Where \\(\\hat{\\mu} = \\frac{1}{N} \\sum_{i=0}^N \\nabla_x f(x + \\delta_i)\\) is the empirical mean. The \\(\\sigma\\) in the formula is controlled using the noise parameter, and the expectation is estimated using \\(N\\) samples controlled by the nb_samples parameter. Tip It is recommended to have a noise level \\(\\sigma\\) at about 20% of the range of your inputs, i.e. \\(\\sigma=0.2\\) if your inputs are between \\([0, 1]\\) or \\(\\sigma=0.4\\) if your inputs are between \\([-1, 1]\\) . Example \u00b6 from xplique.attributions import VarGrad # load images, labels and model # ... method = VarGrad ( model , nb_samples = 50 , noise = 0.15 ) explanations = method . explain ( images , labels ) Notebooks \u00b6 Attribution Methods : Getting started VarGrad : Going Further VarGrad \u00b6 VarGrad is a variance analog to SmoothGrad. __init__( self , model : keras.engine.training.Model , output_layer : Union[str, int, None] = None , batch_size : Union[int, None] = 32 , operator : Union[xplique.commons.operators_operations.Tasks, str , Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float], None] = None, reducer : Union[str, None] = 'mean' , nb_samples : int = 50 , noise : float = 0.2) \u00b6 Parameters model : keras.engine.training.Model The model from which we want to obtain explanations output_layer : Union[str, int, None] = None Layer to target for the outputs (e.g logits or after softmax). If an int is provided it will be interpreted as a layer index. If a string is provided it will look for the layer name. Default to the last layer. It is recommended to use the layer before Softmax. batch_size : Union[int, None] = 32 Number of inputs to explain at once, if None compute all at once. operator : Union[xplique.commons.operators_operations.Tasks, str, Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float], None] = None Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y]. reducer : Union[str, None] = 'mean' String, name of the reducer to use. Either \"min\", \"mean\", \"max\", \"sum\", or None to ignore. Used only for images to obtain explanation with shape (n, h, w, 1). nb_samples : int = 50 Number of noisy samples generated for the smoothing procedure. noise : float = 0.2 Scalar, noise used as standard deviation of a normal law centered on zero. explain( self , inputs : Union[tf.Dataset, tf.Tensor, ] , targets : Union[tf.Tensor, , None] = None) -> tf.Tensor \u00b6 Compute the explanations of the given inputs. Accept Tensor, numpy array or tf.data.Dataset (in that case targets is None) Parameters inputs : Union[tf.Dataset, tf.Tensor, ] Dataset, Tensor or Array. Input samples to be explained. If Dataset, targets should not be provided (included in Dataset). Expected shape among (N, W), (N, T, W), (N, H, W, C). More information in the documentation. targets : Union[tf.Tensor, , None] = None Tensor or Array. One-hot encoding of the model's output from which an explanation is desired. One encoding per input and only one output at a time. Therefore, the expected shape is (N, output_size). More information in the documentation. Return explanations : tf.Tensor Explanation generated by the method.","title":"VarGrad"},{"location":"api/attributions/methods/vargrad/#vargrad","text":"View colab tutorial | View source | \ud83d\udcf0 Paper Similar to SmoothGrad, VarGrad is a gradient-based explanation method, which, as the name suggests, return the variance of the gradient at several points corresponding to small perturbations around the point of interest. The smoothing effect induced by the average help reducing the visual noise, and hence improve the explanations. More precisely, the explanation \\(\\phi\\) for an input \\(x\\) and a classifier \\(f\\) is defined as \\[ \\phi = \\mathbb{V}_{\\delta ~\\sim~ \\mathcal{N}(0, \\sigma^2) }( \\nabla_x f(x + \\delta) ) \\approx \\frac{1}{N-1} \\sum_{i=0}^N (\\nabla_x f(x + \\delta_i) - \\hat{\\mu})^2 \\] Where \\(\\hat{\\mu} = \\frac{1}{N} \\sum_{i=0}^N \\nabla_x f(x + \\delta_i)\\) is the empirical mean. The \\(\\sigma\\) in the formula is controlled using the noise parameter, and the expectation is estimated using \\(N\\) samples controlled by the nb_samples parameter. Tip It is recommended to have a noise level \\(\\sigma\\) at about 20% of the range of your inputs, i.e. \\(\\sigma=0.2\\) if your inputs are between \\([0, 1]\\) or \\(\\sigma=0.4\\) if your inputs are between \\([-1, 1]\\) .","title":"VarGrad"},{"location":"api/attributions/methods/vargrad/#example","text":"from xplique.attributions import VarGrad # load images, labels and model # ... method = VarGrad ( model , nb_samples = 50 , noise = 0.15 ) explanations = method . explain ( images , labels )","title":"Example"},{"location":"api/attributions/methods/vargrad/#notebooks","text":"Attribution Methods : Getting started VarGrad : Going Further","title":"Notebooks"},{"location":"api/attributions/methods/vargrad/#VarGrad","text":"VarGrad is a variance analog to SmoothGrad.","title":"VarGrad"},{"location":"api/attributions/metrics/api_metrics/","text":"API: Metrics \u00b6 Attribution Methods : Metrics Context \u00b6 As the XAI field continues on being trendy, the quantity of materials at disposal to explain DL models keeps on growing. Especially, there is an increasing need to benchmark and evaluate those different approaches. Mainly, there is an urge to evaluate the quality of explanations provided by attribution methods. Info Note that, even though some work exists for other tasks, this challenge has been mainly tackled in the context of Computer Vision tasks. As pointed out by Petsiuk et al. , most explainability approaches used to be evaluated in a human-centered way. For instance, an attribution method was considered good if it pointed at the same relevant pixels as the ones highlighted by human users. While this kind of evaluation allows giving some users trust, it can easily be biased. Therefore, the authors introduced two automatic evaluation metrics that rely solely on the drop or rise in the probability of a class as important pixels (defined by the saliency map) are removed or added. Those are not the only available metrics and we propose here to present the API we used as common ground. Common API \u00b6 Info Metrics described on this page are metrics for attribution methods and explanations. Therefore, the user should first get familiar with the attributions methods API as many parameters are common between both API. For instance, model , inputs , targets , and operator should match for methods and their metrics. All metrics inherit from the base class BaseAttributionMetric which has the following __init__ arguments: model : The model from which we want to obtain explanations inputs : Input samples to be explained targets : Specify the kind of explanations we want depending on the task at end (e.g. a one-hot encoding of a class of interest, a difference to a ground-truth value..) batch_size : an integer which allows to either process inputs per batch or process perturbed samples of an input per batch (inputs are therefore processed one by one). It is most of the time overwritten by the explanation method batch_size . activation : A string that belongs to [None, 'sigmoid', 'softmax']. See the dedicated section for details Then we can distinguish two category of metrics: Those which only need the attribution outputs of an explainer: ExplanationMetric , namely those which evaluate Fidelity ( MuFidelity , Deletion , Insertion ) Those which need the explainer: ExplainerMetric ( AverageStability ) ExplanationMetric \u00b6 Those metrics are agnostic of the explainer used and rely only on the attributions mappings it gives. Tip Therefore, you can use them with other explainer than those provided in Xplique! All metrics inheriting from this class have another argument in their __init__ method: operator : Optional function wrapping the model. It can be seen as a metric which allows to evaluate model evolution. For more details, see the attribution's API Description section on operator . Info The operator used here should match the one used to compute the explanations! All metrics inheriting from this class have to define a method evaluate which will take as input the attributions given by an explainer. Those attributions should correspond to the model , inputs and targets used to build the metric object. ExplainerMetric \u00b6 These metrics will not assess the quality of the explanations provided but (also) the explainer itself. All metrics inheriting from this class have to define a method evaluate which will take as input the explainer evaluated. Info It is even more important that inputs and targets be the same as defined in the attribution's API Description . Currently, there is only one Stability metric inheriting from this class: Activation \u00b6 This parameter specifies if an additional activation layer should be added once a model has been called on the inputs when you have to compute the metric. Indeed, most of the times it is recommended when you instantiate an explainer ( i.e. an attribution methods) to provide a model which gives logits as explaining the logits is to explain the class, while explaining the softmax is to explain why this class rather than another. However, when you compute metrics some were thought to measure a \"drop of probability\" when you occlude the \"most relevant\" part of an input. Thus, once you get your explanations (computed from the logits), you might need to have access to a probability score of occluded inputs of a specific class, thus to have access to the logits after a softmax or sigmoid layer. Consequently, we add this activation parameter so one can provide a model that predicts logits but add an activation layer for the purpose of having probability when using a metric method. The default behavior is to compute the metric without adding any activation layer to the model. Note There does not appear to be a consensus on the activation function to be used for metrics. Some papers use logits values (e.g., with mu-fidelity), while others use sigmoid or softmax (with deletion and insertion). We can only observe that changing the activation function has an effect on the ranking of the best methods. Other Metrics \u00b6 A Representatibity metric: MeGe is also available. Documentation about it should be added soon. Notebooks \u00b6 Metrics : Getting started Metrics : With PyTorch models","title":"API Description"},{"location":"api/attributions/metrics/api_metrics/#api-metrics","text":"Attribution Methods : Metrics","title":"API: Metrics"},{"location":"api/attributions/metrics/api_metrics/#context","text":"As the XAI field continues on being trendy, the quantity of materials at disposal to explain DL models keeps on growing. Especially, there is an increasing need to benchmark and evaluate those different approaches. Mainly, there is an urge to evaluate the quality of explanations provided by attribution methods. Info Note that, even though some work exists for other tasks, this challenge has been mainly tackled in the context of Computer Vision tasks. As pointed out by Petsiuk et al. , most explainability approaches used to be evaluated in a human-centered way. For instance, an attribution method was considered good if it pointed at the same relevant pixels as the ones highlighted by human users. While this kind of evaluation allows giving some users trust, it can easily be biased. Therefore, the authors introduced two automatic evaluation metrics that rely solely on the drop or rise in the probability of a class as important pixels (defined by the saliency map) are removed or added. Those are not the only available metrics and we propose here to present the API we used as common ground.","title":"Context"},{"location":"api/attributions/metrics/api_metrics/#common-api","text":"Info Metrics described on this page are metrics for attribution methods and explanations. Therefore, the user should first get familiar with the attributions methods API as many parameters are common between both API. For instance, model , inputs , targets , and operator should match for methods and their metrics. All metrics inherit from the base class BaseAttributionMetric which has the following __init__ arguments: model : The model from which we want to obtain explanations inputs : Input samples to be explained targets : Specify the kind of explanations we want depending on the task at end (e.g. a one-hot encoding of a class of interest, a difference to a ground-truth value..) batch_size : an integer which allows to either process inputs per batch or process perturbed samples of an input per batch (inputs are therefore processed one by one). It is most of the time overwritten by the explanation method batch_size . activation : A string that belongs to [None, 'sigmoid', 'softmax']. See the dedicated section for details Then we can distinguish two category of metrics: Those which only need the attribution outputs of an explainer: ExplanationMetric , namely those which evaluate Fidelity ( MuFidelity , Deletion , Insertion ) Those which need the explainer: ExplainerMetric ( AverageStability )","title":"Common API"},{"location":"api/attributions/metrics/api_metrics/#activation","text":"This parameter specifies if an additional activation layer should be added once a model has been called on the inputs when you have to compute the metric. Indeed, most of the times it is recommended when you instantiate an explainer ( i.e. an attribution methods) to provide a model which gives logits as explaining the logits is to explain the class, while explaining the softmax is to explain why this class rather than another. However, when you compute metrics some were thought to measure a \"drop of probability\" when you occlude the \"most relevant\" part of an input. Thus, once you get your explanations (computed from the logits), you might need to have access to a probability score of occluded inputs of a specific class, thus to have access to the logits after a softmax or sigmoid layer. Consequently, we add this activation parameter so one can provide a model that predicts logits but add an activation layer for the purpose of having probability when using a metric method. The default behavior is to compute the metric without adding any activation layer to the model. Note There does not appear to be a consensus on the activation function to be used for metrics. Some papers use logits values (e.g., with mu-fidelity), while others use sigmoid or softmax (with deletion and insertion). We can only observe that changing the activation function has an effect on the ranking of the best methods.","title":"Activation"},{"location":"api/attributions/metrics/api_metrics/#other-metrics","text":"A Representatibity metric: MeGe is also available. Documentation about it should be added soon.","title":"Other Metrics"},{"location":"api/attributions/metrics/api_metrics/#notebooks","text":"Metrics : Getting started Metrics : With PyTorch models","title":"Notebooks"},{"location":"api/attributions/metrics/avg_stability/","text":"Average Stability \u00b6 Average Stability is a Stability metric measuring how similar are explanations of similar inputs. Quote [...] We want to ensure that, if inputs are near each other and their model outputs are similar, then their explanations should be close to each other. -- Evaluating and Aggregating Feature-based Model Explanations (2020) 1 Formally, given a predictor \\(f\\) , an explanation function \\(g\\) , a point \\(x\\) , a radius \\(r\\) and a two distance metric: \\(\\rho\\) over the inputs and \\(D\\) over the explanations, the AverageStability is defined as: \\[ S = \\underset{z : \\rho(x, z) \\leq r}{\\int} D(g(f, x), g(f, z))\\ dz \\] Info The better the method, the smaller the score. Example \u00b6 from xplique.metrics import AverageStability from xplique.attributions import Saliency # load images, labels and model # ... explainer = Saliency ( model ) metric = AverageStability ( model , inputs , labels ) score = metric . evaluate ( explainer ) AverageStability \u00b6 Used to compute the average sensitivity metric (or stability). This metric ensure that close inputs with similar predictions yields similar explanations. For each inputs we randomly sample noise to add to the inputs and compute the explanation for the noisy inputs. We then get the average distance between the original explanations and the noisy explanations. __init__( self , model : Callable , inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] , targets : Union[tf.Tensor, numpy.ndarray, None] = None , batch_size : Union[int, None] = 64 , radius : float = 0.1 , distance : Union[str, Callable] = 'l2' , nb_samples : int = 20) \u00b6 Parameters model : Callable Model used for computing metric. inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] Input samples under study. targets : Union[tf.Tensor, numpy.ndarray, None] = None One-hot encoded labels or regression target (e.g {+1, -1}), one for each sample. batch_size : Union[int, None] = 64 Number of samples to explain at once, if None compute all at once. radius : float = 0.1 Maximum value of the uniform noise added to the inputs before recalculating their explanations. distance : Union[str, Callable] = 'l2' Distance metric between the explanations. nb_samples : int = 20 Number of different neighbors points to try on each input to measure the stability. evaluate( self , explainer : Callable , base_explanations : Union[tf.Tensor, numpy.ndarray, None] = None) -> float \u00b6 Evaluate the fidelity score. Parameters explainer : Callable Explainer or Explanations associated to each inputs. base_explanations : Union[tf.Tensor, numpy.ndarray, None] = None Explanation for the inputs under study. Calculates them automatically if they are not provided. Return stability_score : float Average distance between the explanations Warning AverageStability will compute several time explanations for all the inputs (pertubed more or less severly). Thus, it might be very long to compute (especially if the explainer is already time consumming). Evaluating and Aggregating Feature-based Model Explanations (2020) \u21a9","title":"AverageStability"},{"location":"api/attributions/metrics/avg_stability/#average-stability","text":"Average Stability is a Stability metric measuring how similar are explanations of similar inputs. Quote [...] We want to ensure that, if inputs are near each other and their model outputs are similar, then their explanations should be close to each other. -- Evaluating and Aggregating Feature-based Model Explanations (2020) 1 Formally, given a predictor \\(f\\) , an explanation function \\(g\\) , a point \\(x\\) , a radius \\(r\\) and a two distance metric: \\(\\rho\\) over the inputs and \\(D\\) over the explanations, the AverageStability is defined as: \\[ S = \\underset{z : \\rho(x, z) \\leq r}{\\int} D(g(f, x), g(f, z))\\ dz \\] Info The better the method, the smaller the score.","title":"Average Stability"},{"location":"api/attributions/metrics/avg_stability/#example","text":"from xplique.metrics import AverageStability from xplique.attributions import Saliency # load images, labels and model # ... explainer = Saliency ( model ) metric = AverageStability ( model , inputs , labels ) score = metric . evaluate ( explainer )","title":"Example"},{"location":"api/attributions/metrics/avg_stability/#AverageStability","text":"Used to compute the average sensitivity metric (or stability). This metric ensure that close inputs with similar predictions yields similar explanations. For each inputs we randomly sample noise to add to the inputs and compute the explanation for the noisy inputs. We then get the average distance between the original explanations and the noisy explanations.","title":"AverageStability"},{"location":"api/attributions/metrics/deletion/","text":"Deletion \u00b6 The Deletion Fidelity metric measures how well a saliency-map\u2013based explanation localizes the important features. Quote The deletion metric measures the drop in the probability of a class as important pixels (given by the saliency map) are gradually removed from the image. A sharp drop, and thus a small area under the probability curve, are indicative of a good explanation. -- RISE: Randomized Input Sampling for Explanation of Black-box Models (2018) 1 Score interpretation \u00b6 The interpretation of the score depends on your operator , which represents the metrics you use to evaluate your model. For metrics where the score increases with the performance of the model (such as accuracy). If explanations are accurate, the score will quickly fall to the score of a random model. Thus, in this case, a lower score represent a more accurate explanation. For metrics where the score decreases with the performance of the model (such as losses). If explanations are accurate, the score will quickly rise to the score of a random model. Thus, in this case, a higher score represent a more accurate explanation. Remarks \u00b6 This metric only evaluate the order of importance between features. The parameters metric, steps and max_percentage_perturbed may drastically change the score : For inputs with many features, increasing the number of steps will allow you to capture more efficiently the difference between attributions methods. The order of importance of features with low importance may not matter, hence, decreasing the max_percentage_perturbed, may make the score more relevant. Sometimes, attributions methods also returns negative attributions, for those methods, do not take the absolute value before computing insertion and deletion metrics. Otherwise, negative attributions may have higher absolute values, and the order of importance between features will change. Therefore, take those previous remarks into account to get a relevant score. Example \u00b6 from xplique.metrics import Deletion from xplique.attributions import Saliency # load images, targets and model # ... explainer = Saliency ( model ) explanations = explainer ( inputs , targets ) metric = Deletion ( model , inputs , targets ) score = metric . evaluate ( explanations ) Deletion \u00b6 The deletion metric measures the drop in the probability of a class as important pixels (given by the saliency map) are gradually removed from the image. A sharp drop, and thus a small area under the probability curve, are indicative of a good explanation. __init__( self , model : keras.engine.training.Model , inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] , targets : Union[tf.Tensor, numpy.ndarray, None] = None , batch_size : Union[int, None] = 64 , baseline_mode : Union[float, Callable] = 0.0 , steps : int = 10 , max_percentage_perturbed : float = 1.0 , operator : Union[Callable, None] = None , activation : Union[str, None] = None) \u00b6 Parameters model : keras.engine.training.Model Model used for computing metric. inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] Input samples under study. targets : Union[tf.Tensor, numpy.ndarray, None] = None One-hot encoded labels or regression target (e.g {+1, -1}), one for each sample. batch_size : Union[int, None] = 64 Number of samples to explain at once, if None compute all at once. baseline_mode : Union[float, Callable] = 0.0 Value of the baseline state, will be called with the inputs if it is a function. steps : int = 10 Number of steps between the start and the end state. Can be set to -1 for all possible steps to be computed. max_percentage_perturbed : float = 1.0 Maximum percentage of the input perturbed. operator : Union[Callable, None] = None Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y]. activation : Union[str, None] = None A string that belongs to [None, 'sigmoid', 'softmax']. Specify if we should add an activation layer once the model has been called. It is useful, for instance if you want to measure a 'drop of probability' by adding a sigmoid or softmax after getting your logits. If None does not add a layer to your model. detailed_evaluate( self , explanations : Union[tf.Tensor, numpy.ndarray]) -> Dict[int, float] \u00b6 Evaluate model performance for successive perturbations of an input. Used to compute causal score. Parameters explanations : Union[tf.Tensor, numpy.ndarray] Explanation for the inputs, labels to evaluate. Return causal_score_dict : Dict[int, float] Dictionary of scores obtain for different perturbations Keys are the steps, i.e the number of features perturbed Values are the scores, the score of the model on the inputs with the corresponding number of features perturbed evaluate( self , explanations : Union[tf.Tensor, numpy.ndarray]) -> float \u00b6 Evaluate the causal score. Parameters explanations : Union[tf.Tensor, numpy.ndarray] Explanation for the inputs, labels to evaluate. Return causal_score : float Metric score, area over the deletion (lower is better) or insertion (higher is better) curve. RISE: Randomized Input Sampling for Explanation of Black-box Models (2018) \u21a9","title":"Deletion"},{"location":"api/attributions/metrics/deletion/#deletion","text":"The Deletion Fidelity metric measures how well a saliency-map\u2013based explanation localizes the important features. Quote The deletion metric measures the drop in the probability of a class as important pixels (given by the saliency map) are gradually removed from the image. A sharp drop, and thus a small area under the probability curve, are indicative of a good explanation. -- RISE: Randomized Input Sampling for Explanation of Black-box Models (2018) 1","title":"Deletion"},{"location":"api/attributions/metrics/deletion/#score-interpretation","text":"The interpretation of the score depends on your operator , which represents the metrics you use to evaluate your model. For metrics where the score increases with the performance of the model (such as accuracy). If explanations are accurate, the score will quickly fall to the score of a random model. Thus, in this case, a lower score represent a more accurate explanation. For metrics where the score decreases with the performance of the model (such as losses). If explanations are accurate, the score will quickly rise to the score of a random model. Thus, in this case, a higher score represent a more accurate explanation.","title":"Score interpretation"},{"location":"api/attributions/metrics/deletion/#remarks","text":"This metric only evaluate the order of importance between features. The parameters metric, steps and max_percentage_perturbed may drastically change the score : For inputs with many features, increasing the number of steps will allow you to capture more efficiently the difference between attributions methods. The order of importance of features with low importance may not matter, hence, decreasing the max_percentage_perturbed, may make the score more relevant. Sometimes, attributions methods also returns negative attributions, for those methods, do not take the absolute value before computing insertion and deletion metrics. Otherwise, negative attributions may have higher absolute values, and the order of importance between features will change. Therefore, take those previous remarks into account to get a relevant score.","title":"Remarks"},{"location":"api/attributions/metrics/deletion/#example","text":"from xplique.metrics import Deletion from xplique.attributions import Saliency # load images, targets and model # ... explainer = Saliency ( model ) explanations = explainer ( inputs , targets ) metric = Deletion ( model , inputs , targets ) score = metric . evaluate ( explanations )","title":"Example"},{"location":"api/attributions/metrics/deletion/#Deletion","text":"The deletion metric measures the drop in the probability of a class as important pixels (given by the saliency map) are gradually removed from the image. A sharp drop, and thus a small area under the probability curve, are indicative of a good explanation.","title":"Deletion"},{"location":"api/attributions/metrics/insertion/","text":"Insertion \u00b6 The Insertion Fidelity metric measures how well a saliency-map\u2013based explanation can find elements that are minimal for the predictions. Quote The insertion metric, on the other hand, captures the importance of the pixels in terms of their ability to synthesize an image and is measured by the rise in the probability of the class of interest as pixels are added according to the generated importance map. -- RISE: Randomized Input Sampling for Explanation of Black-box Models (2018) 1 Score interpretation \u00b6 The interpretation of the score depends on your operator , which represents the metrics you use to evaluate your model. For metrics where the score increases with the performance of the model (such as accuracy). If explanations are accurate, the score will quickly rise to the score on non-perturbed input. Thus, in this case, a higher score represent a more accurate explanation. For metrics where the score decreases with the performance of the model (such as losses). If explanations are accurate, the score will quickly fall to the score on non-perturbed input. Thus, in this case, a lower score represent a more accurate explanation. Remarks \u00b6 This metric only evaluate the order of importance between features. The parameters metric, steps and max_percentage_perturbed may drastically change the score : For inputs with many features, increasing the number of steps will allow you to capture more efficiently the difference between attributions methods. The order of importance of features with low importance may not matter, hence, decreasing the max_percentage_perturbed, may make the score more relevant. Sometimes, attributions methods also returns negative attributions, for those methods, do not take the absolute value before computing insertion and deletion metrics. Otherwise, negative attributions may have higher absolute values, and the order of importance between features will change. Therefore, take those previous remarks into account to get a relevant score. Example \u00b6 from xplique.metrics import Insertion from xplique.attributions import Saliency # load images, labels and model # ... explainer = Saliency ( model ) explanations = explainer ( inputs , labels ) metric = Insertion ( model , inputs , labels ) score = metric . evaluate ( explanations ) Insertion \u00b6 The insertion metric, on the other hand, captures the importance of the pixels in terms of their ability to synthesize an image and is measured by the rise in the probability of the class of interest as pixels are added according to the generated importance map. __init__( self , model : keras.engine.training.Model , inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] , targets : Union[tf.Tensor, numpy.ndarray, None] = None , batch_size : Union[int, None] = 64 , baseline_mode : Union[float, Callable] = 0.0 , steps : int = 10 , max_percentage_perturbed : float = 1.0 , operator : Union[Callable, None] = None , activation : Union[str, None] = None) \u00b6 Parameters model : keras.engine.training.Model Model used for computing metric. inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] Input samples under study. targets : Union[tf.Tensor, numpy.ndarray, None] = None One-hot encoded labels or regression target (e.g {+1, -1}), one for each sample. batch_size : Union[int, None] = 64 Number of samples to explain at once, if None compute all at once. baseline_mode : Union[float, Callable] = 0.0 Value of the baseline state, will be called with the inputs if it is a function. steps : int = 10 Number of steps between the start and the end state. Can be set to -1 for all possible steps to be computed. max_percentage_perturbed : float = 1.0 Maximum percentage of the input perturbed. operator : Union[Callable, None] = None Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y]. activation : Union[str, None] = None A string that belongs to [None, 'sigmoid', 'softmax']. Specify if we should add an activation layer once the model has been called. It is useful, for instance if you want to measure a 'drop of probability' by adding a sigmoid or softmax after getting your logits. If None does not add a layer to your model. detailed_evaluate( self , explanations : Union[tf.Tensor, numpy.ndarray]) -> Dict[int, float] \u00b6 Evaluate model performance for successive perturbations of an input. Used to compute causal score. Parameters explanations : Union[tf.Tensor, numpy.ndarray] Explanation for the inputs, labels to evaluate. Return causal_score_dict : Dict[int, float] Dictionary of scores obtain for different perturbations Keys are the steps, i.e the number of features perturbed Values are the scores, the score of the model on the inputs with the corresponding number of features perturbed evaluate( self , explanations : Union[tf.Tensor, numpy.ndarray]) -> float \u00b6 Evaluate the causal score. Parameters explanations : Union[tf.Tensor, numpy.ndarray] Explanation for the inputs, labels to evaluate. Return causal_score : float Metric score, area over the deletion (lower is better) or insertion (higher is better) curve. RISE: Randomized Input Sampling for Explanation of Black-box Models (2018) \u21a9","title":"Insertion"},{"location":"api/attributions/metrics/insertion/#insertion","text":"The Insertion Fidelity metric measures how well a saliency-map\u2013based explanation can find elements that are minimal for the predictions. Quote The insertion metric, on the other hand, captures the importance of the pixels in terms of their ability to synthesize an image and is measured by the rise in the probability of the class of interest as pixels are added according to the generated importance map. -- RISE: Randomized Input Sampling for Explanation of Black-box Models (2018) 1","title":"Insertion"},{"location":"api/attributions/metrics/insertion/#score-interpretation","text":"The interpretation of the score depends on your operator , which represents the metrics you use to evaluate your model. For metrics where the score increases with the performance of the model (such as accuracy). If explanations are accurate, the score will quickly rise to the score on non-perturbed input. Thus, in this case, a higher score represent a more accurate explanation. For metrics where the score decreases with the performance of the model (such as losses). If explanations are accurate, the score will quickly fall to the score on non-perturbed input. Thus, in this case, a lower score represent a more accurate explanation.","title":"Score interpretation"},{"location":"api/attributions/metrics/insertion/#remarks","text":"This metric only evaluate the order of importance between features. The parameters metric, steps and max_percentage_perturbed may drastically change the score : For inputs with many features, increasing the number of steps will allow you to capture more efficiently the difference between attributions methods. The order of importance of features with low importance may not matter, hence, decreasing the max_percentage_perturbed, may make the score more relevant. Sometimes, attributions methods also returns negative attributions, for those methods, do not take the absolute value before computing insertion and deletion metrics. Otherwise, negative attributions may have higher absolute values, and the order of importance between features will change. Therefore, take those previous remarks into account to get a relevant score.","title":"Remarks"},{"location":"api/attributions/metrics/insertion/#example","text":"from xplique.metrics import Insertion from xplique.attributions import Saliency # load images, labels and model # ... explainer = Saliency ( model ) explanations = explainer ( inputs , labels ) metric = Insertion ( model , inputs , labels ) score = metric . evaluate ( explanations )","title":"Example"},{"location":"api/attributions/metrics/insertion/#Insertion","text":"The insertion metric, on the other hand, captures the importance of the pixels in terms of their ability to synthesize an image and is measured by the rise in the probability of the class of interest as pixels are added according to the generated importance map.","title":"Insertion"},{"location":"api/attributions/metrics/mu_fidelity/","text":"MuFidelity \u00b6 MuFidelity is a fidelity metric measuring the correlation between important variables defined by the explanation method and the decline in the model score when these variables are reset to a baseline state. Quote [...] when we set particular features \\(x_s\\) to a baseline value \\(x_0\\) the change in predictor\u2019s output should be proportional to the sum of attribution scores. -- Evaluating and Aggregating Feature-based Model Explanations (2020) 1 Formally, given a predictor \\(f\\) , an explanation function \\(g\\) , a point \\(x \\in \\mathbb{R}^n\\) and a subset size \\(k\\) the MuFidelity metric is defined as: \\[ \\mu F = \\underset{S \\subseteq \\{1, ..., d\\} \\\\ |S| = k}{Corr}( \\sum_{i \\in S} g(f, x)_i, f(x) - f(x_{[x_i = x_0 | i \\in S]})) \\] Info The better the method, the higher the score. Example \u00b6 from xplique.metrics import MuFidelity from xplique.attributions import Saliency # load images, labels and model # ... explainer = Saliency ( model ) explanations = explainer ( inputs , lablels ) metric = MuFidelity ( model , inputs , labels ) score = metric . evaluate ( explainations ) MuFidelity \u00b6 Used to compute the fidelity correlation metric. This metric ensure there is a correlation between a random subset of pixels and their attribution score. For each random subset created, we set the pixels of the subset at a baseline state and obtain the prediction score. This metric measures the correlation between the drop in the score and the importance of the explanation. __init__( self , model : Callable , inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] , targets : Union[tf.Tensor, numpy.ndarray, None] = None , batch_size : Union[int, None] = 64 , grid_size : Union[int, None] = 9 , subset_percent : float = 0.2 , baseline_mode : Union[Callable, float] = 0.0 , nb_samples : int = 200 , operator : Union[Callable, None] = None , activation : Union[str, None] = None) \u00b6 Parameters model : Callable Model used for computing metric. inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] Input samples under study. targets : Union[tf.Tensor, numpy.ndarray, None] = None One-hot encoded labels or regression target (e.g {+1, -1}), one for each sample. batch_size : Union[int, None] = 64 Number of samples to explain at once, if None compute all at once. grid_size : Union[int, None] = 9 If none, compute the original metric, else cut the image in (grid_size, grid_size) and each element of the subset will be a super pixel representing one element of the grid. You should use this when dealing with medium / large size images. subset_percent : float = 0.2 Percent of the image that will be set to baseline. baseline_mode : Union[Callable, float] = 0.0 Value of the baseline state, will be called with the a single input if it is a function. nb_samples : int = 200 Number of different subsets to try on each input to measure the correlation. operator : Union[Callable, None] = None Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y]. activation : Union[str, None] = None A string that belongs to [None, 'sigmoid', 'softmax']. Specify if we should add an activation layer once the model has been called. It is useful, for instance if you want to measure a 'drop of probability' by adding a sigmoid or softmax after getting your logits. If None does not add a layer to your model. evaluate( self , explanations : Union[tf.Tensor, numpy.ndarray]) -> float \u00b6 Evaluate the fidelity score. Parameters explanations : Union[tf.Tensor, numpy.ndarray] Explanation for the inputs, labels to evaluate. Return fidelity_score : float Metric score, average correlation between the drop in score when variables are set to a baseline state and the importance of these variables according to the explanations. Evaluating and Aggregating Feature-based Model Explanations (2020) \u21a9","title":"MuFidelity"},{"location":"api/attributions/metrics/mu_fidelity/#mufidelity","text":"MuFidelity is a fidelity metric measuring the correlation between important variables defined by the explanation method and the decline in the model score when these variables are reset to a baseline state. Quote [...] when we set particular features \\(x_s\\) to a baseline value \\(x_0\\) the change in predictor\u2019s output should be proportional to the sum of attribution scores. -- Evaluating and Aggregating Feature-based Model Explanations (2020) 1 Formally, given a predictor \\(f\\) , an explanation function \\(g\\) , a point \\(x \\in \\mathbb{R}^n\\) and a subset size \\(k\\) the MuFidelity metric is defined as: \\[ \\mu F = \\underset{S \\subseteq \\{1, ..., d\\} \\\\ |S| = k}{Corr}( \\sum_{i \\in S} g(f, x)_i, f(x) - f(x_{[x_i = x_0 | i \\in S]})) \\] Info The better the method, the higher the score.","title":"MuFidelity"},{"location":"api/attributions/metrics/mu_fidelity/#example","text":"from xplique.metrics import MuFidelity from xplique.attributions import Saliency # load images, labels and model # ... explainer = Saliency ( model ) explanations = explainer ( inputs , lablels ) metric = MuFidelity ( model , inputs , labels ) score = metric . evaluate ( explainations )","title":"Example"},{"location":"api/attributions/metrics/mu_fidelity/#MuFidelity","text":"Used to compute the fidelity correlation metric. This metric ensure there is a correlation between a random subset of pixels and their attribution score. For each random subset created, we set the pixels of the subset at a baseline state and obtain the prediction score. This metric measures the correlation between the drop in the score and the importance of the explanation.","title":"MuFidelity"},{"location":"api/concepts/cav/","text":"CAV \u00b6 CAV or Concept Activation Vector represent a high-level concept as a vector that indicate the direction to take (for activations of a layer) to maximise this concept. Quote [...] CAV for a concept is simply a vector in the direction of the values (e.g., activations) of that concept\u2019s set of examples\u2026 we derive CAVs by training a linear classifier between a concept\u2019s examples and random counter examples and then taking the vector orthogonal to the decision boundary. -- Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV) (2018). 1 For a layer \\(f_l\\) of a model, we seek the linear classifier \\(v_l \\in \\mathbb{R}^d\\) that separate the activations of the positive examples \\(\\{ f_l(x) : x \\in \\mathcal{P} \\}\\) , and the activations of the random/negative examples \\(\\{ f_l(x) : x \\in \\mathcal{R} \\}\\) . Example \u00b6 from xplique.concepts import Cav cav_renderer = Cav ( model , 'mixed4d' , classifier = 'SGD' , test_fraction = 0.1 ) cav = cav_renderer ( positive_examples , random_examples ) Cav \u00b6 Used to compute the Concept Activation Vector, which is a vector in the direction of the activations of that concept\u2019s set of examples. __init__( self , model : keras.engine.training.Model , target_layer : Union[str, int] , classifier : Union[str, Callable] = 'SGD' , test_fraction : float = 0.2 , batch_size : int = 64 , verbose : bool = False) \u00b6 Parameters model : keras.engine.training.Model Model to extract concept from. target_layer : Union[str, int] Index of the target layer or name of the layer. classifier : 'SGD' or 'SVC' or Sklearn model, optional Default implementation use SGD with hinge classifier (linear SVM), SVC use libsvm but the computation time is longer. test_fraction : float = 0.2 Fraction of the dataset used for test batch_size : int = 64 Batch size during the activations extraction verbose : bool = False If true, display information while training the classifier fit( self , positive_dataset : tf.Tensor , negative_dataset : tf.Tensor) -> tf.Tensor \u00b6 Compute and return the Concept Activation Vector (CAV) associated to the dataset and the layer targeted. Parameters positive_dataset : tf.Tensor Dataset of positive samples : samples containing the concept. negative_dataset : tf.Tensor Dataset of negative samples : samples without the concept Return cav : tf.Tensor Vector of the same shape as the layer output Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV) (2018). \u21a9","title":"Cav"},{"location":"api/concepts/cav/#cav","text":"CAV or Concept Activation Vector represent a high-level concept as a vector that indicate the direction to take (for activations of a layer) to maximise this concept. Quote [...] CAV for a concept is simply a vector in the direction of the values (e.g., activations) of that concept\u2019s set of examples\u2026 we derive CAVs by training a linear classifier between a concept\u2019s examples and random counter examples and then taking the vector orthogonal to the decision boundary. -- Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV) (2018). 1 For a layer \\(f_l\\) of a model, we seek the linear classifier \\(v_l \\in \\mathbb{R}^d\\) that separate the activations of the positive examples \\(\\{ f_l(x) : x \\in \\mathcal{P} \\}\\) , and the activations of the random/negative examples \\(\\{ f_l(x) : x \\in \\mathcal{R} \\}\\) .","title":"CAV"},{"location":"api/concepts/cav/#example","text":"from xplique.concepts import Cav cav_renderer = Cav ( model , 'mixed4d' , classifier = 'SGD' , test_fraction = 0.1 ) cav = cav_renderer ( positive_examples , random_examples )","title":"Example"},{"location":"api/concepts/cav/#Cav","text":"Used to compute the Concept Activation Vector, which is a vector in the direction of the activations of that concept\u2019s set of examples.","title":"Cav"},{"location":"api/concepts/craft/","text":"CRAFT \u00b6 View colab Tensorflow tutorial | View colab Pytorch tutorial | View source | \ud83d\udcf0 Paper CRAFT or Concept Recursive Activation FacTorization for Explainability is a method for automatically extracting human-interpretable concepts from deep networks. This concept activations factorization method aims to explain a trained model's decisions on a per-class and per-image basis by highlighting both \"what\" the model saw and \u201cwhere\u201d it saw it. Thus CRAFT generates post-hoc local and global explanations. It is made up from 3 ingredients: a method to recursively decompose concepts into sub-concepts a method to better estimate the importance of extracted concepts a method to use any attribution method to create concept attribution maps, using implicit differentiation CRAFT requires splitting the model in two parts: \\((g, h)\\) such that \\(f(x) = (g \\cdot h)(x)\\) . To put it simply, \\(g\\) is the function that maps our input to the latent space (an inner layer of our model), and \\(h\\) is the function that maps the latent space to the output. The concepts will be extracted from this latent space. Info It is important to note that if the model contains a global average pooling layer, it is strongly recommended to provide CRAFT with the layer before the global average pooling. Warning Please keep in mind that the activations must be positives (after relu or any positive activation function) Example \u00b6 Use Craft to investigate a single class. from xplique.concepts import CraftTf as Craft # Cut the model in two parts (as explained in the paper) # first part is g(.) our 'input_to_latent' model returning positive activations, # second part is h(.) our 'latent_to_logit' model g = tf . keras . Model ( model . input , model . layers [ - 3 ] . output ) h = tf . keras . Model ( model . layers [ - 2 ] . input , model . layers [ - 1 ] . output ) # Create a Craft concept extractor from these 2 models craft = Craft ( input_to_latent_model = g , latent_to_logit_model = h , number_of_concepts = 10 , patch_size = 80 , batch_size = 64 ) # Use Craft to get the crops (crops), the embedding of the crops (crops_u), # and the concept bank (w) crops , crops_u , w = craft . fit ( images_preprocessed , class_id = rabbit_class_id ) # Compute Sobol indices to understand which concept matters importances = craft . estimate_importance () # Display those concepts by showing the 10 best crops for each concept craft . plot_concepts_crops ( nb_crops = 10 ) Use CraftManager to investigate multiple classes. from xplique.concepts import CraftManagerTf as CraftManager # Cut the model in two parts (as explained in the paper) # first part is g(.) our 'input_to_latent' model returning positive activations, # second part is h(.) our 'latent_to_logit' model g = tf . keras . Model ( model . input , model . layers [ - 3 ] . output ) h = tf . keras . Model ( model . layers [ - 2 ] . input , model . layers [ - 1 ] . output ) # CraftManager will create one instance of Craft per class of interest # to investigate list_of_class_of_interest = [ 0 , 491 , 497 , 569 , 574 ] # list of class_ids cm = CraftManager ( input_to_latent_model = g , latent_to_logit_model = h , inputs = inputs_preprocessed , labels = y , list_of_class_of_interest = list_of_class_of_interest , number_of_concepts = 10 , patch_size = 80 , batch_size = 64 ) cm . fit ( nb_samples_per_class = 50 ) # Compute Sobol indices to understand which concept matters cm . estimate_importance () # Display those concepts by showing the 10 best crops for each concept, # for the 1st class cm . plot_concepts_crops ( class_id = 0 , nb_crops = 10 ) CraftTf \u00b6 Class implementing the CRAFT Concept Extraction Mechanism on Tensorflow. __init__( self , input_to_latent_model : Callable , latent_to_logit_model : Callable , number_of_concepts : int = 20 , batch_size : int = 64 , patch_size : int = 64) \u00b6 Parameters input_to_latent_model : Callable The first part of the model taking an input and returning positive activations, g(.) in the original paper. Must be a Tensorflow model (tf.keras.engine.base_layer.Layer) accepting data of shape (n_samples, height, width, channels). latent_to_logit_model : Callable The second part of the model taking activation and returning logits, h(.) in the original paper. Must be a Tensorflow model (tf.keras.engine.base_layer.Layer). number_of_concepts : int = 20 The number of concepts to extract. Default is 20. batch_size : int = 64 The batch size to use during training and prediction. Default is 64. patch_size : int = 64 The size of the patches to extract from the input data. Default is 64. check_if_fitted( self ) \u00b6 Checks if the factorization model has been fitted to input data. compute_subplots_layout_parameters( images : numpy.ndarray , cols : int = 5 , img_size : float = 2.0 , margin : float = 0.3 , spacing : float = 0.3) \u00b6 Compute layout parameters for subplots, to be used by the method fig.subplots_adjust() Parameters images : numpy.ndarray The images to display with subplots. Should be data of shape (n_samples, height, width, channels). cols : int = 5 Number of columns to configure for the subplots. Defaults to 5. img_size : float = 2.0 Size of each subplots (in inch), considering we keep aspect ratio. Defaults to 2. margin : float = 0.3 The margin to use for the subplots. Defaults to 0.3. spacing : float = 0.3 The spacing to use for the subplots. Defaults to 0.3. Return layout_parameters A dictionary containing the layout description rows The number of rows needed to display the images figwidth The figures width in the subplots figheight The figures height in the subplots estimate_importance( self , inputs : numpy.ndarray = None , nb_design : int = 32) -> numpy.ndarray \u00b6 Estimates the importance of each concept for a given class, either globally on the whole dataset provided in the fit() method (in this case, inputs shall be set to None), or locally on a specific input image. Parameters inputs : numpy array or Tensor The input data on which to compute the importances. If None, then the inputs provided in the fit() method will be used (global importance of the whole dataset). Default is None. nb_design : int = 32 The number of design to use for the importance estimation. Default is 32. Return importances : numpy.ndarray The Sobol total index (importance score) for each concept. fit( self , inputs : numpy.ndarray , class_id : int = 0) -> Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray] \u00b6 Fit the Craft model to the input data. Parameters inputs : numpy.ndarray Input data of shape (n_samples, height, width, channels). (x1, x2, ..., xn) in the paper. class_id : int = 0 The class id of the inputs. Return crops : Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray] The crops (X in the paper) crops_u : Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray] The concepts' values (U in the paper) concept_bank_w : Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray] The concept's basis (W in the paper) plot_concept_attribution_legend( self , nb_most_important_concepts : int = 6 , border_width : int = 5) \u00b6 Plot a legend for the concepts attribution maps. Parameters nb_most_important_concepts : int = 6 The number of concepts to focus on. Default is 6. border_width : int = 5 Width of the border around each concept image, in pixels. Defaults to 5. plot_concept_attribution_map( self , image : numpy.ndarray , most_important_concepts : numpy.ndarray , nb_most_important_concepts : int = 5 , filter_percentile : int = 90 , clip_percentile : Union[float, None] = 10 , alpha : float = 0.65 , **plot_kwargs) \u00b6 Display the concepts attribution map for a single image given in argument. Parameters image : numpy.ndarray The image to display. most_important_concepts : numpy.ndarray The concepts ids to display. nb_most_important_concepts : int = 5 The number of concepts to display. Default is 5. filter_percentile : int = 90 Percentile used to filter the concept heatmap. (only show concept if excess N-th percentile). Defaults to 90. clip_percentile : Union[float, None] = 10 Percentile value to use if clipping is needed when drawing the concept, e.g a value of 1 will perform a clipping between percentile 1 and 99. This parameter allows to avoid outliers in case of too extreme values. It is applied after the filter_percentile operation. Default to 10. alpha : float = 0.65 The alpha channel value for the heatmaps. Defaults to 0.65. plot_kwargs : **plot_kwargs Additional parameters passed to plt.imshow() . plot_concept_attribution_maps( self , images : numpy.ndarray , importances : numpy.ndarray = None , nb_most_important_concepts : int = 5 , filter_percentile : int = 90 , clip_percentile : Union[float, None] = 10.0 , alpha : float = 0.65 , cols : int = 5 , img_size : float = 2.0 , **plot_kwargs) \u00b6 Display the concepts attribution maps for the images given in argument. Parameters images : numpy.ndarray The images to display. importances : numpy.ndarray = None The importances computed by the estimate_importance() method. If None is provided, then the global importances will be used, otherwise the local importances set in this parameter will be used. nb_most_important_concepts : int = 5 The number of concepts to focus on. Default is 5. filter_percentile : int = 90 Percentile used to filter the concept heatmap (only show concept if excess N-th percentile). Defaults to 90. clip_percentile : Union[float, None] = 10.0 Percentile value to use if clipping is needed when drawing the concept, e.g a value of 1 will perform a clipping between percentile 1 and 99. This parameter allows to avoid outliers in case of too extreme values. It is applied after the filter_percentile operation. Default to 10. alpha : float = 0.65 The alpha channel value for the heatmaps. Defaults to 0.65. cols : int = 5 Number of columns. Default to 3. img_size : float = 2.0 Size of each subplots (in inch), considering we keep aspect ratio. plot_kwargs : **plot_kwargs Additional parameters passed to plt.imshow() . plot_concepts_crops( self , nb_crops : int = 10 , nb_most_important_concepts : int = None , verbose : bool = False) -> None \u00b6 Display the crops for each concept. Parameters nb_crops : int = 10 The number of crops (patches) to display per concept. Defaults to 10. nb_most_important_concepts : int = None The number of concepts to display. If provided, only display nb_most_important_concepts, otherwise display them all. Default is None. verbose : bool = False If True, then print the importance value of each concept, otherwise no textual output will be printed. plot_concepts_importances( self , importances : numpy.ndarray = None , display_importance_order: xplique.concepts.craft.DisplayImportancesOrder = , nb_most_important_concepts : int = None , verbose : bool = False) \u00b6 Plot a bar chart displaying the importance value of each concept. Parameters importances : numpy.ndarray = None The importances computed by the estimate_importance() method. Default is None, in this case the importances computed on the whole dataset will be used. display_importance_order : 0> Selects the order in which the concepts will be displayed, either following the global importance on the whole dataset (same order for all images) or the local importance of the concepts for a single image sample (local importance). nb_most_important_concepts : int = None The number of concepts to display. If None is provided, then all the concepts will be displayed unordered, otherwise only nb_most_important_concepts will be displayed, ordered by importance. Default is None. verbose : bool = False If True, then print the importance value of each concept, otherwise no textual output will be printed. plot_image_concepts( self , img : numpy.ndarray , display_importance_order: xplique.concepts.craft.DisplayImportancesOrder = , nb_most_important_concepts : int = 5 , filter_percentile : int = 90 , clip_percentile : Union[float, None] = 10 , alpha : float = 0.65 , filepath : Union[str, None] = None , **plot_kwargs) \u00b6 All in one method displaying several plots for the image id given in argument: - the concepts attribution map for this image - the best crops for each concept (displayed around the heatmap) - the importance of each concept Parameters img : numpy.ndarray The image to display. display_importance_order : 0> Selects the order in which the concepts will be displayed, either following the global importance on the whole dataset (same order for all images) or the local importance of the concepts for a single image sample (local importance). Default to GLOBAL. nb_most_important_concepts : int = 5 The number of concepts to display. Default is 5. filter_percentile : int = 90 Percentile used to filter the concept heatmap (only show concept if excess N-th percentile). Defaults to 90. clip_percentile : Union[float, None] = 10 Percentile value to use if clipping is needed when drawing the concept, e.g a value of 1 will perform a clipping between percentile 1 and 99. This parameter allows to avoid outliers in case of too extreme values. Default to 10. alpha : float = 0.65 The alpha channel value for the heatmaps. Defaults to 0.65. filepath : Union[str, None] = None Path the file will be saved at. If None, the function will call plt.show(). plot_kwargs : **plot_kwargs Additional parameters passed to plt.imshow() . transform( self , inputs : numpy.ndarray , activations : numpy.ndarray = None) -> numpy.ndarray \u00b6 Transforms the inputs data into its concept representation. Parameters inputs : numpy.ndarray The input data to be transformed. activations : numpy.ndarray = None Pre-computed activations of the input data. If not provided, the activations will be computed using the input_to_latent_model model on the inputs. Return coeffs_u : numpy.ndarray The concepts' values of the inputs (U in the paper). CraftManagerTf \u00b6 Class implementing the CraftManager on Tensorflow. This manager creates one CraftTf instance per class to explain. __init__( self , input_to_latent_model : Callable , latent_to_logit_model : Callable , inputs : numpy.ndarray , labels : numpy.ndarray , list_of_class_of_interest : Union[list, None] = None , number_of_concepts : int = 20 , batch_size : int = 64 , patch_size : int = 64) \u00b6 Parameters input_to_latent_model : Callable The first part of the model taking an input and returning positive activations, g(.) in the original paper. Must return positive activations. latent_to_logit_model : Callable The second part of the model taking activation and returning logits, h(.) in the original paper. inputs : numpy.ndarray Input data of shape (n_samples, height, width, channels). (x1, x2, ..., xn) in the paper. labels : numpy.ndarray Labels of the inputs of shape (n_samples, class_id) list_of_class_of_interest : Union[list, None] = None A list of the classes id to explain. The manager will instanciate one CraftTf object per element of this list. number_of_concepts : int = 20 The number of concepts to extract. Default is 20. batch_size : int = 64 The batch size to use during training and prediction. Default is 64. patch_size : int = 64 The size of the patches (crops) to extract from the input data. Default is 64. compute_predictions( self ) \u00b6 Compute the predictions for the current dataset, using the 2 models input_to_latent_model and latent_to_logit_model chained. Return y_preds the predictions estimate_importance( self , nb_design : int = 32 , verbose : bool = False) \u00b6 Estimates the importance of each concept for all the classes of interest. Parameters nb_design : int = 32 The number of design to use for the importance estimation. Default is 32. verbose : bool = False If True, then print the current class CRAFT is estimating importances for, otherwise no textual output will be printed. fit( self , nb_samples_per_class : Union[int, None] = None , verbose : bool = False) \u00b6 Fit the Craft models on their respective class of interest. Parameters nb_samples_per_class : Union[int, None] = None Number of samples to use to fit the Craft model. Default is None, which means that all the samples will be used. verbose : bool = False If True, then print the current class CRAFT is fitting, otherwise no textual output will be printed. plot_concepts_crops( self , class_id : int , nb_crops : int = 10 , nb_most_important_concepts : int = None) \u00b6 Display the crops for each concept. Parameters class_id : int The class to explain. nb_crops : int = 10 The number of crops (patches) to display per concept. Defaults to 10. nb_most_important_concepts : int = None The number of concepts to display. If provided, only display nb_most_important_concepts, otherwise display them all. Default is None. plot_concepts_importances( self , class_id : int , nb_most_important_concepts : int = 5 , verbose : bool = False) \u00b6 Plot a bar chart displaying the importance value of each concept. Parameters class_id : int The class to explain. nb_most_important_concepts : int = 5 The number of concepts to focus on. Default is 5. verbose : bool = False If True, then print the importance value of each concept, otherwise no textual output will be printed. plot_image_concepts( self , img : numpy.ndarray , class_id : int , display_importance_order: xplique.concepts.craft.DisplayImportancesOrder = , nb_most_important_concepts : int = 5 , filter_percentile : int = 90 , clip_percentile : Union[float, None] = 10 , alpha : float = 0.65 , filepath : Union[str, None] = None) \u00b6 All in one method displaying several plots for the image id given in argument: - the concepts attribution map for this image - the best crops for each concept (displayed around the heatmap) - the importance of each concept Parameters img : numpy.ndarray The image to explain. class_id : int The class to explain. display_importance_order : 0> Selects the order in which the concepts will be displayed, either following the global importance on the whole dataset (same order for all images) or the local importance of the concepts for a single image sample (local importance). Default to GLOBAL. nb_most_important_concepts : int = 5 The number of concepts to focus on. Default is 5. filter_percentile : int = 90 Percentile used to filter the concept heatmap (only show concept if excess N-th percentile). Defaults to 90. clip_percentile : Union[float, None] = 10 Percentile value to use if clipping is needed when drawing the concept, e.g a value of 1 will perform a clipping between percentile 1 and 99. This parameter allows to avoid outliers in case of too extreme values. Default to 10. alpha : float = 0.65 The alpha channel value for the heatmaps. Defaults to 0.65. filepath : Union[str, None] = None Path the file will be saved at. If None, the function will call plt.show(). CRAFT: Concept Recursive Activation FacTorization for Explainability (2023). \u21a9","title":"Craft"},{"location":"api/concepts/craft/#craft","text":"View colab Tensorflow tutorial | View colab Pytorch tutorial | View source | \ud83d\udcf0 Paper CRAFT or Concept Recursive Activation FacTorization for Explainability is a method for automatically extracting human-interpretable concepts from deep networks. This concept activations factorization method aims to explain a trained model's decisions on a per-class and per-image basis by highlighting both \"what\" the model saw and \u201cwhere\u201d it saw it. Thus CRAFT generates post-hoc local and global explanations. It is made up from 3 ingredients: a method to recursively decompose concepts into sub-concepts a method to better estimate the importance of extracted concepts a method to use any attribution method to create concept attribution maps, using implicit differentiation CRAFT requires splitting the model in two parts: \\((g, h)\\) such that \\(f(x) = (g \\cdot h)(x)\\) . To put it simply, \\(g\\) is the function that maps our input to the latent space (an inner layer of our model), and \\(h\\) is the function that maps the latent space to the output. The concepts will be extracted from this latent space. Info It is important to note that if the model contains a global average pooling layer, it is strongly recommended to provide CRAFT with the layer before the global average pooling. Warning Please keep in mind that the activations must be positives (after relu or any positive activation function)","title":"CRAFT"},{"location":"api/concepts/craft/#example","text":"Use Craft to investigate a single class. from xplique.concepts import CraftTf as Craft # Cut the model in two parts (as explained in the paper) # first part is g(.) our 'input_to_latent' model returning positive activations, # second part is h(.) our 'latent_to_logit' model g = tf . keras . Model ( model . input , model . layers [ - 3 ] . output ) h = tf . keras . Model ( model . layers [ - 2 ] . input , model . layers [ - 1 ] . output ) # Create a Craft concept extractor from these 2 models craft = Craft ( input_to_latent_model = g , latent_to_logit_model = h , number_of_concepts = 10 , patch_size = 80 , batch_size = 64 ) # Use Craft to get the crops (crops), the embedding of the crops (crops_u), # and the concept bank (w) crops , crops_u , w = craft . fit ( images_preprocessed , class_id = rabbit_class_id ) # Compute Sobol indices to understand which concept matters importances = craft . estimate_importance () # Display those concepts by showing the 10 best crops for each concept craft . plot_concepts_crops ( nb_crops = 10 ) Use CraftManager to investigate multiple classes. from xplique.concepts import CraftManagerTf as CraftManager # Cut the model in two parts (as explained in the paper) # first part is g(.) our 'input_to_latent' model returning positive activations, # second part is h(.) our 'latent_to_logit' model g = tf . keras . Model ( model . input , model . layers [ - 3 ] . output ) h = tf . keras . Model ( model . layers [ - 2 ] . input , model . layers [ - 1 ] . output ) # CraftManager will create one instance of Craft per class of interest # to investigate list_of_class_of_interest = [ 0 , 491 , 497 , 569 , 574 ] # list of class_ids cm = CraftManager ( input_to_latent_model = g , latent_to_logit_model = h , inputs = inputs_preprocessed , labels = y , list_of_class_of_interest = list_of_class_of_interest , number_of_concepts = 10 , patch_size = 80 , batch_size = 64 ) cm . fit ( nb_samples_per_class = 50 ) # Compute Sobol indices to understand which concept matters cm . estimate_importance () # Display those concepts by showing the 10 best crops for each concept, # for the 1st class cm . plot_concepts_crops ( class_id = 0 , nb_crops = 10 )","title":"Example"},{"location":"api/concepts/craft/#CraftTf","text":"Class implementing the CRAFT Concept Extraction Mechanism on Tensorflow.","title":"CraftTf"},{"location":"api/concepts/craft/#CraftManagerTf","text":"Class implementing the CraftManager on Tensorflow. This manager creates one CraftTf instance per class to explain.","title":"CraftManagerTf"},{"location":"api/concepts/tcav/","text":"TCAV \u00b6 TCAV or Testing with Concept Activation Vector consist consists in using a concept activation vector (CAV) to quantify the relationship between this concept and a class. This is done by using the directional derivative of the concept vector on several samples of a given class and measuring the percentage of positive (a positive directional derivative indicating that an infinitesimal addition of the concept increases the probability of the class). For a Concept Activation Vector \\(v_l\\) of a layer \\(f_l\\) of a model, and \\(f_{c}\\) the logit of the class \\(c\\) , we measure the directional derivative \\(S_c(x) = v_l \\cdot \\frac{ \\partial{f_c(x)} } { \\partial{f_l}(x) }\\) . The TCAV score is the percentage of elements of the class \\(c\\) for which the \\(S_c\\) is positive. \\[ TCAV_c = \\frac{|x \\in \\mathcal{X}^c : S_c(x) > 0 |}{ | \\mathcal{X}^c | } \\] Example \u00b6 from xplique.concepts import Tcav tcav_renderer = Tcav ( model , 'mixed4d' ) # you can also pass the layer index (e.g -1) tcav_score = tcav_renderer ( samples , class_index , cav ) Tcav \u00b6 Used to Test a Concept Activation Vector, using the sign of the directional derivative of a concept vector relative to a class. __init__( self , model : keras.engine.training.Model , target_layer : Union[str, int] , batch_size : Union[int, None] = 64) \u00b6 Parameters model : keras.engine.training.Model Model to extract concept from. target_layer : Union[str, int] Index of the target layer or name of the layer. batch_size : Union[int, None] = 64 Batch size during the predictions. directional_derivative( multi_head_model : keras.engine.training.Model , inputs : tf.Tensor , label : int , cav : tf.Tensor) -> tf.Tensor \u00b6 Compute the gradient of the label relative to the activations of the CAV layer. Parameters multi_head_model : keras.engine.training.Model Model reconfigured, first output is the activations of the CAV layer, and the second output is the prediction layer. inputs : tf.Tensor Input sample on which to test the influence of the concept. label : int Index of the class to test. cav : tf.Tensor Concept Activation Vector, same shape as the activations output. Return directional_derivative : tf.Tensor Directional derivative values of each samples. score( self , inputs : tf.Tensor , label : int , cav : tf.Tensor) -> float \u00b6 Compute and return the TCAV score of the CAV associated to class tested. Parameters inputs : tf.Tensor Input sample on which to test the influence of the concept. label : int Index of the class to test. cav : tf.Tensor Concept Activation Vector, see CAV module. Return tcav : float Percentage of sample for which increasing the concept has a positive impact on the class logit. Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV) (2018). \u21a9","title":"Tcav"},{"location":"api/concepts/tcav/#tcav","text":"TCAV or Testing with Concept Activation Vector consist consists in using a concept activation vector (CAV) to quantify the relationship between this concept and a class. This is done by using the directional derivative of the concept vector on several samples of a given class and measuring the percentage of positive (a positive directional derivative indicating that an infinitesimal addition of the concept increases the probability of the class). For a Concept Activation Vector \\(v_l\\) of a layer \\(f_l\\) of a model, and \\(f_{c}\\) the logit of the class \\(c\\) , we measure the directional derivative \\(S_c(x) = v_l \\cdot \\frac{ \\partial{f_c(x)} } { \\partial{f_l}(x) }\\) . The TCAV score is the percentage of elements of the class \\(c\\) for which the \\(S_c\\) is positive. \\[ TCAV_c = \\frac{|x \\in \\mathcal{X}^c : S_c(x) > 0 |}{ | \\mathcal{X}^c | } \\]","title":"TCAV"},{"location":"api/concepts/tcav/#example","text":"from xplique.concepts import Tcav tcav_renderer = Tcav ( model , 'mixed4d' ) # you can also pass the layer index (e.g -1) tcav_score = tcav_renderer ( samples , class_index , cav )","title":"Example"},{"location":"api/concepts/tcav/#Tcav","text":"Used to Test a Concept Activation Vector, using the sign of the directional derivative of a concept vector relative to a class.","title":"Tcav"},{"location":"api/feature_viz/feature_viz/","text":"Feature Visualization \u00b6 View colab tutorial One of the specificities of neural networks is their differentiability. This characteristic allows us to compute gradients, either the gradient of a loss with respect to the parameters, or in the case we are interested in here, of a part of the network with respect to the input. This gradient then allows us to iteratively modify the input in order to maximize an objective such as a neuron, a channel or a combination of objectives. Quote If we want to understand individual features, we can search for examples where they have high values either for a neuron at an individual position, or for an entire channel. -- Feature Visualization -- How neural networks build up their understanding of images (2017) 1 More precisely, the explanation of a neuron \\(n\\) denoted as \\(\\phi^{(n)}\\) is an input \\(x* \\in \\mathcal{X}\\) such that \\[ \\phi^{(n)} = \\underset{x}{arg\\ max}\\ f(x)^{(n)} - \\mathcal{R}(x) \\] with \\(f(x)^{(n)}\\) the neuron score for an input \\(x\\) and \\(\\mathcal{R}(x)\\) a regularization term. In practice it turns out that preconditioning the input in a decorrelated space such as the frequency domain allows to obtain more consistent results and to better formulate the regularization (e.g. by controlling the rate of high frequency and low frequency desired). Examples \u00b6 Optimize the ten logits of a neural network (we recommend to remove the softmax activation of your network). from xplique.features_visualizations import Objective from xplique.features_visualizations import optimize # load a model... # targeting the 10 logits of the layer 'logits' # we can also target a layer by its index, like -1 for the last layer logits_obj = Objective . neuron ( model , \"logits\" , list ( range ( 10 ))) images , obj_names = optimize ( logits_obj ) # 10 images, one for each logits Create a combination of multiple objectives and aggregate them from xplique.features_visualizations import Objective from xplique.features_visualizations import optimize # load a model... # target the first logits neuron logits_obj = Objective . neuron ( model , \"logits\" , 0 ) # target the third layer layer_obj = Objective . layer ( model , \"conv2d_1\" ) # target the second channel of another layer channel_obj = Objective . channel ( model , \"mixed4_2\" , 2 ) # combine the objective obj = logits_obj * 1.0 + layer_obj * 3.0 + channel_obj * ( - 5.0 ) images , obj_names = optimize ( logits_obj ) # 1 resulting image Objective \u00b6 Use to combine several sub-objectives into one. __init__( self , model : keras.engine.training.Model , layers : List[keras.engine.base_layer.Layer] , masks : List[tf.Tensor] , funcs : List[Callable] , multipliers : List[float] , names : List[str]) \u00b6 Parameters model : keras.engine.training.Model Model used for optimization. layers : List[keras.engine.base_layer.Layer] A list of the layers output for each sub-objectives. masks : List[tf.Tensor] A list of masks that will be applied on the targeted layer for each sub-objectives. funcs : List[Callable] A list of loss functions for each sub-objectives. multipliers : List[float] A list of multiplication factor for each sub-objectives names : List[str] A list of name for each sub-objectives channel( model : keras.engine.training.Model , layer : Union[str, int] , channel_ids : Union[int, List[int]] , multiplier : float = 1.0 , names : Union[str, List[str], None] = None) \u00b6 Util to build an objective to maximise a channel. Parameters model : keras.engine.training.Model Model used for optimization. layer : Union[str, int] Index or name of the targeted layer. channel_ids : Union[int, List[int]] Indexes of the channels to maximise. multiplier : float = 1.0 Multiplication factor of the objectives. names : Union[str, List[str], None] = None Names for each objectives. Return objective An objective containing a sub-objective for each channels. compile( self ) -> Tuple[keras.engine.training.Model, Callable, List[str], Tuple] \u00b6 Compile all the sub-objectives into one and return the objects for the optimisation process. Return model_reconfigured : Tuple[keras.engine.training.Model, Callable, List[str], Tuple] Model with the outputs needed for the optimization. objective_function : Tuple[keras.engine.training.Model, Callable, List[str], Tuple] Function to call that compute the loss for the objectives. names : Tuple[keras.engine.training.Model, Callable, List[str], Tuple] Names of each objectives. input_shape : Tuple[keras.engine.training.Model, Callable, List[str], Tuple] Shape of the input, one sample for each optimization. direction( model : keras.engine.training.Model , layer : Union[str, int] , vectors : Union[tf.Tensor, List[tf.Tensor]] , multiplier : float = 1.0 , cossim_pow : float = 2.0 , names : Union[str, List[str], None] = None) \u00b6 Util to build an objective to maximise a direction of a layer. Parameters model : keras.engine.training.Model Model used for optimization. layer : Union[str, int] Index or name of the targeted layer. vectors : Union[tf.Tensor, List[tf.Tensor]] Direction(s) to optimize. multiplier : float = 1.0 Multiplication factor of the objective. cossim_pow : float = 2.0 Power of the cosine similarity, higher value encourage the objective to care more about the angle of the activations. names : Union[str, List[str], None] = None A name for each objectives. Return objective An objective ready to be compiled layer( model : keras.engine.training.Model , layer : Union[str, int] , reducer : str = 'magnitude' , multiplier : float = 1.0 , name : Union[str, None] = None) \u00b6 Util to build an objective to maximise a layer. Parameters model : keras.engine.training.Model Model used for optimization. layer : Union[str, int] Index or name of the targeted layer. reducer : str = 'magnitude' Type of reduction to apply, 'mean' will optimize the mean value of the layer, 'magnitude' will optimize the mean of the absolute values. multiplier : float = 1.0 Multiplication factor of the objective. name : Union[str, None] = None A name for the objective. Return objective An objective ready to be compiled neuron( model : keras.engine.training.Model , layer : Union[str, int] , neurons_ids : Union[int, List[int]] , multiplier : float = 1.0 , names : Union[str, List[str], None] = None) \u00b6 Util to build an objective to maximise a neuron. Parameters model : keras.engine.training.Model Model used for optimization. layer : Union[str, int] Index or name of the targeted layer. neurons_ids : Union[int, List[int]] Indexes of the neurons to maximise. multiplier : float = 1.0 Multiplication factor of the objectives. names : Union[str, List[str], None] = None Names for each objectives. Return objective An objective containing a sub-objective for each neurons. Feature Visualization -- How neural networks build up their understanding of images (2017) \u21a9","title":"Feature visualization"},{"location":"api/feature_viz/feature_viz/#feature-visualization","text":"View colab tutorial One of the specificities of neural networks is their differentiability. This characteristic allows us to compute gradients, either the gradient of a loss with respect to the parameters, or in the case we are interested in here, of a part of the network with respect to the input. This gradient then allows us to iteratively modify the input in order to maximize an objective such as a neuron, a channel or a combination of objectives. Quote If we want to understand individual features, we can search for examples where they have high values either for a neuron at an individual position, or for an entire channel. -- Feature Visualization -- How neural networks build up their understanding of images (2017) 1 More precisely, the explanation of a neuron \\(n\\) denoted as \\(\\phi^{(n)}\\) is an input \\(x* \\in \\mathcal{X}\\) such that \\[ \\phi^{(n)} = \\underset{x}{arg\\ max}\\ f(x)^{(n)} - \\mathcal{R}(x) \\] with \\(f(x)^{(n)}\\) the neuron score for an input \\(x\\) and \\(\\mathcal{R}(x)\\) a regularization term. In practice it turns out that preconditioning the input in a decorrelated space such as the frequency domain allows to obtain more consistent results and to better formulate the regularization (e.g. by controlling the rate of high frequency and low frequency desired).","title":"Feature Visualization"},{"location":"api/feature_viz/feature_viz/#examples","text":"Optimize the ten logits of a neural network (we recommend to remove the softmax activation of your network). from xplique.features_visualizations import Objective from xplique.features_visualizations import optimize # load a model... # targeting the 10 logits of the layer 'logits' # we can also target a layer by its index, like -1 for the last layer logits_obj = Objective . neuron ( model , \"logits\" , list ( range ( 10 ))) images , obj_names = optimize ( logits_obj ) # 10 images, one for each logits Create a combination of multiple objectives and aggregate them from xplique.features_visualizations import Objective from xplique.features_visualizations import optimize # load a model... # target the first logits neuron logits_obj = Objective . neuron ( model , \"logits\" , 0 ) # target the third layer layer_obj = Objective . layer ( model , \"conv2d_1\" ) # target the second channel of another layer channel_obj = Objective . channel ( model , \"mixed4_2\" , 2 ) # combine the objective obj = logits_obj * 1.0 + layer_obj * 3.0 + channel_obj * ( - 5.0 ) images , obj_names = optimize ( logits_obj ) # 1 resulting image","title":"Examples"},{"location":"api/feature_viz/feature_viz/#Objective","text":"Use to combine several sub-objectives into one.","title":"Objective"},{"location":"api/feature_viz/maco/","text":"Modern Feature Visualization (MaCo) \u00b6 View colab tutorial | View source | \ud83d\udcf0 Paper Feature visualization has become increasingly popular, especially after the groundbreaking work by Olah et al. 1 , which established it as a vital tool for enhancing explainability. Despite its significance, the widespread adoption of feature visualization has been hindered by the reliance on various tricks to create interpretable images, making it challenging to scale the method effectively for deeper neural networks. Addressing these limitations, a recent method called MaCo 2 offers a straightforward solution. The core concept involves generating images by optimizing the phase spectrum while keeping the magnitude of the Fourier spectrum constant. This ensures that the generated images reside in the space of natural images in the Fourier domain, providing a more stable and interpretable approach. Quote It is known that human recognition of objects in images is driven not by magnitude but by phase. Motivated by this, we propose to optimize the phase of the Fourier spectrum while fixing its magnitude to a typical value of a natural image (with few high frequencies). In particular, the magnitude is kept constant at the average magnitude computed over a set of natural images (such as ImageNet) MaCo -- Unlocking Feature Visualization for Deeper Networks with MAgnitude Constrained Optimization (2023) 2 To put it more precisely, let \\(\\phi^{(n)}\\) be an explanation of a neuron \\(n\\) , and let \\(x* \\in \\mathcal{X}\\) be the corresponding input defined as: \\[ \\varphi* = \\underset{\\varphi}{arg\\ max}\\ f(\\mathcal{F}^{-1}(r e^{i \\varphi}))^{(n)} \\] where \\(x* = \\mathcal{F}^{-1}(r e^{i \\varphi*})\\) , \\(f(x)^{(n)}\\) represents the neuron score for a given input, and \\(\\mathcal{F}^{-1}\\) denotes the 2-D inverse Fourier transform. In the optimization process, MaCo also generates an alpha mask, which is used to identify the most important area of the generated image. For the purpose of correctly visualizing the image blended with the alpha mask, we provide utilities in the xplique.plot module. Notebooks \u00b6 MaCo : Getting started In this notebook, you'll be introduced to the fundamentals of MaCo while also experimenting with various hyperparameters. Examples \u00b6 To optimize the logit 1 of your neural network (we recommend to remove the softmax activation of your network). from xplique.features_visualizations import Objective from xplique.features_visualizations import maco from xplique.plot import plot_maco # load a model... # targeting the logit 1 of the layer 'logits' # we can also target a layer by its index, like -1 for the last layer logits_obj = Objective . neuron ( model , \"logits\" , 1 ) image , alpha = maco ( logits_obj ) plot_maco ( image , alpha ) Or if you want to visualize a specific CAV (or any direction, like multiple neurons) in your models: from xplique.features_visualizations import Objective from xplique.features_visualizations import maco from xplique.plot import plot_maco # load a model... # cav is a vector of the shape of an activation in the -2 layer # e.g 2048 for Resnet50 logits_obj = Objective . direction ( model , - 2 , cav ) image , alpha = maco ( logits_obj ) plot_maco ( image , alpha ) maco( objective : xplique.features_visualizations.objectives.Objective , optimizer : Union[keras.optimizers.optimizer_v2.optimizer_v2.OptimizerV2, None] = None , nb_steps : int = 256 , noise_intensity : Union[float, Callable, None] = 0.08 , box_size : Union[float, Callable, None] = None , nb_crops : Union[int, None] = 32 , values_range : Tuple[float, float] = (-1, 1) , custom_shape : Union[Tuple, None] = (512, 512)) -> Tuple[tf.Tensor, tf.Tensor] \u00b6 Optimise a single objective using MaCo method. Note that, unlike classic fourier optimization, we can only optimize for one objective at a time. Parameters objective : xplique.features_visualizations.objectives.Objective Objective object. optimizer : Union[keras.optimizers.optimizer_v2.optimizer_v2.OptimizerV2, None] = None Optimizer used for gradient ascent, default Nadam(lr=1.0). nb_steps : int = 256 Number of iterations. noise_intensity : Union[float, Callable, None] = 0.08 Control the noise injected at each step. Either a float : each step we add noise with same std, or a function that associate for each step a noise intensity. box_size : Union[float, Callable, None] = None Control the average size of the crop at each step. Either a fixed float (e.g 0.5 means the crops will be 50% of the image size) or a function that take as parameter the step and return the average box size. Default to linear decay from 50% to 5%. nb_crops : Union[int, None] = 32 Number of crops used at each steps, higher make the optimisation slower but make the results more stable. Default to 32. values_range : Tuple[float, float] = (-1, 1) Range of values of the inputs that will be provided to the model, e.g (0, 1) or (-1, 1). custom_shape : Union[Tuple, None] = (512, 512) If specified, optimizes images of the given size. Used with a low box size to optimize bigger images crop by crop. Return image_optimized : Tuple[tf.Tensor, tf.Tensor] Optimized image for the given objective. transparency : Tuple[tf.Tensor, tf.Tensor] Transparency of the image, i.e the sum of the absolute value of the gradients of the image with respect to the objective. Feature Visualization -- How neural networks build up their understanding of images (2017) \u21a9 MaCo -- Unlocking Feature Visualization for Deeper Networks with MAgnitude Constrained Optimization (2023) \u21a9 \u21a9","title":"Modern Feature Visualization (MaCo)"},{"location":"api/feature_viz/maco/#modern-feature-visualization-maco","text":"View colab tutorial | View source | \ud83d\udcf0 Paper Feature visualization has become increasingly popular, especially after the groundbreaking work by Olah et al. 1 , which established it as a vital tool for enhancing explainability. Despite its significance, the widespread adoption of feature visualization has been hindered by the reliance on various tricks to create interpretable images, making it challenging to scale the method effectively for deeper neural networks. Addressing these limitations, a recent method called MaCo 2 offers a straightforward solution. The core concept involves generating images by optimizing the phase spectrum while keeping the magnitude of the Fourier spectrum constant. This ensures that the generated images reside in the space of natural images in the Fourier domain, providing a more stable and interpretable approach. Quote It is known that human recognition of objects in images is driven not by magnitude but by phase. Motivated by this, we propose to optimize the phase of the Fourier spectrum while fixing its magnitude to a typical value of a natural image (with few high frequencies). In particular, the magnitude is kept constant at the average magnitude computed over a set of natural images (such as ImageNet) MaCo -- Unlocking Feature Visualization for Deeper Networks with MAgnitude Constrained Optimization (2023) 2 To put it more precisely, let \\(\\phi^{(n)}\\) be an explanation of a neuron \\(n\\) , and let \\(x* \\in \\mathcal{X}\\) be the corresponding input defined as: \\[ \\varphi* = \\underset{\\varphi}{arg\\ max}\\ f(\\mathcal{F}^{-1}(r e^{i \\varphi}))^{(n)} \\] where \\(x* = \\mathcal{F}^{-1}(r e^{i \\varphi*})\\) , \\(f(x)^{(n)}\\) represents the neuron score for a given input, and \\(\\mathcal{F}^{-1}\\) denotes the 2-D inverse Fourier transform. In the optimization process, MaCo also generates an alpha mask, which is used to identify the most important area of the generated image. For the purpose of correctly visualizing the image blended with the alpha mask, we provide utilities in the xplique.plot module.","title":"Modern Feature Visualization (MaCo)"},{"location":"api/feature_viz/maco/#notebooks","text":"MaCo : Getting started In this notebook, you'll be introduced to the fundamentals of MaCo while also experimenting with various hyperparameters.","title":"Notebooks"},{"location":"api/feature_viz/maco/#examples","text":"To optimize the logit 1 of your neural network (we recommend to remove the softmax activation of your network). from xplique.features_visualizations import Objective from xplique.features_visualizations import maco from xplique.plot import plot_maco # load a model... # targeting the logit 1 of the layer 'logits' # we can also target a layer by its index, like -1 for the last layer logits_obj = Objective . neuron ( model , \"logits\" , 1 ) image , alpha = maco ( logits_obj ) plot_maco ( image , alpha ) Or if you want to visualize a specific CAV (or any direction, like multiple neurons) in your models: from xplique.features_visualizations import Objective from xplique.features_visualizations import maco from xplique.plot import plot_maco # load a model... # cav is a vector of the shape of an activation in the -2 layer # e.g 2048 for Resnet50 logits_obj = Objective . direction ( model , - 2 , cav ) image , alpha = maco ( logits_obj ) plot_maco ( image , alpha )","title":"Examples"}]}