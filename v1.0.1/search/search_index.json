{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>     \ud83e\udd8a Xplique (pronounced \\\u025bks.plik\\) is a Python toolkit dedicated to explainability, currently based on Tensorflow.     The goal of this library is to gather the state of the art of Explainable AI to help you understand your complex neural network models.     Explore Xplique docs \u00bb Attributions   \u00b7   Concept   \u00b7   Feature Visualization   \u00b7   Metrics </p> <p>The library is composed of several modules, the Attributions Methods module implements various methods (e.g Saliency, Grad-CAM, Integrated-Gradients...), with explanations, examples and links to official papers. The Feature Visualization module allows to see how neural networks build their understanding of images by finding inputs that maximize neurons, channels, layers or compositions of these elements. The Concepts module allows you to extract human concepts from a model and to test their usefulness with respect to a class. Finally, the Metrics module covers the current metrics used in explainability. Used in conjunction with the Attribution Methods module, it allows you to test the different methods or evaluate the explanations of a model.</p> <p> </p> <p></p>"},{"location":"#table-of-contents","title":"\ud83d\udcda Table of contents","text":"<ul> <li>\ud83d\udcda Table of contents</li> <li>\ud83d\udd25 Tutorials</li> <li>\ud83d\ude80 Quick Start</li> <li>\ud83d\udce6 What's Included</li> <li>\ud83d\udcde Callable</li> <li>\ud83d\udc4d Contributing</li> <li>\ud83d\udc40 See Also</li> <li>\ud83d\ude4f Acknowledgments</li> <li>\ud83d\udc68\u200d\ud83c\udf93 Creator</li> <li>\ud83d\uddde\ufe0f Citation</li> <li>\ud83d\udcdd License</li> </ul>"},{"location":"#tutorials","title":"\ud83d\udd25 Tutorials","text":"<p>We propose some Hands-on tutorials to get familiar with the library and its api:</p> <ul> <li>Attribution Methods: Getting started </li> </ul> <p> </p> <ul> <li>Attribution Methods: Sanity checks paper </li> <li>Attribution Methods: Tabular data and Regression </li> <li>Attribution Methods: Metrics </li> </ul> <p> </p> <ul> <li>Concepts Methods: Testing with Concept Activation Vectors </li> </ul> <p> </p> <ul> <li>Feature Visualization: Getting started </li> </ul> <p> </p> <p>You can find a certain number of other practical tutorials just here. This section is actively developed and more contents will be included. We will try to cover all the possible usage of the library, feel free to contact us if you have any suggestions or recommandations towards tutorials you would like to see.</p>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>Xplique requires a version of python higher than 3.6 and several libraries including Tensorflow and Numpy. Installation can be done using Pypi:</p> <pre><code>pip install xplique\n</code></pre> <p>Now that Xplique is installed, here are 4 basic examples of what you can do with the available modules.</p>"},{"location":"#attributions-methods","title":"Attributions Methods","text":"<p>let's start with a simple example, by computing Grad-CAM for several images (or a complete dataset) on a trained model.</p> <pre><code>from xplique.attributions import GradCAM\n\n# load images, labels and model\n# ...\n\nexplainer = GradCAM(model)\nexplanations = explainer.explain(images, labels)\n# or just `explainer(images, labels)`\n</code></pre> <p>All attributions methods share a common API. You can find out more about it here.</p>"},{"location":"#attributions-metrics","title":"Attributions Metrics","text":"<p>In order to measure if the explanations provided by our method are faithful (it reflects well the functioning of the model) we can use a fidelity metric such as Deletion</p> <pre><code>from xplique.attributions import GradCAM\nfrom xplique.metrics import Deletion\n\n# load images, labels and model\n# ...\n\nexplainer = GradCAM(model)\nexplanations = explainer(inputs, labels)\nmetric = Deletion(model, inputs, labels)\n\nscore_grad_cam = metric(explanations)\n</code></pre> <p>All attributions metrics share a common API. You can find out more about it here</p>"},{"location":"#concepts-extraction","title":"Concepts Extraction","text":"<p>Concerning the concept-based methods, we can for example extract a concept vector from a layer of a model. In order to do this, we use two datasets, one containing inputs containing the concept: <code>positive_samples</code>, the other containing other entries which do not contain the concept: <code>negative_samples</code>.</p> <pre><code>from xplique.concepts import Cav\n\n# load a model, samples that contain a concept\n# (positive) and samples who don't (negative)\n# ...\n\nextractor = Cav(model, 'mixed3')\nconcept_vector = extractor(positive_samples,\n                           negative_samples)\n</code></pre> <p>More information on CAV here and on TCAV here.</p>"},{"location":"#feature-visualization","title":"Feature Visualization","text":"<p>Finally, in order to find an image that maximizes a neuron and at the same time a layer, we build two objectives that we combine together. We then call the optimizer which returns our images</p> <pre><code>from xplique.features_visualizations import Objective\nfrom xplique.features_visualizations import optimize\n\n# load a model...\n\nneuron_obj = Objective.neuron(model, \"logits\", 200)\nchannel_obj = Objective.layer(model, \"mixed3\", 10)\n\nobj = neuron_obj + 2.0 * channel_obj\nimages, obj_names = optimize(obj)\n</code></pre> <p>Want to know more ? Check the Feature Viz documentation</p>"},{"location":"#whats-included","title":"\ud83d\udce6 What's Included","text":"<p>All the attributions method presented below handle both Classification and Regression tasks.</p> Attribution Method Type of Model Source Tabular Data Images Time-Series Tutorial Deconvolution TF Paper \u2714 \u2714 WIP Grad-CAM TF Paper \u2714 Grad-CAM++ TF Paper \u2714 Gradient Input TF Paper \u2714 \u2714 WIP Guided Backprop TF Paper \u2714 \u2714 WIP Integrated Gradients TF Paper \u2714 \u2714 WIP Kernel SHAP Callable* Paper \u2714 \u2714 WIP Lime Callable* Paper \u2714 \u2714 WIP Occlusion Callable* Paper \u2714 \u2714 WIP Rise Callable* Paper WIP \u2714 Saliency TF Paper \u2714 \u2714 WIP SmoothGrad TF Paper \u2714 \u2714 WIP SquareGrad TF Paper \u2714 \u2714 WIP VarGrad TF Paper \u2714 \u2714 WIP Sobol Attribution TF Paper \u2714 Hsic Attribution TF Paper \u2714 <ul> <li>: See the Callable documentation</li> </ul> Attribution Metrics Type of Model Property Source MuFidelity TF Fidelity Paper Deletion TF Fidelity Paper Insertion TF Fidelity Paper Average Stability TF Stability Paper MeGe TF Representativity Paper ReCo TF Consistency Paper (WIP) e-robustness Concepts method Type of Model Source Concept Activation Vector (CAV) TF Paper Testing CAV (TCAV) TF Paper (WIP) Robust TCAV (WIP) Automatic Concept Extraction (ACE) Feature Visualization (Paper) Type of Model Details Neurons TF Optimizes for specific neurons Layer TF Optimizes for specific layers Channel TF Optimizes for specific channels Direction TF Optimizes for specific vector Fourrier Preconditioning TF Optimize in Fourier basis (see preconditioning) Objective combination TF Allows to combine objectives methods with TF need a Tensorflow model."},{"location":"#contributing","title":"\ud83d\udc4d Contributing","text":"<p>Feel free to propose your ideas or come and contribute with us on the Xplique toolbox! We have a specific document where we describe in a simple way how to make your first pull request: just here.</p>"},{"location":"#see-also","title":"\ud83d\udc40 See Also","text":"<p>This library is one approach of many to explain your model. We don't expect it to be the perfect  solution; we create it to explore one point in the space of possibilities.</p> <p>Other tools to explain your model include:</p> <ul> <li>Lucid the wonderful library specialized in feature visualization from OpenAI.</li> <li>Captum the Pytorch library for Interpretability research</li> <li>Tf-explain that implement multiples attribution methods and propose callbacks API for tensorflow.</li> <li>Alibi Explain for model inspection and interpretation</li> <li>SHAP a very popular library to compute local explanations using the classic Shapley values from game theory and their related extensions</li> </ul> <p>To learn more about Explainable AI in general, see:</p> <ul> <li>Interpretable Machine Learning by Christophe Molnar.</li> <li>Interpretability Beyond Feature Attribution by Been Kim.</li> <li>Explaining ML Predictions: State-of-the-art, Challenges, and Opportunities by Himabindu Lakkaraju, Julius Adebayo and Sameer Singh.</li> <li>A Roadmap for the Rigorous Science of Interpretability by Finale Doshi-Velez.</li> <li>DEEL White paper a summary of the DEEL team on the challenges of certifiable AI and the role of explainability for this purpose</li> </ul>"},{"location":"#acknowledgments","title":"\ud83d\ude4f Acknowledgments","text":"<p> This project received funding from the French \u201dInvesting for the Future \u2013 PIA3\u201d program within the Artificial and Natural Intelligence Toulouse Institute (ANITI). The authors gratefully acknowledge the support of the  DEEL  project.</p>"},{"location":"#creators","title":"\ud83d\udc68\u200d\ud83c\udf93 Creators","text":"<p>This library was started as a side-project by Thomas FEL who is currently a graduate student at the Artificial and Natural Intelligence Toulouse Institute under the direction of Thomas SERRE. His thesis work focuses on explainability for deep neural networks.</p> <p>He then received help from some members of the  DEEL  team to enhance the library namely from Lucas Hervier and Antonin Poch\u00e9.</p> <p>  Thomas Fel   Lucas Hervier   Antonin Poch\u00e9 </p>"},{"location":"#citation","title":"\ud83d\uddde\ufe0f Citation","text":"<p>If you use Xplique as part of your workflow in a scientific publication, please consider citing the \ud83d\uddde\ufe0f Xplique official paper:</p> <pre><code>@article{fel2022xplique,\n  title={Xplique: A Deep Learning Explainability Toolbox},\n  author={Fel, Thomas and Hervier, Lucas and Vigouroux, David and Poche, Antonin and Plakoo, Justin and Cadene, Remi and Chalvidal, Mathieu and Colin, Julien and Boissin, Thibaut and Bethune, Louis and Picard, Agustin and Nicodeme, Claire \n          and Gardes, Laurent and Flandin, Gregory and Serre, Thomas},\n  journal={Workshop on Explainable Artificial Intelligence for Computer Vision (CVPR)},\n  year={2022}\n}\n</code></pre>"},{"location":"#license","title":"\ud83d\udcdd License","text":"<p>The package is released under  MIT license.</p>"},{"location":"callable/","title":"Callable","text":""},{"location":"callable/#callable-or-models-handle-by-blackbox-attribution-methods","title":"\ud83d\udcde Callable or Models handle by BlackBox Attribution methods","text":"<p>The model can be something else than a <code>tf.keras.Model</code> if it respects one of the following condition: - <code>model(inputs: np.ndarray)</code> return either a <code>np.ndarray</code> or a <code>tf.Tensor</code> of shape \\((N, L)\\) where \\(N\\) is the number of samples and \\(L\\) the number of targets - The model has a <code>scikit-learn</code> API and has a <code>predict_proba</code> function - The model is a <code>xgboost.XGBModel</code> from the XGBoost python library - The model is a TF Lite model. Note this feature is experimental.</p> <p>On the other hand, a PyTorch model can be used with method having Callable as type of model. In order to makes it work you should write a wrapper as follow:</p> <pre><code>class TemplateTorchWrapper(nn.Module):\n  def __init__(self, torch_model):\n    super(TemplateTorchWrapper, self).__init__()\n    self.model = torch_model\n\n  def __call__(self, inputs):\n    # transform your numpy inputs to torch\n    torch_inputs = self._transform_np_inputs(inputs)\n    # mak predictions\n    with torch.no_grad():\n        outputs = self.model(torch_inputs)\n    # convert to numpy\n    outputs = outputs.detach().numpy()\n    # convert to tf.Tensor\n    outputs = tf.cast(outputs, tf.float32)\n    return outputs\n\n  def _transform_np_inputs(self, np_inputs):\n    # include in this function all transformation\n    # needed for your torch model to work, here\n    # for example we swap from channels last to\n    # channels first\n    np_inputs = np.swapaxes(np_inputs, -1, 1)\n    torch_inputs = torch.Tensor(np_inputs)\n    return torch_inputs\n\nwrapped_model = TemplateTorchWrapper(torch_model)\nexplainer = Lime(wrapped_model)\nexplanations = explainer.explain(images, labels)\n</code></pre> <p>As a matter of fact, if the instance of your model doesn't belong to [<code>tf.keras.Model</code>, <code>tf.lite.Interpreter</code>, <code>sklearn.base.BaseEstimator</code>, <code>xgboost.XGBModel</code>] when the explainer will need to make inference the following will happen:</p> <p><pre><code># inputs are automatically transform to tf.Tensor when using an explainer\npred = model(inputs.numpy())\npred = tf.cast(pred, dtype=tf.float32)\nscores = tf.reduce_sum(pred * targets, axis=-1)\n</code></pre> Knowing that, you are free to wrap your model to make it work with our API!</p>"},{"location":"contributing/","title":"Contributing \ud83d\ude4f","text":"<p>Thanks for taking the time to contribute! \ud83c\udf89\ud83d\udc4d</p> <p>From opening a bug report to creating a pull request: every contribution is appreciated and welcome. If you're planning to implement a new feature or change the api please create an issue first. This way we can ensure that your precious work is not in vain.</p>"},{"location":"contributing/#setup-with-make","title":"Setup with make \u2699\ufe0f","text":"<ul> <li>Clone the repo <code>git clone https://github.com/deel-ai/xplique.git</code>.</li> <li>Go to your freshly downloaded repo <code>cd xplique</code></li> <li>Create a virtual environment and install the necessary dependencies for development <code>make prepare-dev &amp;&amp; source xplique_dev_env/bin/activate</code>.</li> <li>You are ready to install the library <code>pip install -e .</code> or run the test suite <code>make test</code>.</li> </ul> <p>Welcome to the team \ud83d\udd25\ud83d\ude80 !</p>"},{"location":"contributing/#setup-without-make","title":"Setup without make \u2699\ufe0f","text":"<ul> <li>Clone the repo <code>git clone https://github.com/deel-ai/xplique.git</code>.</li> <li>Go to your freshly downloaded repo <code>cd xplique</code></li> <li>Install virtualenv with <code>pip</code>: <pre><code>pip install virtualenv\n</code></pre> Or with <code>conda</code>: <pre><code>conda install -c conda-forge virtualenv\n</code></pre></li> <li>Create a new virtual environment <pre><code>venv xplique_dev_env\n</code></pre></li> <li>Activate your new environment <pre><code>. xplique_dev_env/bin/activate\n</code></pre> Depending on your machine, this operation might be slightly different. For instance, on Windows you should probably do (with cmd.exe): <pre><code>~/xplique&gt; path\\to\\xplique_dev_env\\bin\\activate.bat\n</code></pre> Or with Powershell: <pre><code>PS ~/xplique&gt; path\\to\\xplique_dev_env\\bin\\Activate.ps1\n</code></pre> Anyway, if you suceed you should see your virtual environment name in front of any other command: <pre><code>(xplique_dev_env) :~/xplique$\n</code></pre></li> <li>You can now install all necessary packages, with pip: <pre><code>pip install -r requirements.txt\npip install -r requirements_dev.txt\n</code></pre> Or with conda: <pre><code>conda install --file requirements.txt\nconda install --file requirements_dev.txt\n</code></pre></li> <li>You are ready to install the library: <pre><code>pip install -e .\n</code></pre></li> <li>Or run the test suite: <pre><code>tox\n</code></pre></li> </ul> <p>You are now ready to code and to be part of the team \ud83d\udd25\ud83d\ude80 !</p>"},{"location":"contributing/#tests","title":"Tests \u2705","text":"<p>A pretty fair question would be to know what is <code>make test</code> doing ? It is actually just a command which activate your virtual environment and launch the <code>tox</code> command. So basically, if you do not succeed to use <code>make</code> just activate your virtual env and do <code>tox</code> !</p> <p><code>tox</code> on the otherhand will do the following: - run pytest on the tests folder with python 3.6, python 3.7 and python 3.8</p> <p>Note: If you do not have those 3 interpreters the tests would be only performs with your current interpreter - run pylint on the xplique main files, also with python 3.6, python 3.7 and python 3.8 Note: It is possible that pylint throw false-positive errors. If the linting test failed please check first pylint output to point out the reasons.</p> <p>Please, make sure you run all the tests at least once before opening a pull request.</p> <p>A word toward Pylint for those that don't know it:</p> <p>Pylint is a Python static code analysis tool which looks for programming errors, helps enforcing a coding standard, sniffs for code smells and offers simple refactoring suggestions.</p> <p>Basically, it will check that your code follow a certain number of convention. Any Pull Request will go through a Github workflow ensuring that your code respect the Pylint conventions (most of them at least).</p>"},{"location":"contributing/#submitting-changes","title":"Submitting Changes \ud83d\udd03","text":"<p>After getting some feedback, push to your fork and submit a pull request. We may suggest some changes or improvements or alternatives, but for small changes your pull request should be accepted quickly.</p> <p>Something that will increase the chance that your pull request is accepted:</p> <ul> <li>Write tests and ensure that the existing ones pass.</li> <li>If <code>make test</code> is succesful, you have fair chances to pass the CI workflows (linting and test)</li> <li>Follow the existing coding style.</li> <li>Write a good commit message (we follow a lowercase convention).</li> <li>For a major fix/feature make sure your PR has an issue and if it doesn't, please create one. This would help discussion with the community, and polishing ideas in case of a new feature.</li> </ul>"},{"location":"contributing/#documentation","title":"Documentation \ud83d\udcda","text":"<p>Xplique is a small library but documentation is often a huge time sink for users. That's why we greatly appreciate any time spent fixing typos or clarifying sections in the documentation. To setup a local live-server to update the documentation: <code>make serve-doc</code> or activate your virtual env and: <pre><code>CUDA_VISIBLE_DEVICES=-1 mkdocs serve\n</code></pre></p>"},{"location":"tutorials/","title":"Tutorials: Notebooks \ud83d\udcd4","text":"<p>We propose here several tutorials to discover the different functionnalities that the library has to offer.</p> <p>We decided to host those tutorials on Google Colab mainly because you will be able to play the notebooks with a GPU which should greatly improve your User eXperience.</p> <p>Here is the lists of the availables tutorial for now:</p>"},{"location":"tutorials/#getting-started","title":"Getting Started","text":"Tutorial Name Notebook Getting Started Sanity checks for Saliency Maps Tabular data and Regression Metrics Concept Activation Vectors Feature Visualization"},{"location":"tutorials/#attributions","title":"Attributions","text":"Category Tutorial Name Notebook BlackBox KernelShap BlackBox Lime BlackBox Occlusion BlackBox Rise WhiteBox DeconvNet WhiteBox GradCAM WhiteBox GradCAM++ WhiteBox GradientInput WhiteBox GuidedBackpropagation WhiteBox IntegratedGradients WhiteBox Saliency WhiteBox SmoothGrad WhiteBox SquareGrad WhiteBox VarGrad Tabular Data Regression Tabular Data"},{"location":"tutorials/#metrics","title":"Metrics","text":"Category Tutorial Name Notebook Fidelity MuFidelity Fidelity Insertion Fidelity Deletion Stability AverageStability (WIP)"},{"location":"tutorials/#concepts-extraction","title":"Concepts extraction","text":"<p>WIP</p>"},{"location":"tutorials/#features-visualizations","title":"Features Visualizations","text":"<p>WIP</p>"},{"location":"api/attributions/api_attributions/","title":"API: Attributions Methods","text":"<ul> <li>Attribution Methods: Getting started </li> </ul>"},{"location":"api/attributions/api_attributions/#context","title":"Context","text":"<p>In 2013, Simonyan et al. proposed a first attribution method, opening the way to a wide range of approaches which could be defined as follow:</p> <p>Definition</p> <p>The main objective in attributions techniques is to highlight the discriminating variables for decision-making. For instance, with Computer Vision (CV) tasks, the main goal is to underline the pixels contributing the most in the input image(s) leading to the model\u2019s output(s).</p>"},{"location":"api/attributions/api_attributions/#common-api","title":"Common API","text":"<p>All attribution methods inherit from the Base class <code>BlackBoxExplainer</code>. This base class can be initialized with two parameters:</p> <ul> <li><code>model</code>: the model from which we want to obtain attributions (e.g: InceptionV3, ResNet, ...)</li> <li><code>batch_size</code>: an integer which allows to either process inputs per batch (gradient-based methods) or process perturbed samples of an input per batch (inputs are therefore process one by one)</li> </ul> <p>In addition, all class inheriting from <code>BlackBoxExplainer</code> should implement an <code>explain</code> method:</p> <pre><code>@abstractmethod\ndef explain(self,\n            inputs: Union[tf.data.Dataset, tf.Tensor, np.array],\n            targets: Optional[Union[tf.Tensor, np.array]] = None) -&gt; tf.Tensor:\n    raise NotImplementedError()\n\ndef __call__(self,\n             inputs: tf.Tensor,\n             labels: tf.Tensor) -&gt; tf.Tensor:\n\"\"\"Explain alias\"\"\"\n    return self.explain(inputs, labels)\n</code></pre> <p><code>inputs</code>: Must be one of the following: a <code>tf.data.Dataset</code> (in which case you should not provide targets), a <code>tf.Tensor</code> or a <code>np.ndarray</code>.</p> <ul> <li> <p>- If inputs are images, the expected shape of <code>inputs</code> is \\((N, W, H, C)\\) following the TF's conventions where:</p> <ul> <li>- \\(N\\) is the number of inputs</li> <li>- \\(W\\) is the width of the images</li> <li>- \\(H\\) is the height of the images</li> <li>- \\(C\\) is the number of channels (works for \\(C=3\\) or \\(C=1\\), other values might not work or need further customization)</li> </ul> </li> <li> <p>- If inputs are tabular data, the expected shape of <code>inputs</code> is \\((N, W)\\) where:</p> <ul> <li>- \\(N\\) is the number of inputs</li> <li>- \\(W\\) is the feature dimension of a single input</li> </ul> <p>Tip</p> <p>Please refer to the table to see which methods might work with Tabular Data</p> </li> <li> <p>- (Experimental) If inputs are Time Series, the expected shape of <code>inputs</code> is \\((N, T, W)\\)</p> <ul> <li>- \\(N\\) is the number of inputs</li> <li>- \\(T\\) is the temporal dimension of a single input</li> <li> <p>- \\(W\\) is the feature dimension of a single input</p> <p>Warning</p> <p>By default <code>Lime</code> &amp; <code>KernelShap</code> will treat such inputs as grey images. You will need to define a custom <code>map_to_interpret_space</code> when building such interpreters.</p> </li> </ul> <p>Warning</p> <p>If your model is not following the same conventions it might lead to poor results.</p> <p>On the bright side, there is only need for your <code>model</code> to be called on <code>inputs</code> with such shape. Therefore, you can overcome this by writing a wrapper around your model.</p> <p> For example, imagine you have a trained model which takes images with channel first (i.e \\(inputs.shape=(N, C, W, H)\\)). However, we saw that an explainer need images inputs with \\((N, W, H, C)\\) shape. Then, we can wrap the original model and redefine its call function so that it swaps inputs axes before proceding to the original call:</p> </li> </ul> <pre><code>class TemplateModelWrapper(nn.Module):\n    def __init__(self, ncwh_model):\n        super(TemplateModelWrapper, self).__init__()\n        self.model = ncwh_model\n\n    def __call__(self, nwhc_inputs):\n        # transform your NWHC inputs to NCWH inputs\n        nchw_inputs = self._transform_inputs(nwhc_inputs)\n        # make predictions\n        outputs = self.ncwh_model(nchw_inputs)\n\n        return outputs\n\n    def _transform_inputs(self, nwhc_inputs):\n        # include in this function all transformation\n        # needed for your model to work with NWHC inputs\n        # , here for example we swap from channels last\n        # to channels first\n        ncwh_inputs = tf.transpose(nwhc_inputs, [0, 3, 1, 2])\n\n        return ncwh_inputs\n\nwrapped_model = TemplateModelWrapper(model)\nexplainer = Saliency(wrapped_model)\n# images should be (N, W, H, C) for the explain call\nexplanations = explainer.explain(images, labels)\n</code></pre> <p>Warning</p> <p>In any case, when you are out of the scope of the original API, you should take a deep look at the source code to be sure that your Use Case will make sense.</p> <p><code>targets</code>: Must be one of the following: a <code>tf.Tensor</code> or a <code>np.ndarray</code>.</p> <p>Info</p> <p><code>targets</code> should be a one hot encoding of the output you want an explanation of!</p> <ul> <li>- Therefore, targets's shape must match: \\((N, outputs\\_size)\\)<ul> <li>- \\(N\\) is the number of inputs</li> <li>- \\(outputs\\_size\\) is the number of outputs</li> </ul> </li> <li> <p>- For a classification task, the \\(1\\) value should be on the class of interest's (and only this one) index on the outputs. For example, I have three classes ('dogs, 'cats', 'fish') and a classifier with three outputs (the probability to belong to each class). I have an image of a fish and I want to know why my model think it is a fish. Then, the corresponding target of my image will be \\([0, 0, 1]\\)</p> <p>Warning</p> <p>Sometimes the explanation might be non-sense. One possible reason is that your model did not predict at all the output you asked an explanation for. For example, in the previous configuration, the model might have predicted a cat on your fish image. Therefore, you might want to see why it made such a prediction and use \\([0, 1, 0]\\) as target.</p> <p>Tip</p> <p>If you replace \\(1\\) by \\(-1\\) you can also see what goes against an output prediction!</p> <ul> <li>- For a regression task, you might have only one output then one target will be the vector \\([1]\\) (and not the regression value!)</li> </ul> </li> </ul> <p>Even though we made an harmonized API for all attributions methods it might be relevant for the user to distinguish Gradient based and Perturbation based methods, also often referenced respectively as white-box and black-box methods, as their hyperparameters settings might be quite different.</p>"},{"location":"api/attributions/api_attributions/#perturbation-based-approaches","title":"Perturbation-based approaches","text":"<p>Perturbation based methods focus on perturbing an input with a variety of techniques and, with the analysis of the resulting outputs, define an attribution representation. Therefore, there is no need to explicitly know the model architecture as long as forward pass is available, which explain why they are also referenced as black-box methods.</p> <p>Therefore, to use perturbation-based approaches you do not need a TF model. To know more, please see the Callable documentation.</p> <p>Xplique includes the following black-box attributions:</p> Method Name Tutorial KernelShap Lime Occlusion Rise"},{"location":"api/attributions/api_attributions/#gradient-based-approaches","title":"Gradient-based approaches","text":"<p>Those approaches are also called white-box methods as they require a full access to the model architecture, notably it should allow computing gradients with TensorFlow (for Xplique, in general any automatic differentiation framework would work). Indeed, the core idea with the gradient-based approach is to use back-propagation, along other techniques, not to update the model\u2019s weights (which is already trained) but to reveal the most contributing inputs, potentially in a specific layer. Xplique includes the following white-box attributions:</p> Method Name Tutorial DeconvNet GradCAM GradCAM++ GradientInput GuidedBackpropagation IntegratedGradients Saliency SmoothGrad SquareGrad VarGrad <p>In addition, those methods inherits from <code>WhiteBoxExplainer</code> (itself inheriting from <code>BlackBoxExplainer</code>). Thus, an additional <code>__init__</code> argument is added: <code>output_layer</code>. It is the layer to target for the output (e.g logits or after softmax). If an <code>int</code> is provided, it will be interpreted as a layer index, if a <code>string</code> is provided it will look for the layer name. Default to the last layer.</p> <p>Tip</p> <p>It is recommended to use the layer before Softmax</p>"},{"location":"api/attributions/deconvnet/","title":"Deconvnet","text":"<p> View colab tutorial |  View source | \ud83d\udcf0 Paper</p> <p>Deconvnet is one of the first attribution method and was proposed in 2013. Its operation is similar to Saliency: it consists in backpropagating the output score with respect to the input, however, at each non-linearity (the ReLUs), only the positive gradient (even of negative activations) are backpropagated.</p> <p>More precisely, with \\(f\\) our classifier and \\(f_l(x)\\) the activation at layer \\(l\\), we usually have:</p> \\[ \\frac{\\partial f(x)}{\\partial f_{l}(x)} =  \\frac{\\partial f(x)}{\\partial \\text{ReLU}(f_{l}(x))} \\frac{\\partial \\text{ReLU}(f_l(x))}{\\partial f_{l}(x)} = \\frac{\\partial f(x)}{\\partial \\text{ReLU}(f_{l}(x))} \\odot \\mathbb{1}(f_{l}(x)) \\] <p>with \\(\\mathbb{1}(.)\\) the indicator function. With Deconvnet, the backpropagation is modified such that : </p> \\[ \\frac{\\partial f(x)}{\\partial f_{l}(x)} = \\frac{\\partial f(x)}{\\partial \\text{ReLU}(f_{l}(x))} \\odot \\mathbb{1}(\\frac{\\partial f(x)}{\\partial \\text{ReLU}(f_{l}(x))}) \\]"},{"location":"api/attributions/deconvnet/#example","title":"Example","text":"<pre><code>from xplique.attributions import DeconvNet\n\n# load images, labels and model\n# ...\n\nmethod = DeconvNet(model)\nexplanations = method.explain(images, labels)\n</code></pre>"},{"location":"api/attributions/deconvnet/#notebooks","title":"Notebooks","text":"<ul> <li>Attribution Methods: Getting started</li> <li>DeconvNet: Going Further</li> </ul>"},{"location":"api/attributions/deconvnet/#DeconvNet","title":"<code>DeconvNet</code>","text":"<p>Used to compute the DeconvNet method, which modifies the classic Saliency procedure on ReLU's non linearities, allowing only the positive gradients (even from negative inputs) to pass through. </p>"},{"location":"api/attributions/deconvnet/#__init__","title":"<code>__init__(self,  model:  keras.engine.training.Model,  output_layer:  Union[str, int, None] = None,  batch_size:  Optional[int] = 32,  operator:  Optional[Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float]] = None)</code>","text":"<p>Parameters</p> <ul> <li> <p>model            : keras.engine.training.Model </p> <ul> <li><p> The model from which we want to obtain explanations</p> </li> </ul> </li> <li> <p>output_layer            : Union[str, int, None] = None </p> <ul> <li><p> Layer to target for the outputs (e.g logits or after softmax).</p><p> If an <code>int</code> is provided it will be interpreted as a layer index.</p><p> If a <code>string</code> is provided it will look for the layer name.</p><p> Default to the last layer.</p><p> It is recommended to use the layer before Softmax.</p> </li> </ul> </li> <li> <p>batch_size            : Optional[int] = 32 </p> <ul> <li><p> Number of inputs to explain at once, if None compute all at once.</p> </li> </ul> </li> <li> <p>operator            : Optional[Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float]] = None </p> <ul> <li><p> Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y].</p> </li> </ul> </li> </ul>"},{"location":"api/attributions/deconvnet/#explain","title":"<code>explain(self,  inputs:  Union[tf.Dataset, tf.Tensor, numpy.ndarray],  targets:  Union[tf.Tensor, numpy.ndarray, None] = None) -&gt; tf.Tensor</code>","text":"<p>Compute DeconvNet for a batch of samples. Accept Tensor, numpy array or tf.data.Dataset (in that case targets is None) </p> <p>Parameters</p> <ul> <li> <p>inputs            : Union[tf.Dataset, tf.Tensor, numpy.ndarray] </p> <ul> <li><p> Dataset, Tensor or Array. Input samples to be explained.</p><p> If Dataset, targets should not be provided (included in Dataset).</p><p> Expected shape among (N, W), (N, T, W), (N, W, H, C).</p><p> More information in the documentation.</p> </li> </ul> </li> <li> <p>targets            : Union[tf.Tensor, numpy.ndarray, None] = None </p> <ul> <li><p> Tensor or Array. One-hot encoding of the model's output from which an explanation is desired. One encoding per input and only one output at a time. Therefore, the expected shape is (N, output_size).</p><p> More information in the documentation.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>explanations            : tf.Tensor </p> <ul> <li><p> Deconv maps.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/attributions/grad_cam/","title":"GradCAM","text":"<p> View colab tutorial |  View source | \ud83d\udcf0 Paper</p> <p>Grad-CAM is a technique for producing visual explanations that can be used on Convolutional Neural Network (CNN) which uses both gradients and the feature maps of the last convolutional layer.</p> <p>Quote</p> <p>Grad-CAM uses the gradients of any target concept (say logits for \u201cdog\u201d or even a caption), flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept.</p> <p>-- Visual Explanations from Deep Networks via Gradient-based Localization (2016).</p> <p>More precisely, to obtain the localization map for a prediction \\(f(x)\\), we need to compute the weights \\(w_k\\) associated to each of the feature map channel \\(A^k \\in \\mathbb{R}^{W \\times H}\\). As we use the last convolutionnal layer, \\(k\\) will be the number of filters, \\(Z\\) is the number of pixels in each feature map (\\(Z = W \\times H\\), e.g. 7x7 for ResNet50).</p> \\[ w_k = \\frac{1}{Z} \\sum_i \\sum_j \\frac{\\partial f(x)}{\\partial A^k_{i,j}} \\] <p>We now use this weight to ponderate and aggregate the feature maps to obtain our grad-cam attribution \\(\\phi\\):</p> \\[ \\phi = \\text{max}(0, \\sum_k w_k A^k) \\] <p>Notice that \\(\\phi \\in \\mathbb{R}^{W \\times H}\\) and thus the size of the explanation depends on the size of the feature map (\\(W, H\\)) of the last feature map. In order to compare it to the original input \\(x\\), we upsample \\(\\phi\\) using bicubic interpolation.</p>"},{"location":"api/attributions/grad_cam/#example","title":"Example","text":"<pre><code>from xplique.attributions import GradCAM\n\n# load images, labels and model\n# ...\n\nmethod = GradCAM(model)\nexplanations = method.explain(images, labels)\n</code></pre>"},{"location":"api/attributions/grad_cam/#notebooks","title":"Notebooks","text":"<ul> <li>Attribution Methods: Getting started</li> <li>GradCAM: Going Further</li> </ul>"},{"location":"api/attributions/grad_cam/#GradCAM","title":"<code>GradCAM</code>","text":"<p>Used to compute the Grad-CAM visualization method. </p>"},{"location":"api/attributions/grad_cam/#__init__","title":"<code>__init__(self,  model:  keras.engine.training.Model,  output_layer:  Union[str, int, None] = None,  batch_size:  Optional[int] = 32,  operator:  Optional[Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float]] = None,  conv_layer:  Union[str, int, None] = None)</code>","text":"<p>Parameters</p> <ul> <li> <p>model            : keras.engine.training.Model </p> <ul> <li><p> The model from which we want to obtain explanations</p> </li> </ul> </li> <li> <p>output_layer            : Union[str, int, None] = None </p> <ul> <li><p> Layer to target for the outputs (e.g logits or after softmax).</p><p> If an <code>int</code> is provided it will be interpreted as a layer index.</p><p> If a <code>string</code> is provided it will look for the layer name.</p><p>  Default to the last layer.</p><p> It is recommended to use the layer before Softmax.</p> </li> </ul> </li> <li> <p>batch_size            : Optional[int] = 32 </p> <ul> <li><p> Number of inputs to explain at once, if None compute all at once.</p> </li> </ul> </li> <li> <p>operator            : Optional[Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float]] = None </p> <ul> <li><p> Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y].</p> </li> </ul> </li> <li> <p>conv_layer            : Union[str, int, None] = None </p> <ul> <li><p> Layer to target for Grad-CAM algorithm.</p><p> If an int is provided it will be interpreted as a layer index.</p><p> If a string is provided it will look for the layer name.</p> </li> </ul> </li> </ul>"},{"location":"api/attributions/grad_cam/#explain","title":"<code>explain(self,  inputs:  Union[tf.Dataset, tf.Tensor, numpy.ndarray],  targets:  Union[tf.Tensor, numpy.ndarray, None] = None) -&gt; tf.Tensor</code>","text":"<p>Compute and resize explanations to match inputs shape. Accept Tensor, numpy array or tf.data.Dataset (in that case targets is None) </p> <p>Parameters</p> <ul> <li> <p>inputs            : Union[tf.Dataset, tf.Tensor, numpy.ndarray] </p> <ul> <li><p> Dataset, Tensor or Array. Input samples to be explained.</p><p> If Dataset, targets should not be provided (included in Dataset).</p><p> Expected shape among (N, W), (N, T, W), (N, W, H, C).</p><p> More information in the documentation.</p> </li> </ul> </li> <li> <p>targets            : Union[tf.Tensor, numpy.ndarray, None] = None </p> <ul> <li><p> Tensor or Array. One-hot encoding of the model's output from which an explanation is desired. One encoding per input and only one output at a time. Therefore, the expected shape is (N, output_size).</p><p> More information in the documentation.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>grad_cam            : tf.Tensor </p> <ul> <li><p> Grad-CAM explanations, same shape as the inputs except for the channels.</p> </li> </ul> </li> </ul> <p></p> <ol> <li> <p>Visual Explanations from Deep Networks via Gradient-based Localization (2016). \u21a9</p> </li> </ol>"},{"location":"api/attributions/grad_cam_pp/","title":"Grad-CAM ++","text":"<p> View colab tutorial |  View source | \ud83d\udcf0 Paper</p> <p>Grad-CAM++ is a technique for producing visual explanations that can be used on Convolutional Neural Network (CNN) which uses both gradients and the feature maps of the last convolutional layer.</p> <p>More precisely, to obtain the localization map for a prediction \\(f(x)\\), we need to compute the weights \\(w_k\\) associated to each of the feature map channel \\(A^k \\in \\mathbb{R}^{W \\times H}\\). As we use the last convolutionnal layer, \\(k\\) will be the number of filters, \\(Z\\) is the number of pixels in each feature map (\\(Z = W \\times H\\), e.g. 7x7 for ResNet50). once this weights are obtained, we use them to ponderate  and aggregate the feature maps to obtain our grad-cam++ attribution \\(\\phi\\):</p> \\[ \\phi = \\text{max}(0, \\sum_k w_k A^k) \\] <p>Notice that \\(\\phi \\in \\mathbb{R}^{W \\times H}\\) and thus the size of the explanation depends on the size of the feature map (\\(W, H\\)) of the last feature map. In order to compare it to the original input \\(x\\), we upsample \\(\\phi\\) using bicubic interpolation.</p>"},{"location":"api/attributions/grad_cam_pp/#example","title":"Example","text":"<pre><code>from xplique.attributions import GradCAMPP\n\n# load images, labels and model\n# ...\n\nmethod = GradCAMPP(model)\nexplanations = method.explain(images, labels)\n</code></pre>"},{"location":"api/attributions/grad_cam_pp/#notebooks","title":"Notebooks","text":"<ul> <li>Attribution Methods: Getting started</li> <li>GradCAMPP: Going Further</li> </ul>"},{"location":"api/attributions/grad_cam_pp/#GradCAMPP","title":"<code>GradCAMPP</code>","text":"<p>Used to compute the Grad-CAM++ visualization method. </p>"},{"location":"api/attributions/grad_cam_pp/#__init__","title":"<code>__init__(self,  model:  keras.engine.training.Model,  output_layer:  Union[str, int, None] = None,  batch_size:  Optional[int] = 32,  operator:  Optional[Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float]] = None,  conv_layer:  Union[str, int, None] = None)</code>","text":"<p>Parameters</p> <ul> <li> <p>model            : keras.engine.training.Model </p> <ul> <li><p> The model from which we want to obtain explanations</p> </li> </ul> </li> <li> <p>output_layer            : Union[str, int, None] = None </p> <ul> <li><p> Layer to target for the outputs (e.g logits or after softmax).</p><p> If an <code>int</code> is provided it will be interpreted as a layer index.</p><p> If a <code>string</code> is provided it will look for the layer name.</p><p>  Default to the last layer.</p><p> It is recommended to use the layer before Softmax.</p> </li> </ul> </li> <li> <p>batch_size            : Optional[int] = 32 </p> <ul> <li><p> Number of inputs to explain at once, if None compute all at once.</p> </li> </ul> </li> <li> <p>operator            : Optional[Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float]] = None </p> <ul> <li><p> Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y].</p> </li> </ul> </li> <li> <p>conv_layer            : Union[str, int, None] = None </p> <ul> <li><p> Layer to target for Grad-CAM++ algorithm.</p><p> If an int is provided it will be interpreted as a layer index.</p><p> If a string is provided it will look for the layer name.</p> </li> </ul> </li> </ul>"},{"location":"api/attributions/grad_cam_pp/#explain","title":"<code>explain(self,  inputs:  Union[tf.Dataset, tf.Tensor, numpy.ndarray],  targets:  Union[tf.Tensor, numpy.ndarray, None] = None) -&gt; tf.Tensor</code>","text":"<p>Compute and resize explanations to match inputs shape. Accept Tensor, numpy array or tf.data.Dataset (in that case targets is None) </p> <p>Parameters</p> <ul> <li> <p>inputs            : Union[tf.Dataset, tf.Tensor, numpy.ndarray] </p> <ul> <li><p> Dataset, Tensor or Array. Input samples to be explained.</p><p> If Dataset, targets should not be provided (included in Dataset).</p><p> Expected shape among (N, W), (N, T, W), (N, W, H, C).</p><p> More information in the documentation.</p> </li> </ul> </li> <li> <p>targets            : Union[tf.Tensor, numpy.ndarray, None] = None </p> <ul> <li><p> Tensor or Array. One-hot encoding of the model's output from which an explanation is desired. One encoding per input and only one output at a time. Therefore, the expected shape is (N, output_size).</p><p> More information in the documentation.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>grad_cam            : tf.Tensor </p> <ul> <li><p> Grad-CAM explanations, same shape as the inputs except for the channels.</p> </li> </ul> </li> </ul> <p></p> <ol> <li> <p>Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks (2017). \u21a9</p> </li> </ol>"},{"location":"api/attributions/gradient_input/","title":"Gradient \\(\\odot\\) Input","text":"<p> View colab tutorial |  View source | \ud83d\udcf0 Paper</p> <p>Gradient \\(\\odot\\) Input is a visualization techniques based on the gradient of a class score relative to the input, element-wise with the input. This method was introduced by Shrikumar et al., 20161, in an old version of their DeepLIFT paper2.</p> <p>Quote</p> <p>Gradient inputs was at first proposed as a technique to improve the sharpness of the attribution maps. The attribution is computed taking the (signed) partial derivatives of the output with respect to the input and multiplying them with the input itself.</p> <p>-- Towards better understanding of the gradient-based attribution methods for Deep Neural Networks (2017)3</p> <p>A theoretical analysis conducted by Ancona et al, 20183 showed that Gradient \\(\\odot\\) Input is equivalent to \\(\\epsilon\\)-LRP and DeepLift methods under certain conditions: using a baseline of zero, and with all biases to zero.</p> <p>More precisely, the explanation \\(\\phi\\) for an input \\(x\\) and a classifier \\(f\\) is defined as</p> \\[ \\phi = x \\odot \\nabla_x f(x) \\] <p>with \\(\\odot\\) the Hadamard product.</p>"},{"location":"api/attributions/gradient_input/#example","title":"Example","text":"<pre><code>from xplique.attributions import GradientInput\n\n# load images, labels and model\n# ...\n\nmethod = GradientInput(model)\nexplanations = method.explain(images, labels)\n</code></pre>"},{"location":"api/attributions/gradient_input/#notebooks","title":"Notebooks","text":"<ul> <li>Attribution Methods: Getting started </li> <li>Gradient \\(\\odot\\) Input: Going Further </li> </ul>"},{"location":"api/attributions/gradient_input/#GradientInput","title":"<code>GradientInput</code>","text":"<p>Used to compute elementwise product between the saliency maps of Simonyan et al. and the input (Gradient x Input). </p>"},{"location":"api/attributions/gradient_input/#__init__","title":"<code>__init__(self,  model:  keras.engine.training.Model,  output_layer:  Union[str, int, None] = None,  batch_size:  Optional[int] = 64,  operator:  Optional[Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float]] = None)</code>","text":"<p>Parameters</p> <ul> <li> <p>model            : keras.engine.training.Model </p> <ul> <li><p> The model from which we want to obtain explanations</p> </li> </ul> </li> <li> <p>output_layer            : Union[str, int, None] = None </p> <ul> <li><p> Layer to target for the outputs (e.g logits or after softmax).</p><p> If an <code>int</code> is provided it will be interpreted as a layer index.</p><p> If a <code>string</code> is provided it will look for the layer name.</p><p>  Default to the last layer.</p><p> It is recommended to use the layer before Softmax.</p> </li> </ul> </li> <li> <p>batch_size            : Optional[int] = 64 </p> <ul> <li><p> Number of inputs to explain at once, if None compute all at once.</p> </li> </ul> </li> <li> <p>operator            : Optional[Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float]] = None </p> <ul> <li><p> Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y].</p> </li> </ul> </li> </ul>"},{"location":"api/attributions/gradient_input/#explain","title":"<code>explain(self,  inputs:  Union[tf.Dataset, tf.Tensor, numpy.ndarray],  targets:  Union[tf.Tensor, numpy.ndarray, None] = None) -&gt; tf.Tensor</code>","text":"<p>Compute gradients x inputs for a batch of samples. </p> <p>Parameters</p> <ul> <li> <p>inputs            : Union[tf.Dataset, tf.Tensor, numpy.ndarray] </p> <ul> <li><p> Dataset, Tensor or Array. Input samples to be explained.</p><p> If Dataset, targets should not be provided (included in Dataset).</p><p> Expected shape among (N, W), (N, T, W), (N, W, H, C).</p><p> More information in the documentation.</p> </li> </ul> </li> <li> <p>targets            : Union[tf.Tensor, numpy.ndarray, None] = None </p> <ul> <li><p> Tensor or Array. One-hot encoding of the model's output from which an explanation is desired. One encoding per input and only one output at a time. Therefore, the expected shape is (N, output_size).</p><p> More information in the documentation.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>explanations            : tf.Tensor </p> <ul> <li><p> Gradients x Inputs, with the same shape as the inputs.</p> </li> </ul> </li> </ul> <p></p> <ol> <li> <p>Not Just a Black Box: Learning Important Features Through Propagating Activation Differences \u21a9</p> </li> <li> <p>Learning Important Features Through Propagating Activation Differences \u21a9</p> </li> <li> <p>Towards better understanding of gradient-based attribution methods for Deep Neural Networks \u21a9\u21a9</p> </li> </ol>"},{"location":"api/attributions/guided_backpropagation/","title":"Guided Backpropagation","text":"<p> View colab tutorial |  View source | \ud83d\udcf0 Paper</p> <p>Guided-backprop is one of the first attribution method and was proposed in 2014. Its operation is similar to Saliency: it consists in backpropagating the output score with respect to the input,  however, at each non-linearity (the ReLUs), only the positive gradient of positive activations are backpropagated. We can see this as a filter on the backprop.</p> <p>More precisely, with \\(f\\) our classifier and \\(f_l(x)\\) the activation at layer \\(l\\), we usually have:</p> \\[ \\frac{\\partial f(x)}{\\partial f_{l}(x)} =  \\frac{\\partial f(x)}{\\partial \\text{ReLU}(f_{l}(x))} \\frac{\\partial \\text{ReLU}(f_l(x))}{\\partial f_{l}(x)} = \\frac{\\partial f(x)}{\\partial \\text{ReLU}(f_{l}(x))} \\odot \\mathbb{1}(f_{l}(x)) \\] <p>with \\(\\mathbb{1}(.)\\) the indicator function. With Guided-backprop, the backpropagation is modified such that : </p> \\[ \\frac{\\partial f(x)}{\\partial f_{l}(x)} =   \\frac{\\partial f(x)}{\\partial \\text{ReLU}(f_{l}(x))} \\odot \\mathbb{1}(f_{l}(x)) \\odot \\mathbb{1}(\\frac{\\partial f(x)}{\\partial \\text{ReLU}(f_{l}(x))}) \\]"},{"location":"api/attributions/guided_backpropagation/#example","title":"Example","text":"<pre><code>from xplique.attributions import GuidedBackprop\n\n# load images, labels and model\n# ...\n\nmethod = GuidedBackprop(model)\nexplanations = method.explain(images, labels)\n</code></pre>"},{"location":"api/attributions/guided_backpropagation/#notebooks","title":"Notebooks","text":"<ul> <li>Attribution Methods: Getting started</li> <li>Guided Backprop: Going Further</li> </ul>"},{"location":"api/attributions/guided_backpropagation/#GuidedBackprop","title":"<code>GuidedBackprop</code>","text":"<p>Used to compute the Guided Backpropagation, which modifies the classic Saliency procedure on ReLU's non linearities, allowing only the positive gradients from positive activations to pass through. </p>"},{"location":"api/attributions/guided_backpropagation/#__init__","title":"<code>__init__(self,  model:  keras.engine.training.Model,  output_layer:  Union[str, int, None] = None,  batch_size:  Optional[int] = 32,  operator:  Optional[Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float]] = None)</code>","text":"<p>Parameters</p> <ul> <li> <p>model            : keras.engine.training.Model </p> <ul> <li><p> The model from which we want to obtain explanations</p> </li> </ul> </li> <li> <p>output_layer            : Union[str, int, None] = None </p> <ul> <li><p> Layer to target for the outputs (e.g logits or after softmax).</p><p> If an <code>int</code> is provided it will be interpreted as a layer index.</p><p> If a <code>string</code> is provided it will look for the layer name.</p><p>  Default to the last layer.</p><p> It is recommended to use the layer before Softmax.</p> </li> </ul> </li> <li> <p>batch_size            : Optional[int] = 32 </p> <ul> <li><p> Number of inputs to explain at once, if None compute all at once.</p> </li> </ul> </li> <li> <p>operator            : Optional[Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float]] = None </p> <ul> <li><p> Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y].</p> </li> </ul> </li> </ul>"},{"location":"api/attributions/guided_backpropagation/#explain","title":"<code>explain(self,  inputs:  Union[tf.Dataset, tf.Tensor, numpy.ndarray],  targets:  Union[tf.Tensor, numpy.ndarray, None] = None) -&gt; tf.Tensor</code>","text":"<p>Compute Guided Backpropagation for a batch of samples. </p> <p>Parameters</p> <ul> <li> <p>inputs            : Union[tf.Dataset, tf.Tensor, numpy.ndarray] </p> <ul> <li><p> Dataset, Tensor or Array. Input samples to be explained.</p><p> If Dataset, targets should not be provided (included in Dataset).</p><p> Expected shape among (N, W), (N, T, W), (N, W, H, C).</p><p> More information in the documentation.</p> </li> </ul> </li> <li> <p>targets            : Union[tf.Tensor, numpy.ndarray, None] = None </p> <ul> <li><p> Tensor or Array. One-hot encoding of the model's output from which an explanation is desired. One encoding per input and only one output at a time. Therefore, the expected shape is (N, output_size).</p><p> More information in the documentation.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>explanations            : tf.Tensor </p> <ul> <li><p> Guided Backpropagation maps.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/attributions/hsic/","title":"Hsic Attribution Method","text":"<p> View colab tutorial |   View source | \ud83d\udcf0 Paper</p> <p>The Hsic attribution method from Novello, Fel, Vigouroux1 explains a neural network's prediction for a given input image by assessing the dependence between the output and patches of the input. Thanks to the sample efficiency of HSIC Estimator, this black box method requires fewer forward passes to produce relevant explanations.</p> <p>Let's consider two random variables which are the perturbation associated with each patch of the input image, \\(X_i, i \\in \\{1,...d\\}\\) with \\(d= \\text{grid_size}^2\\) image patches and the output \\(Y\\). Let \\(X^1_i,...,X^p_i\\) and \\(Y^1,...,Y^p\\) be \\(p\\) samples of \\(X_i\\) and \\(Y\\). HSIC attribution method requires selecting a kernel for the input and the output to construct an RKHS on which is computed the Maximum Mean Discrepancy, a dissimilarity metric between distributions. Let \\(k:\\mathbb{R}^2 \\rightarrow \\mathbb{R}\\) and \\(l:\\mathbb{R}^2 \\rightarrow \\mathbb{R}\\) the kernels selected for \\(X_i\\) and \\(Y\\), HSIC is estimated with an error \\(\\mathcal{O}(1/\\sqrt{p})\\) using the estimator  $$ \\mathcal{H}^p_{X_i, Y} = \\frac{1}{(p-1)^2} \\operatorname{tr} (KHLH), $$ where \\(H, L, K \\in \\mathbb{R}^{p \\times p}\\) and \\(K_{ij} = k(x_i, x_j), L_{i,j} = l(y_i, y_j)\\) and \\(H_{ij} = \\delta(i=j) - p^{-1}\\) where \\(\\delta(i=j) = 1\\) if \\(i=j\\) and \\(0\\) otherwise.</p> <p>In the paper Making Sense of Dependence: Efficient Black-box Explanations Using Dependence Measure,  the sampler <code>LatinHypercube</code> is used to sample the perturbations. Note however that the present implementation uses <code>TFSobolSequence</code> as default sampler because <code>LatinHypercube</code> requires scipy \\(\\geq\\) <code>1.7.0</code>. you can nevertheless use this sampler -- which is included in the library -- by specifying it during the init of your explainer. </p> <p>For the kernel \\(k\\) applied on \\(X_i\\), a modified Dirac kernel is used to enable an ANOVA-like decomposition property that allows assessing pairwise patch interactions (see the paper for more details). For the kernel \\(l\\) of output \\(Y\\), a Radial Basis Function (RBF) is used.</p> <p>Tip</p> <p>We recommend using a grid size of \\(7 \\times 7\\) to define the image patches. The paper uses a number of forwards of \\(1500\\) to obtain the most faithful explanations and \\(750\\) for a more budget - but still faithful - version.</p>"},{"location":"api/attributions/hsic/#example","title":"Example","text":"<p>Low budget version</p> <pre><code>from xplique.attributions import HsicAttributionMethod\n\n# load images, labels and model\n# ...\n\nexplainer = HsicAttributionMethod(model, grid_size=7, nb_design=750)\nexplanations = explainer(images, labels)\n</code></pre> <p>High budget version</p> <pre><code>from xplique.attributions import HsicAttributionMethod\n\n# load images, labels and model\n# ...\n\nexplainer = HsicAttributionMethod(model, grid_size=7, nb_design=1500)\nexplanations = explainer(images, labels)\n</code></pre> <p>Recommended version, (you need scipy \\(\\geq\\) <code>1.7.0</code>)</p> <pre><code>from xplique.attributions import HsicAttributionMethod\nfrom xplique.attributions.global_sensitivity_analysis import LatinHypercube\n\n# load images, labels and model\n# ...\n\nexplainer = HsicAttributionMethod(model, \n                                  grid_size=7, nb_design=1500,\n                                  sampler = LatinHypercube(binary=True))\nexplanations = explainer(images, labels)\n</code></pre>"},{"location":"api/attributions/hsic/#notebooks","title":"Notebooks","text":"<ul> <li>Attribution Methods: Getting started </li> </ul>"},{"location":"api/attributions/hsic/#HsicAttributionMethod","title":"<code>HsicAttributionMethod</code>","text":"<p>HSIC Attribution Method. Compute the dependance of each input dimension wrt the output using Hilbert-Schmidt Independance Criterion, a perturbation function on a grid and an adapted sampling as described in the original paper. </p>"},{"location":"api/attributions/hsic/#__init__","title":"<code>__init__(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 model,  grid_size:  int = 8,  nb_design:  int = 500,  sampler:  Optional[xplique.attributions.global_sensitivity_analysis.samplers.Sampler] = None,  estimator:  Optional[xplique.attributions.global_sensitivity_analysis.hsic_estimators.HsicEstimator] = None,  perturbation_function:  Union[Callable, str, None] = 'inpainting',\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 batch_size=256,  operator:  Optional[Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float]] = None)</code>","text":"<p>Parameters</p> <ul> <li> <p>model            : model </p> <ul> <li><p> Model used for computing explanations.</p> </li> </ul> </li> <li> <p>grid_size            : int = 8 </p> <ul> <li><p> Cut the image in a grid of (grid_size, grid_size) to estimate an indice per cell.</p> </li> </ul> </li> <li> <p>nb_design            : int = 500 </p> <ul> <li><p> Number of design for the sampler.</p> </li> </ul> </li> <li> <p>sampler            : Optional[xplique.attributions.global_sensitivity_analysis.samplers.Sampler] = None </p> <ul> <li><p> Sampler used to generate the (quasi-)monte carlo samples, LHS or QMC.</p><p> For more option, see the sampler module. Note that the original paper uses LHS but here the default sampler is TFSobolSequence as LHS requires scipy 1.7.0.</p> </li> </ul> </li> <li> <p>estimator            : Optional[xplique.attributions.global_sensitivity_analysis.hsic_estimators.HsicEstimator] = None </p> <ul> <li><p> Estimator used to compute the HSIC score.</p> </li> </ul> </li> <li> <p>perturbation_function            : Union[Callable, str, None] = 'inpainting' </p> <ul> <li><p> Function to call to apply the perturbation on the input. Can also be string in 'inpainting', 'blur'.</p> </li> </ul> </li> <li> <p>batch_size            : batch_size=256 </p> <ul> <li><p> Batch size to use for the forwards.</p> </li> </ul> </li> <li> <p>operator            : Optional[Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float]] = None </p> <ul> <li><p> Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y].</p> </li> </ul> </li> </ul>"},{"location":"api/attributions/hsic/#explain","title":"<code>explain(self,  inputs:  Union[tf.Dataset, tf.Tensor, numpy.ndarray],  targets:  Union[tf.Tensor, numpy.ndarray, None] = None) -&gt; tf.Tensor</code>","text":"<p>Compute the total Sobol' indices according to the explainer parameter (perturbation function, grid size...). Accept Tensor, numpy array or tf.data.Dataset (in that case targets is None). </p> <p>Parameters</p> <ul> <li> <p>inputs            : Union[tf.Dataset, tf.Tensor, numpy.ndarray] </p> <ul> <li><p> Images to be explained, either tf.dataset, Tensor or numpy array.</p><p> If Dataset, targets should not be provided (included in Dataset).</p><p> Expected shape (N, W, H, C) or (N, W, H).</p> </li> </ul> </li> <li> <p>targets            : Union[tf.Tensor, numpy.ndarray, None] = None </p> <ul> <li><p> One-hot encoding for classification or direction {-1, +1} for regression.</p><p> Tensor or numpy array.</p><p> Expected shape (N, C) or (N).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>attributions_maps            : tf.Tensor </p> <ul> <li><p> GSA Attribution Method explanations, same shape as the inputs except for the channels.</p> </li> </ul> </li> </ul> <p></p> <ol> <li> <p>Making Sense of Dependence: Efficient Black-box Explanations Using Dependence Measure (2022) \u21a9</p> </li> </ol>"},{"location":"api/attributions/integrated_gradients/","title":"Integrated Gradients","text":"<p> View colab tutorial |  View source | \ud83d\udcf0 Paper</p> <p>Integrated Gradients is a visualization technique resulting of a theoretical search for an explanatory method that satisfies two axioms, Sensitivity and Implementation Invariance (Sundararajan et al1).</p> <p>Quote</p> <p>We consider the straightline path (in \\(R^n\\)) from the baseline \\(\\bar{x}\\) to the input \\(x\\), and compute the gradients at all points along the path. Integrated gradients are obtained by cumulating these gradients.</p> <p>-- Axiomatic Attribution for Deep Networks (2017)1</p> <p>Rather than calculating only the gradient relative to the image, the method consists of averaging the gradient values along the path from a baseline state to the current value. The baseline state is often set to zero, representing the complete absence of features.</p> <p>More precisely, with \\(x_0\\) the baseline state, \\(x\\) the image and \\(f\\) our classifier,  the Integrated Gradient attribution is defined as</p> \\[\\phi = (x - x_0) \\cdot \\int_0^1{ \\nabla_x f(x_0 + \\alpha (x - x_0) ) d\\alpha }\\] <p>In order to approximate from a finite number of steps, the implementation here use the Trapezoidal rule3 and not a left-Riemann summation, which allows for more accurate results and improved performance. (see the paper below for a comparison of the methods2).</p>"},{"location":"api/attributions/integrated_gradients/#example","title":"Example","text":"<pre><code>from xplique.attributions import IntegratedGradients\n\n# load images, labels and model\n# ...\n\nmethod = IntegratedGradients(model, steps=50, baseline_value=0.0)\nexplanations = method.explain(images, labels)\n</code></pre>"},{"location":"api/attributions/integrated_gradients/#notebooks","title":"Notebooks","text":"<ul> <li>Attribution Methods: Getting started</li> <li>Integrated Gradients: Going Further</li> </ul>"},{"location":"api/attributions/integrated_gradients/#IntegratedGradients","title":"<code>IntegratedGradients</code>","text":"<p>Used to compute the Integrated Gradients, by cumulating the gradients along a path from a baseline to the desired point. </p>"},{"location":"api/attributions/integrated_gradients/#__init__","title":"<code>__init__(self,  model:  keras.engine.training.Model,  output_layer:  Union[str, int, None] = None,  batch_size:  Optional[int] = 32,  operator:  Optional[Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float]] = None,  steps:  int = 50,  baseline_value:  float = 0.0)</code>","text":"<p>Parameters</p> <ul> <li> <p>model            : keras.engine.training.Model </p> <ul> <li><p> The model from which we want to obtain explanations</p> </li> </ul> </li> <li> <p>output_layer            : Union[str, int, None] = None </p> <ul> <li><p> Layer to target for the outputs (e.g logits or after softmax).</p><p> If an <code>int</code> is provided it will be interpreted as a layer index.</p><p> If a <code>string</code> is provided it will look for the layer name.</p><p>  Default to the last layer.</p><p> It is recommended to use the layer before Softmax.</p> </li> </ul> </li> <li> <p>batch_size            : Optional[int] = 32 </p> <ul> <li><p> Number of inputs to explain at once, if None compute all at once.</p> </li> </ul> </li> <li> <p>operator            : Optional[Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float]] = None </p> <ul> <li><p> Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y].</p> </li> </ul> </li> <li> <p>steps            : int = 50 </p> <ul> <li><p> Number of points to interpolate between the baseline and the desired point.</p> </li> </ul> </li> <li> <p>baseline_value            : float = 0.0 </p> <ul> <li><p> Scalar used to create the the baseline point.</p> </li> </ul> </li> </ul>"},{"location":"api/attributions/integrated_gradients/#explain","title":"<code>explain(self,  inputs:  Union[tf.Dataset, tf.Tensor, numpy.ndarray],  targets:  Union[tf.Tensor, numpy.ndarray, None] = None) -&gt; tf.Tensor</code>","text":"<p>Compute Integrated Gradients for a batch of samples. </p> <p>Parameters</p> <ul> <li> <p>inputs            : Union[tf.Dataset, tf.Tensor, numpy.ndarray] </p> <ul> <li><p> Dataset, Tensor or Array. Input samples to be explained.</p><p> If Dataset, targets should not be provided (included in Dataset).</p><p> Expected shape among (N, W), (N, T, W), (N, W, H, C).</p><p> More information in the documentation.</p> </li> </ul> </li> <li> <p>targets            : Union[tf.Tensor, numpy.ndarray, None] = None </p> <ul> <li><p> Tensor or Array. One-hot encoding of the model's output from which an explanation is desired. One encoding per input and only one output at a time. Therefore, the expected shape is (N, output_size).</p><p> More information in the documentation.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>explanations            : tf.Tensor </p> <ul> <li><p> Integrated gradients, same shape as the inputs.</p> </li> </ul> </li> </ul> <p></p> <ol> <li> <p>Axiomatic Attribution for Deep Networks \u21a9\u21a9</p> </li> <li> <p>Computing Linear Restrictions of Neural Networks \u21a9</p> </li> <li> <p>Trapezoidal rule \u21a9</p> </li> </ol>"},{"location":"api/attributions/kernel_shap/","title":"Kernel Shap","text":"<p> View colab tutorial |  View source</p> <p>By setting appropriately the perturbation function, the similarity kernel and the interpretable model in the LIME framework we can theoretically obtain the Shapley Values more efficiently. Therefore, KernelShap is a method based on LIME with specific attributes.</p> <p>Quote</p> <p>The exact computation of SHAP values is challenging. However, by combining insights from current additive feature attribution methods, we can approximate them. We describe two model-agnostic approximation methods, [...] and another that is novel (Kernel SHAP)</p> <p>-- A Unified Approach to Interpreting Model Predictions1</p>"},{"location":"api/attributions/kernel_shap/#example","title":"Example","text":"<pre><code>from xplique.attributions import KernelShap\n\n# load images, labels and model\n# define a custom map_to_interpret_space function\n# ...\n\nmethod = KernelShap(model, map_to_interpret_space=custom_map)\nexplanations = method.explain(images, labels)\n</code></pre> <p>The choice of the map function will have a great deal toward the quality of explanation. By default, the map function use the quickshift segmentation of scikit-images</p>"},{"location":"api/attributions/kernel_shap/#notebooks","title":"Notebooks","text":"<ul> <li>Attribution Methods: Getting started</li> <li>KernelShap: Going Further</li> </ul>"},{"location":"api/attributions/kernel_shap/#KernelShap","title":"<code>KernelShap</code>","text":"<p>By setting appropriately the perturbation function, the similarity kernel and the interpretable model in the LIME framework we can theoretically obtain the Shapley Values more efficiently. Therefore, KernelShap is a method based on LIME with specific attributes. </p>"},{"location":"api/attributions/kernel_shap/#__init__","title":"<code>__init__(self,  model:  Callable,  batch_size:  int = 64,  operator:  Optional[Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float]] = None,  map_to_interpret_space:  Optional[Callable] = None,  nb_samples:  int = 800,  ref_value:  Optional[numpy.ndarray] = None)</code>","text":"<p>Parameters</p> <ul> <li> <p>model            : Callable </p> <ul> <li><p> The model from which we want to obtain explanations.</p> </li> </ul> </li> <li> <p>batch_size            : int = 64 </p> <ul> <li><p> Number of perturbed samples to process at once, mandatory when nb_samples is huge.</p><p> Notice, it is different compare to WhiteBox explainers which batch the inputs.</p><p> Here inputs are process one by one.</p> </li> </ul> </li> <li> <p>operator            : Optional[Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float]] = None </p> <ul> <li><p> Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y].</p> </li> </ul> </li> <li> <p>map_to_interpret_space            : Optional[Callable] = None </p> <ul> <li><p> Function which group features of an input corresponding to the same interpretable feature (e.g super-pixel).</p><p> It allows to transpose from (resp. to) the original input space to (resp. from) the interpretable space.</p> </li> </ul> </li> <li> <p>nb_samples            : int = 800 </p> <ul> <li><p> The number of perturbed samples you want to generate for each input sample.</p><p> Default to 800.</p> </li> </ul> </li> <li> <p>ref_value            : Optional[numpy.ndarray] = None </p> <ul> <li><p> It defines reference value which replaces each feature when the corresponding interpretable feature is set to 0.</p><p> It should be provided as: a ndarray of shape (1) if there is no channels in your input and (C,) otherwise.</p><p> The default ref value is set to (0.5,0.5,0.5) for inputs with 3 channels (corresponding to a grey pixel when inputs are normalized by 255) and to 0 otherwise.</p> </li> </ul> </li> </ul>"},{"location":"api/attributions/kernel_shap/#explain","title":"<code>explain(self,  inputs:  Union[tf.Dataset, tf.Tensor, numpy.ndarray],  targets:  Union[tf.Tensor, numpy.ndarray, None] = None) -&gt; tf.Tensor</code>","text":"<p>This method attributes the output of the model with given targets to the inputs of the model using the approach described above, training an interpretable model and returning a representation of the interpretable model. </p> <p>Parameters</p> <ul> <li> <p>inputs            : Union[tf.Dataset, tf.Tensor, numpy.ndarray] </p> <ul> <li><p> Dataset, Tensor or Array. Input samples to be explained.</p><p> If Dataset, targets should not be provided (included in Dataset).</p><p> Expected shape among (N, W), (N, T, W), (N, W, H, C).</p><p> More information in the documentation.</p> </li> </ul> </li> <li> <p>targets            : Union[tf.Tensor, numpy.ndarray, None] = None </p> <ul> <li><p> Tensor or Array. One-hot encoding of the model's output from which an explanation is desired. One encoding per input and only one output at a time. Therefore, the expected shape is (N, output_size).</p><p> More information in the documentation.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>explanations            : tf.Tensor </p> <ul> <li><p> Interpretable coefficients, same shape as the inputs, except for the channels.</p><p> Coefficients of the interpretable model. Those coefficients having the size of the interpretable space will be given the same value to coefficient which were grouped together (e.g belonging to the same super-pixel).</p> </li> </ul> </li> </ul> <p></p> <p>Warning</p> <p>The computation time might be very long depending on the hyperparameters settings. A huge number of perturbed samples and a fine-grained mapping may lead to better results but it is long to compute.</p>"},{"location":"api/attributions/kernel_shap/#parameters-in-depth","title":"Parameters in-depth","text":""},{"location":"api/attributions/kernel_shap/#map_to_interpret_space","title":"<code>map_to_interpret_space</code>:","text":"<p>Function which group features of an input corresponding to the same interpretable feature (e.g super-pixel).</p> <p>It allows to transpose from (resp. to) the original input space to (resp. from) the interpretable space.</p> <p>The default mappings are:</p> <ul> <li>- the quickshift segmentation algorithm for inputs with \\((N, W, H, C)\\) shape, we assume here such shape is used to represent \\((W, H, C)\\) images.</li> <li>- the felzenszwalb segmentation algorithm for inputs with \\((N, W, H)\\) shape, we assume here such shape is used to represent \\((W, H)\\) images.</li> <li>- an identity mapping if inputs has shape \\((N, W)\\), we assume here your inputs are tabular data.</li> </ul> <p>To use your own custom map function you should use the following scheme:</p> <pre><code>def custom_map_to_interpret_space(inputs: tf.tensor) -&gt;\ntf.tensor:\n    **some grouping techniques**\n    return mappings\n</code></pre> <p><code>mappings</code> should have the same dimension as input except for channels.</p> <p>For instance you can use the scikit-image (as we did for the quickshift algorithm) library to defines super pixels on your images.</p> <p>Info</p> <p>The quality of your explanation relies strongly on this mapping.</p> <p>Warning</p> <p>Depending on the mapping you might have a huge number of <code>interpretable_features</code>  (e.g you map pixels 2 by 2 on a 299x299 image). Thus, the compuation time might be very long!</p> <p>Danger</p> <p>As you may have noticed, by default Time Series are not handled. Consequently, a custom mapping should be implented. Either to assign each feature to a different group or to group consecutive features together, by group of 4 timesteps for example. In the second example, we try to cover patterns. An example is provided below.</p> <pre><code>def map_time_series(single_input: tf.tensor) -&gt; tf.Tensor:\n    time_dim = single_input.shape[0]\n    feat_dim = single_input.shape[1]\n    mapping = tf.range(time_dim*feat_dim)\n    mapping = tf.reshape(mapping, (time_dim, feat_dim))\n    return mapping\n</code></pre> <ol> <li> <p>A Unified Approach to Interpreting Model Predictions \u21a9</p> </li> </ol>"},{"location":"api/attributions/lime/","title":"LIME","text":"<p> View colab tutorial |  View source</p> <p>The Lime method use an interpretable model to provide an explanation. More specifically, you map inputs (\\(x \\in R^d\\)) to an interpretable space (e.g super-pixels) of size num_interpetable_features. From there you generate perturbed interpretable samples (\\(z' \\in \\{0,1\\}^{num\\_interpretable\\_samples}\\) where \\(1\\) means we keep this specific interpretable feature).</p> <p>Once you have your interpretable samples you can map them back to their original space (the perturbed samples \\(z \\in R^d\\)) and obtain the label prediction of your model for each perturbed samples.</p> <p>In the Lime method you define a similarity kernel which compute the similarity between an input and its perturbed representations (either in the original input space or in the interpretable space): \\(\\pi_x(z',z)\\).</p> <p>Finally, you train an interpretable model per input, using interpretable samples along the corresponding perturbed labels and it will draw interpretable samples weighted by the similarity kernel. Thus, you will have an interpretable explanation (i.e in the interpretable space) which can be broadcasted afterwards to the original space considering the mapping you used.</p> <p>Quote</p> <p>The overall goal of LIME is to identify an interpretable model over the interpretable representation that is locally faithful to the classifier.</p> <p>-- \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier.1</p>"},{"location":"api/attributions/lime/#example","title":"Example","text":"<pre><code>from xplique.attributions import Lime\n\n# load images, labels and model\n# define a custom map_to_interpret_space function\n# ...\n\nmethod = Lime(model, map_to_interpret_space=custom_map)\nexplanations = method.explain(images, labels)\n</code></pre> <p>The choice of the interpretable model and the map function will have a great deal toward the quality of explanation. By default, the map function use the quickshift segmentation of scikit-images</p>"},{"location":"api/attributions/lime/#notebooks","title":"Notebooks","text":"<ul> <li>Attribution Methods: Getting started</li> <li>LIME: Going Further</li> </ul>"},{"location":"api/attributions/lime/#Lime","title":"<code>Lime</code>","text":"<p>Used to compute the LIME method. </p>"},{"location":"api/attributions/lime/#__init__","title":"<code>__init__(self,  model:  Callable,  batch_size:  Optional[int] = None,  operator:  Optional[Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float]] = None,  interpretable_model:  Any = Ridge(alpha=2),  similarity_kernel:  Optional[Callable[[tf.Tensor, tf.Tensor, tf.Tensor], tf.Tensor]] = None,  pertub_func:  Optional[Callable[[Union[int, tf.Tensor], int], tf.Tensor]] = None,  map_to_interpret_space:  Optional[Callable[[tf.Tensor], tf.Tensor]] = None,  ref_value:  Optional[numpy.ndarray] = None,  nb_samples:  int = 150,  distance_mode:  str = 'euclidean',  kernel_width:  float = 45.0,  prob:  float = 0.5)</code>","text":"<p>Parameters</p> <ul> <li> <p>model            : Callable </p> <ul> <li><p> The model from which we want to obtain explanations</p> </li> </ul> </li> <li> <p>batch_size            : Optional[int] = None </p> <ul> <li><p> Number of perturbed samples to process at once, mandatory when nb_samples is huge.</p><p> Notice, it is different compare to WhiteBox explainers which batch the inputs.</p><p> Here inputs are process one by one.</p> </li> </ul> </li> <li> <p>operator            : Optional[Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float]] = None </p> <ul> <li><p> Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y].</p> </li> </ul> </li> <li> <p>interpretable_model            : Any = Ridge(alpha=2) </p> <ul> <li><p> Model object to train interpretable model.</p><p> See the documentation for more information.</p> </li> </ul> </li> <li> <p>similarity_kernel            : Optional[Callable[[tf.Tensor, tf.Tensor, tf.Tensor], tf.Tensor]] = None </p> <ul> <li><p> Function which considering an input, perturbed instances of these input and the interpretable version of those perturbed samples compute the similarities between the input and the perturbed samples.</p><p> See the documentation for more information.</p> </li> </ul> </li> <li> <p>pertub_func            : Optional[Callable[[Union[int, tf.Tensor], int], tf.Tensor]] = None </p> <ul> <li><p> Function which generate perturbed interpretable samples in the interpretation space from the number of interpretable features (e.g nb of super pixel) and the number of perturbed samples you want per original input.</p><p> See the documentation for more information.</p> </li> </ul> </li> <li> <p>ref_value            : Optional[numpy.ndarray] = None </p> <ul> <li><p> It defines reference value which replaces each feature when the corresponding interpretable feature is set to 0.</p><p> It should be provided as: a ndarray of shape (1) if there is no channels in your input and (C,) otherwise  The default ref value is set to (0.5,0.5,0.5) for inputs with 3 channels (corresponding to a grey pixel when inputs are normalized by 255) and to 0 otherwise.</p> </li> </ul> </li> <li> <p>map_to_interpret_space            : Optional[Callable[[tf.Tensor], tf.Tensor]] = None </p> <ul> <li><p> Function which group features of an input corresponding to the same interpretable feature (e.g super-pixel).</p><p> It allows to transpose from (resp. to) the original input space to (resp. from) the interpretable space.</p><p> See the documentation for more information.</p> </li> </ul> </li> <li> <p>nb_samples            : int = 150 </p> <ul> <li><p> The number of perturbed samples you want to generate for each input sample.</p><p> Default to 150.</p> </li> </ul> </li> <li> <p>prob            : float = 0.5 </p> <ul> <li><p> The probability argument for the default pertub function.</p> </li> </ul> </li> <li> <p>distance_mode            : str = 'euclidean' </p> <ul> <li><p> The distance mode used in the default similarity kernel, you can choose either \"euclidean\" or \"cosine\" (will compute cosine similarity).</p><p> Default value set to \"euclidean\".</p> </li> </ul> </li> <li> <p>kernel_width            : float = 45.0 </p> <ul> <li><p> Width of your kernel. It is important to make it evolving depending on your inputs size otherwise you will get all similarity close to 0 leading to poor performance or NaN values.</p><p> Default to 45 (i.e adapted for RGB images).</p> </li> </ul> </li> </ul>"},{"location":"api/attributions/lime/#explain","title":"<code>explain(self,  inputs:  Union[tf.Dataset, tf.Tensor, numpy.ndarray],  targets:  Union[tf.Tensor, numpy.ndarray, None] = None) -&gt; tf.Tensor</code>","text":"<p>This method attributes the output of the model with given targets to the inputs of the model using the approach described above, training an interpretable model and returning a representation of the interpretable model. </p> <p>Parameters</p> <ul> <li> <p>inputs            : Union[tf.Dataset, tf.Tensor, numpy.ndarray] </p> <ul> <li><p> Dataset, Tensor or Array. Input samples to be explained.</p><p> If Dataset, targets should not be provided (included in Dataset).</p><p> Expected shape among (N, W), (N, T, W), (N, W, H, C).</p><p> More information in the documentation.</p> </li> </ul> </li> <li> <p>targets            : Union[tf.Tensor, numpy.ndarray, None] = None </p> <ul> <li><p> Tensor or Array. One-hot encoding of the model's output from which an explanation is desired. One encoding per input and only one output at a time. Therefore, the expected shape is (N, output_size).</p><p> More information in the documentation.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>explanations            : tf.Tensor </p> <ul> <li><p> Interpretable coefficients, same shape as the inputs, except for the channels.</p><p> Coefficients of the interpretable model. Those coefficients having the size of the interpretable space will be given the same value to coefficient which were grouped together (e.g belonging to the same super-pixel).</p> </li> </ul> </li> </ul> <p></p> <p>Warning</p> <p>The computation time might be very long depending on the hyperparameters settings. A huge number of perturbed samples and a fine-grained mapping may lead to better results but it is long to compute.</p>"},{"location":"api/attributions/lime/#parameters-in-depth","title":"Parameters in-depth","text":""},{"location":"api/attributions/lime/#interpretable_model","title":"<code>interpretable_model</code>:","text":"<p>A Model object providing a <code>fit</code> method that train the model with the following inputs:</p> <ul> <li>- <code>interpretable_inputs</code>: 2D <code>ndarray</code> of shape (\\(nb\\_samples\\) x \\(num\\_interp\\_features\\)),</li> <li>- <code>expected_outputs</code>: 1D <code>ndarray</code> of shape (\\(nb\\_samples\\)),</li> <li>- <code>weights</code>: 1D <code>ndarray</code> of shape (\\(nb\\_samples\\))</li> </ul> <p>The model object should also provide a <code>predict</code> and <code>fit</code> method.</p> <p>It should also have a <code>coef_</code> attributes (the interpretable explanations) at least once <code>fit</code> is called.</p> <p>As interpretable model you can use linear models from scikit-learn.</p> <p>Warning</p> <p>Note that here <code>nb_samples</code> doesn't indicates the length of inputs but the number of perturbed samples we want to generate for each input.</p>"},{"location":"api/attributions/lime/#similarity_kernel","title":"<code>similarity_kernel</code>:","text":"<p>Function which considering an input, perturbed instances of these input and the interpretable version of those perturbed samples compute the similarities between the input and the perturbed samples.</p> <p>Info</p> <p>The similarities can be computed in the original input space or in the interpretable space.</p> <p>You can provide a custom function. Note that to use a custom function, you have to follow the following scheme:</p> <pre><code>def custom_similarity(\n    original_input, interpret_samples , perturbed_samples\n) -&gt; tf.tensor (shape=(nb_samples,), dtype = tf.float32):\n    ** some tf actions **\n    return similarities\n</code></pre> <p>where:</p> <ul> <li>- <code>original_input</code> has shape among \\((W)\\), \\((W, H)\\), \\((W, H, C)\\)</li> <li>- <code>interpret_samples</code> is a <code>tf.tensor</code> of shape \\((nb\\_samples, num\\_interp\\_features)\\)</li> <li>- <code>perturbed_samples</code> is a <code>tf.tensor</code> of shape \\((nb\\_samples, *original\\_input.shape)\\)</li> </ul> <p>If it is possible you can add the <code>@tf.function</code> decorator.</p> <p>Warning</p> <p>Note that here <code>nb_samples</code> doesn't indicates the length of inputs but the number of perturbed samples we want to generate for each input.</p> <p>Info</p> <p>The default similarity kernel use the euclidean distance between the original input and the perturbed samples in the input space.</p>"},{"location":"api/attributions/lime/#pertub_func","title":"<code>pertub_func</code>:","text":"<p>Function which generate perturbed interpretable samples in the interpretation space from the number of interpretable features (e.g nb of super pixel) and the number of perturbed samples you want per original input.</p> <p>The generated <code>interp_samples</code> belong to \\(\\{0,1\\}^{num\\_features}\\). Where \\(1\\) indicates that we keep the corresponding feature (e.g super pixel) in the mapping.</p> <p>To use your own custom pertub function you should use the following scheme:</p> <pre><code>@tf.function\ndef custom_pertub_function(num_features, nb_samples) -&gt;\ntf.tensor (shape=(nb_samples, num_interp_features), dtype=tf.int32):\n    ** some tf actions**\n    return perturbed_sample\n</code></pre> <p>Info</p> <p>The default pertub function provided keep a feature (e.g super pixel) with a probability 0.5. If you want to change it, define the <code>prob</code> value when initiating the explainer or define your own function.</p>"},{"location":"api/attributions/lime/#map_to_interpret_space","title":"<code>map_to_interpret_space</code>:","text":"<p>Function which group features of an input corresponding to the same interpretable feature (e.g super-pixel).</p> <p>It allows to transpose from (resp. to) the original input space to (resp. from) the interpretable space.</p> <p>The default mappings are:</p> <ul> <li>- the quickshift segmentation algorithm for inputs with \\((N, W, H, C)\\) shape, we assume here such shape is used to represent \\((W, H, C)\\) images.</li> <li>- the felzenszwalb segmentation algorithm for inputs with \\((N, W, H)\\) shape, we assume here such shape is used to represent \\((W, H)\\) images.</li> <li>- an identity mapping if inputs has shape \\((N, W)\\), we assume here your inputs are tabular data.</li> </ul> <p>To use your own custom map function you should use the following scheme:</p> <pre><code>def custom_map_to_interpret_space(single_inp: tf.tensor) -&gt;\ntf.tensor:\n    **some grouping techniques**\n    return mapping\n</code></pre> <p><code>mapping</code> should have the same dimension as single input except for channels.</p> <p>For instance you can use the scikit-image (as we did for the quickshift algorithm) library to defines super pixels on your images.</p> <p>Info</p> <p>The quality of your explanation relies strongly on this mapping.</p> <p>Warning</p> <p>Depending on the mapping you might have a huge number of <code>interpretable_features</code>  (e.g you map pixels 2 by 2 on a 299x299 image). Thus, the compuation time might be very long!</p> <p>Danger</p> <p>As you may have noticed, by default Time Series are not handled. Consequently, a custom mapping should be implented. Either to assign each feature to a different group or to group consecutive features together, by group of 4 timesteps for example. In the second example, we try to cover patterns. An example is provided below.</p> <pre><code>def map_time_series(single_input: tf.tensor) -&gt; tf.Tensor:\n    time_dim = single_input.shape[0]\n    feat_dim = single_input.shape[1]\n    mapping = tf.range(time_dim*feat_dim)\n    mapping = tf.reshape(mapping, (time_dim, feat_dim))\n    return mapping\n</code></pre> <ol> <li> <p>\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier. \u21a9</p> </li> </ol>"},{"location":"api/attributions/occlusion/","title":"Occlusion sensitivity","text":"<p> View colab tutorial |  View source</p> <p>The Occlusion sensitivity method sweep a patch that occludes pixels over the images, and use the variations of the model prediction to deduce critical areas.1</p> <p>Quote</p> <p>[...] this method, referred to as Occlusion, replacing one feature \\(x_i\\) at the time with a  baseline and measuring the effect of this perturbation on the target output.</p> <p>-- Towards better understanding of the gradient-based attribution methods for Deep Neural Networks (2017)2</p> <p>with \\(S_c\\) the unormalized class score (layer before softmax) and \\(\\bar{x}\\) a baseline, the Occlusion sensitivity map \\(\\phi\\) is defined as :</p> \\[ \\phi_i = S_c(x) - S_c(x_{[x_i = \\bar{x}]}) \\]"},{"location":"api/attributions/occlusion/#example","title":"Example","text":"<pre><code>from xplique.attributions import Occlusion\n\n# load images, labels and model\n# ...\n\nmethod = Occlusion(model, patch_size=(10, 10),\n                   patch_stride=(2, 2), occlusion_value=0.5)\nexplanations = method.explain(images, labels)\n</code></pre>"},{"location":"api/attributions/occlusion/#notebooks","title":"Notebooks","text":"<ul> <li>Attribution Methods: Getting started</li> <li>Occlusion: Going Further</li> </ul>"},{"location":"api/attributions/occlusion/#Occlusion","title":"<code>Occlusion</code>","text":"<p>Used to compute the Occlusion sensitivity method, sweep a patch that occludes pixels over the images and use the variations of the model prediction to deduce critical areas. </p>"},{"location":"api/attributions/occlusion/#__init__","title":"<code>__init__(self,  model:  Callable,  batch_size:  Optional[int] = 32,  operator:  Optional[Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float]] = None,  patch_size:  Union[int, Tuple[int, int]] = 3,  patch_stride:  Union[int, Tuple[int, int]] = 3,  occlusion_value:  float = 0.0)</code>","text":"<p>Parameters</p> <ul> <li> <p>model            : Callable </p> <ul> <li><p> The model from which we want to obtain explanations</p> </li> </ul> </li> <li> <p>batch_size            : Optional[int] = 32 </p> <ul> <li><p> Number of pertubed samples to explain at once.</p><p> Default to 32.</p> </li> </ul> </li> <li> <p>operator            : Optional[Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float]] = None </p> <ul> <li><p> Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y].</p> </li> </ul> </li> <li> <p>patch_size            : Union[int, Tuple[int, int]] = 3 </p> <ul> <li><p> Size of the patches to apply, if integer then assume an hypercube.</p> </li> </ul> </li> <li> <p>patch_stride            : Union[int, Tuple[int, int]] = 3 </p> <ul> <li><p> Stride between two patches, if integer then assume an hypercube.</p> </li> </ul> </li> <li> <p>occlusion_value            : float = 0.0 </p> <ul> <li><p> Value used as occlusion.</p> </li> </ul> </li> </ul>"},{"location":"api/attributions/occlusion/#explain","title":"<code>explain(self,  inputs:  Union[tf.Dataset, tf.Tensor, numpy.ndarray],  targets:  Union[tf.Tensor, numpy.ndarray, None] = None) -&gt; tf.Tensor</code>","text":"<p>Compute Occlusion sensitivity for a batch of samples. </p> <p>Parameters</p> <ul> <li> <p>inputs            : Union[tf.Dataset, tf.Tensor, numpy.ndarray] </p> <ul> <li><p> Dataset, Tensor or Array. Input samples to be explained.</p><p> If Dataset, targets should not be provided (included in Dataset).</p><p> Expected shape among (N, W), (N, T, W), (N, W, H, C).</p><p> More information in the documentation.</p> </li> </ul> </li> <li> <p>targets            : Union[tf.Tensor, numpy.ndarray, None] = None </p> <ul> <li><p> Tensor or Array. One-hot encoding of the model's output from which an explanation is desired. One encoding per input and only one output at a time. Therefore, the expected shape is (N, output_size).</p><p> More information in the documentation.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>explanations            : tf.Tensor </p> <ul> <li><p> Occlusion sensitivity, same shape as the inputs, except for the channels.</p> </li> </ul> </li> </ul> <p></p> <p>Info</p> <p><code>patch_size</code> and <code>patch_stride</code> will define patch to apply to the original input. Thus, a combination of patches will generate pertubed samples of the original input (masked by patches with <code>occlusion_value</code> value). Consequently, the number of pertubed instances of an input depend on those parameters. Too little value of those two arguments on large image might lead to an incredible amount of pertubed samples and increase compuation time. On another hand too huge values might not be accurate enough.</p> <ol> <li> <p>Visualizing and Understanding Convolutional Networks (2014). \u21a9</p> </li> <li> <p>Towards better understanding of gradient-based attribution methods for Deep Neural Networks \u21a9</p> </li> </ol>"},{"location":"api/attributions/rise/","title":"RISE","text":"<p> View colab tutorial |   View source | \ud83d\udcf0 Paper</p> <p>The RISE method consist of probing the model with randomly masked versions of the input image and obtaining the corresponding outputs to deduce critical areas.</p> <p>Quote</p> <p>[...] we estimate the importance of pixels by dimming them in random combinations, reducing their intensities down to zero. We model this by multiplying an image with a [0,1] valued mask.</p> <p>-- RISE: Randomized Input Sampling for Explanation of Black-box Models (2018)1</p> <p>with \\(f(x)\\) the prediction of a classifier, for an input \\(x\\) and \\(m  \\sim \\mathcal{M}\\) a mask with value in \\([0,1]\\) created from a low dimension (\\(m\\) is in \\({0, 1}^{w \\times h}\\) with \\(w \\ll W\\) and \\(h \\ll H\\) then upsampled, see the paper for more details).</p> <p>The RISE importance estimator is defined as:</p> \\[ \\phi_i = \\mathbb{E}( f(x \\odot m) | m_i = 1)  \\approx \\frac{1}{\\mathbb{E}(\\mathcal{M}) N} \\sum_{i=1}^N f(x \\odot m_i) m_i \\] <p>The most important parameters here are (1) the <code>grid_size</code> that control \\(w\\) and \\(h\\) and (2) <code>nb_samples</code> that control \\(N\\). The pourcentage of visible pixels \\(\\mathbb{E}(\\mathcal{M})\\) is controlled using the <code>preservation_probability</code> parameter.</p>"},{"location":"api/attributions/rise/#example","title":"Example","text":"<pre><code>from xplique.attributions import Rise\n\n# load images, labels and model\n# ...\n\nmethod = Rise(model, nb_samples=4000, grid_size=7, preservation_probability=0.5)\nexplanations = method.explain(images, labels)\n</code></pre>"},{"location":"api/attributions/rise/#notebooks","title":"Notebooks","text":"<ul> <li>Attribution Methods: Getting started</li> <li>RISE: Going Further</li> </ul>"},{"location":"api/attributions/rise/#Rise","title":"<code>Rise</code>","text":"<p>Used to compute the RISE method, by probing the model with randomly masked versions of the input image and obtaining the corresponding outputs to deduce critical areas. </p>"},{"location":"api/attributions/rise/#__init__","title":"<code>__init__(self,  model:  Callable,  batch_size:  Optional[int] = 32,  operator:  Optional[Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float]] = None,  nb_samples:  int = 4000,  grid_size:  Union[int, Tuple[int]] = 7,  preservation_probability:  float = 0.5)</code>","text":"<p>Parameters</p> <ul> <li> <p>model            : Callable </p> <ul> <li><p> The model from which we want to obtain explanations</p> </li> </ul> </li> <li> <p>batch_size            : Optional[int] = 32 </p> <ul> <li><p> Number of pertubed samples to explain at once.</p><p> Default to 32.</p> </li> </ul> </li> <li> <p>operator            : Optional[Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float]] = None </p> <ul> <li><p> Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y].</p> </li> </ul> </li> <li> <p>nb_samples            : int = 4000 </p> <ul> <li><p> Number of masks generated for Monte Carlo sampling.</p> </li> </ul> </li> <li> <p>grid_size            : Union[int, Tuple[int]] = 7 </p> <ul> <li><p> Size of the grid used to generate the scaled-down masks. Masks are then rescale to and cropped to input_size. Can be a tuple for different cutting depending on the dimension.</p> </li> </ul> </li> <li> <p>preservation_probability            : float = 0.5 </p> <ul> <li><p> Probability of preservation for each pixel (or the percentage of non-masked pixels in each masks), also the expectation value of the mask.</p> </li> </ul> </li> </ul>"},{"location":"api/attributions/rise/#explain","title":"<code>explain(self,  inputs:  Union[tf.Dataset, tf.Tensor, numpy.ndarray],  targets:  Union[tf.Tensor, numpy.ndarray, None] = None) -&gt; tf.Tensor</code>","text":"<p>Compute RISE for a batch of samples. </p> <p>Parameters</p> <ul> <li> <p>inputs            : Union[tf.Dataset, tf.Tensor, numpy.ndarray] </p> <ul> <li><p> Dataset, Tensor or Array. Input samples to be explained.</p><p> If Dataset, targets should not be provided (included in Dataset).</p><p> Expected shape among (N, W), (N, T, W), (N, W, H, C).</p><p> More information in the documentation.</p> </li> </ul> </li> <li> <p>targets            : Union[tf.Tensor, numpy.ndarray, None] = None </p> <ul> <li><p> Tensor or Array. One-hot encoding of the model's output from which an explanation is desired. One encoding per input and only one output at a time. Therefore, the expected shape is (N, output_size).</p><p> More information in the documentation.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>explanations            : tf.Tensor </p> <ul> <li><p> RISE maps, same shape as the inputs, except for the channels.</p> </li> </ul> </li> </ul> <p></p> <ol> <li> <p>RISE: Randomized Input Sampling for Explanation of Black-box Models (2018) \u21a9</p> </li> </ol>"},{"location":"api/attributions/saliency/","title":"Saliency Maps","text":"<p> View colab tutorial |  View source | \ud83d\udcf0 Paper</p> <p>Saliency is one of the most easy explanation method based on the gradient of a class score relative to the input.</p> <p>Quote</p> <p>An interpretation of computing the image-specific class saliency using the class score derivative is that the magnitude of the derivative indicates which pixels need to be changed the least to affect the class score the most. One can expect that such pixels correspond to the object location in the image.</p> <p>-- Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps (2013)1</p> <p>More precisely, for an image \\(x\\) the importance map \\(\\phi\\) according to a classifier \\(f\\) is defined as:</p> \\[ \\phi = | \\nabla_{x} f(x) | \\] <p>more precisely, in the image case, Xplique is faithful to the original method and returns the max on the axis of channels, with \\(\\phi_i \\in \\mathbb{R}^3\\) for RGB, the importance for the pixel \\(i\\) is given by \\(||\\phi_i||_{\\infty}\\)</p>"},{"location":"api/attributions/saliency/#example","title":"Example","text":"<pre><code>from xplique.attributions import Saliency\n\n# load images, labels and model\n# ...\n\nmethod = Saliency(model)\nexplanations = method.explain(images, labels)\n</code></pre>"},{"location":"api/attributions/saliency/#notebooks","title":"Notebooks","text":"<ul> <li>Attribution Methods: Getting started</li> <li>Saliency: Going Further</li> </ul>"},{"location":"api/attributions/saliency/#Saliency","title":"<code>Saliency</code>","text":"<p>Used to compute the absolute gradient of the output relative to the input. </p>"},{"location":"api/attributions/saliency/#__init__","title":"<code>__init__(self,  model:  keras.engine.training.Model,  output_layer:  Union[str, int, None] = None,  batch_size:  Optional[int] = 64,  operator:  Optional[Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float]] = None)</code>","text":"<p>Parameters</p> <ul> <li> <p>model            : keras.engine.training.Model </p> <ul> <li><p> The model from which we want to obtain explanations</p> </li> </ul> </li> <li> <p>output_layer            : Union[str, int, None] = None </p> <ul> <li><p> Layer to target for the outputs (e.g logits or after softmax).</p><p> If an <code>int</code> is provided it will be interpreted as a layer index.</p><p> If a <code>string</code> is provided it will look for the layer name.</p><p>  Default to the last layer.</p><p> It is recommended to use the layer before Softmax.</p> </li> </ul> </li> <li> <p>batch_size            : Optional[int] = 64 </p> <ul> <li><p> Number of inputs to explain at once, if None compute all at once.</p> </li> </ul> </li> <li> <p>operator            : Optional[Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float]] = None </p> <ul> <li><p> Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y].</p> </li> </ul> </li> </ul>"},{"location":"api/attributions/saliency/#explain","title":"<code>explain(self,  inputs:  Union[tf.Dataset, tf.Tensor, numpy.ndarray],  targets:  Union[tf.Tensor, numpy.ndarray, None] = None) -&gt; tf.Tensor</code>","text":"<p>Compute saliency maps for a batch of samples. </p> <p>Parameters</p> <ul> <li> <p>inputs            : Union[tf.Dataset, tf.Tensor, numpy.ndarray] </p> <ul> <li><p> Dataset, Tensor or Array. Input samples to be explained.</p><p> If Dataset, targets should not be provided (included in Dataset).</p><p> Expected shape among (N, W), (N, T, W), (N, W, H, C).</p><p> More information in the documentation.</p> </li> </ul> </li> <li> <p>targets            : Union[tf.Tensor, numpy.ndarray, None] = None </p> <ul> <li><p> Tensor or Array. One-hot encoding of the model's output from which an explanation is desired. One encoding per input and only one output at a time. Therefore, the expected shape is (N, output_size).</p><p> More information in the documentation.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>explanations            : tf.Tensor </p> <ul> <li><p> Saliency maps.</p> </li> </ul> </li> </ul> <p></p> <ol> <li> <p>Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps \u21a9</p> </li> </ol>"},{"location":"api/attributions/smoothgrad/","title":"SmoothGrad","text":"<p> View colab tutorial |  View source | \ud83d\udcf0 Paper</p> <p>SmoothGrad is a gradient-based explanation method, which, as the name suggests, averages the gradient at several points corresponding to small perturbations around the point of interest. The smoothing effect induced by the average help reducing the visual noise, and hence improve the explanations.</p> <p>Quote</p> <p>[...] The gradient at any given point will be less meaningful than a local average of gradient values. This suggests a new way to create improved sensitivity maps: instead of basing a visualization directly on the gradient, we could base it on a smoothing of the gradients with a Gaussian kernel.</p> <p>-- SmoothGrad: removing noise by adding noise (2017)1</p> <p>More precisely, the explanation \\(\\phi\\) for an input \\(x\\) and a classifier \\(f\\) is defined as</p> \\[ \\phi = \\mathbb{E}_{\\delta ~\\sim~ \\mathcal{N}(0, \\sigma^2) }( \\nabla_x f(x + \\delta) )  \\approx \\frac{1}{N} \\sum_{i=0}^N \\nabla_x f(x + \\delta_i) \\] <p>The \\(\\sigma\\) in the formula is controlled using the <code>noise</code> parameter, and the expectation is estimated using \\(N\\) samples controlled by the <code>nb_samples</code> parameter.</p> <p>Tip</p> <p>It is recommended to have a noise level \\(\\sigma\\) at about 20% of the range of your inputs, i.e. \\(\\sigma=0.2\\) if your inputs are between \\([0, 1]\\) or \\(\\sigma=0.4\\) if your inputs are between \\([-1, 1]\\).</p>"},{"location":"api/attributions/smoothgrad/#example","title":"Example","text":"<pre><code>from xplique.attributions import SmoothGrad\n\n# load images, labels and model\n# ...\n\nmethod = SmoothGrad(model, nb_samples=50, noise=0.15)\nexplanations = method.explain(images, labels)\n</code></pre>"},{"location":"api/attributions/smoothgrad/#notebooks","title":"Notebooks","text":"<ul> <li>Attribution Methods: Getting started</li> <li>SmoothGrad: Going Further</li> </ul>"},{"location":"api/attributions/smoothgrad/#SmoothGrad","title":"<code>SmoothGrad</code>","text":"<p>Used to compute the SmoothGrad, by averaging Saliency maps of noisy samples centered on the original sample. </p>"},{"location":"api/attributions/smoothgrad/#__init__","title":"<code>__init__(self,  model:  keras.engine.training.Model,  output_layer:  Union[str, int, None] = None,  batch_size:  Optional[int] = 32,  operator:  Optional[Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float]] = None,  nb_samples:  int = 50,  noise:  float = 0.2)</code>","text":"<p>Parameters</p> <ul> <li> <p>model            : keras.engine.training.Model </p> <ul> <li><p> The model from which we want to obtain explanations</p> </li> </ul> </li> <li> <p>output_layer            : Union[str, int, None] = None </p> <ul> <li><p> Layer to target for the outputs (e.g logits or after softmax).</p><p> If an <code>int</code> is provided it will be interpreted as a layer index.</p><p> If a <code>string</code> is provided it will look for the layer name.</p><p>  Default to the last layer.</p><p> It is recommended to use the layer before Softmax.</p> </li> </ul> </li> <li> <p>batch_size            : Optional[int] = 32 </p> <ul> <li><p> Number of inputs to explain at once, if None compute all at once.</p> </li> </ul> </li> <li> <p>operator            : Optional[Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float]] = None </p> <ul> <li><p> Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y].</p> </li> </ul> </li> <li> <p>nb_samples            : int = 50 </p> <ul> <li><p> Number of noisy samples generated for the smoothing procedure.</p> </li> </ul> </li> <li> <p>noise            : float = 0.2 </p> <ul> <li><p> Scalar, noise used as standard deviation of a normal law centered on zero.</p> </li> </ul> </li> </ul>"},{"location":"api/attributions/smoothgrad/#explain","title":"<code>explain(self,  inputs:  Union[tf.Dataset, tf.Tensor, numpy.ndarray],  targets:  Union[tf.Tensor, numpy.ndarray, None] = None) -&gt; tf.Tensor</code>","text":"<p>Compute SmoothGrad for a batch of samples. </p> <p>Parameters</p> <ul> <li> <p>inputs            : Union[tf.Dataset, tf.Tensor, numpy.ndarray] </p> <ul> <li><p> Dataset, Tensor or Array. Input samples to be explained.</p><p> If Dataset, targets should not be provided (included in Dataset).</p><p> Expected shape among (N, W), (N, T, W), (N, W, H, C).</p><p> More information in the documentation.</p> </li> </ul> </li> <li> <p>targets            : Union[tf.Tensor, numpy.ndarray, None] = None </p> <ul> <li><p> Tensor or Array. One-hot encoding of the model's output from which an explanation is desired. One encoding per input and only one output at a time. Therefore, the expected shape is (N, output_size).</p><p> More information in the documentation.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>explanations            : tf.Tensor </p> <ul> <li><p> Smoothed gradients, same shape as the inputs.</p> </li> </ul> </li> </ul> <p></p> <ol> <li> <p>SmoothGrad: removing noise by adding noise (2017) \u21a9</p> </li> </ol>"},{"location":"api/attributions/sobol/","title":"Sobol Attribution Method","text":"<p> View colab tutorial |   View source | \ud83d\udcf0 Paper</p> <p>The Sobol attribution method from Fel, Cad\u00e8ne &amp; al.1 is an attribution method grounded in Sensitivity Analysis. Beyond modeling the individual contributions of image regions, Sobol indices provide efficient way to capture higher-order interactions between image regions and their contributions to a neural network\u2019s prediction through the lens of variance.</p> <p>Quote</p> <p>The total Sobol index \\(ST_i\\) which measures the contribution of the variable \\(X_i\\) as well as its interactions of any order with any other input variables to the model output variance.</p> <p>-- Look at the Variance! Efficient Black-box Explanations with Sobol-based Sensitivity Analysis (2021)1</p> <p>More precisely, the attribution score \\(\\phi_i\\) for an input variable \\(x_i\\), is defined as</p> \\[ \\phi_i = \\frac{\\mathbb{E}_{X \\sim i}(Var_{X_i}(f(x) | X_{\\sim i}))} {Var (f(X ))} \\] <p>Where \\(\\mathbb{E}_{X \\sim i}(Var_{X_i}(f(x) | X_{\\sim i}))\\) is the expected variance that would be left if all variables but \\(X_{\\sim i}\\) were to be fixed.</p> <p>In order to generate stochasticity(\\(X_i\\)), a perturbation function is used and uses perturbation masks to modulate the generated perturbation. The perturbation functions available are inpainting that modulates pixel regions to a baseline state, amplitude and blurring.</p> <p>The calculation of the indices also requires an estimator -- in practice this parameter does not change the results much -- <code>JansenEstimator</code> being recommended. </p> <p>Finally the exploration of the manifold exploration is made using a sampling method, several samplers are proposed: Quasi-Monte Carlo (<code>ScipySobolSequence</code>, recommended) using Scipy's sobol sequence, Latin hypercubes  -- <code>LHSAmpler</code> -- or Halton's sequences <code>HaltonSequence</code>.</p> <p>Tip</p> <p>For a quick a faithful explanations, we recommend to use <code>grid_size</code> in \\([7, 12)\\), <code>nb_design</code> in \\(\\{16, 32, 64\\}\\) (more is useless), and a QMC sampler.</p>"},{"location":"api/attributions/sobol/#example","title":"Example","text":"<pre><code>from xplique.attributions import SobolAttributionMethod\nfrom xplique.attributions.global_sensitivity_analysis import (\n    JansenEstimator, GlenEstimator,\n    LHSampler, ScipySobolSequence,\n    HaltonSequence)\n\n# load images, labels and model\n# ...\n\n# default explainer (recommended)\nexplainer = SobolAttributionMethod(model, grid_size=8, nb_design=32)\nexplanations = method(images, labels) # one-hot encoded labels\n</code></pre> <p>If you want to change the estimator or the sampling:</p> <pre><code>from xplique.attributions import SobolAttributionMethod\nfrom xplique.attributions.global_sensitivity_analysis import (\n    JansenEstimator, GlenEstimator,\n    LHSampler, ScipySobolSequence,\n    HaltonSequence)\n\n# load images, labels and model\n# ...\n\nexplainer_lhs = SobolAttributionMethod(model, grid_size=8, nb_design=32, \n                                       sampler=LHSampler(), \n                                       estimator=GlenEstimator())\nexplanations_lhs = explainer_lhs(images, labels)\n</code></pre>"},{"location":"api/attributions/sobol/#notebooks","title":"Notebooks","text":"<ul> <li>Attribution Methods: Getting started </li> </ul>"},{"location":"api/attributions/sobol/#SobolAttributionMethod","title":"<code>SobolAttributionMethod</code>","text":"<p>Sobol' Attribution Method. Compute the total order Sobol' indices using a perturbation function on a grid and an adapted sampling as described in the original paper. </p>"},{"location":"api/attributions/sobol/#__init__","title":"<code>__init__(self,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 model,  grid_size:  int = 8,  nb_design:  int = 32,  sampler:  Optional[xplique.attributions.global_sensitivity_analysis.replicated_designs.ReplicatedSampler] = None,  estimator:  Optional[xplique.attributions.global_sensitivity_analysis.sobol_estimators.SobolEstimator] = None,  perturbation_function:  Union[Callable, str, None] = 'inpainting',\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 batch_size=256,  operator:  Optional[Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float]] = None)</code>","text":"<p>Parameters</p> <ul> <li> <p>model            : model </p> <ul> <li><p> Model used for computing explanations.</p> </li> </ul> </li> <li> <p>grid_size            : int = 8 </p> <ul> <li><p> Cut the image in a grid of (grid_size, grid_size) to estimate an indice per cell.</p> </li> </ul> </li> <li> <p>nb_design            : int = 32 </p> <ul> <li><p> Must be a power of two. Number of design, the number of forward will be: nb_design * (grid_size**2 + 2). Generally not above 32.</p> </li> </ul> </li> <li> <p>sampler            : Optional[xplique.attributions.global_sensitivity_analysis.replicated_designs.ReplicatedSampler] = None </p> <ul> <li><p> Sampler used to generate the (quasi-)monte carlo samples, QMC (sobol sequence recommended). For more option, see the sampler module.</p> </li> </ul> </li> <li> <p>estimator            : Optional[xplique.attributions.global_sensitivity_analysis.sobol_estimators.SobolEstimator] = None </p> <ul> <li><p> Estimator used to compute the total order sobol' indices, Jansen recommended. For more option, see the estimator module.</p> </li> </ul> </li> <li> <p>perturbation_function            : Union[Callable, str, None] = 'inpainting' </p> <ul> <li><p> Function to call to apply the perturbation on the input. Can also be string in 'inpainting', 'blur'.</p> </li> </ul> </li> <li> <p>batch_size            : batch_size=256 </p> <ul> <li><p> Batch size to use for the forwards.</p> </li> </ul> </li> <li> <p>operator            : Optional[Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float]] = None </p> <ul> <li><p> Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y].</p> </li> </ul> </li> </ul>"},{"location":"api/attributions/sobol/#explain","title":"<code>explain(self,  inputs:  Union[tf.Dataset, tf.Tensor, numpy.ndarray],  targets:  Union[tf.Tensor, numpy.ndarray, None] = None) -&gt; tf.Tensor</code>","text":"<p>Compute the total Sobol' indices according to the explainer parameter (perturbation function, grid size...). Accept Tensor, numpy array or tf.data.Dataset (in that case targets is None). </p> <p>Parameters</p> <ul> <li> <p>inputs            : Union[tf.Dataset, tf.Tensor, numpy.ndarray] </p> <ul> <li><p> Images to be explained, either tf.dataset, Tensor or numpy array.</p><p> If Dataset, targets should not be provided (included in Dataset).</p><p> Expected shape (N, W, H, C) or (N, W, H).</p> </li> </ul> </li> <li> <p>targets            : Union[tf.Tensor, numpy.ndarray, None] = None </p> <ul> <li><p> One-hot encoding for classification or direction {-1, +1} for regression.</p><p> Tensor or numpy array.</p><p> Expected shape (N, C) or (N).</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>attributions_maps            : tf.Tensor </p> <ul> <li><p> GSA Attribution Method explanations, same shape as the inputs except for the channels.</p> </li> </ul> </li> </ul> <p></p> <ol> <li> <p>Look at the Variance! Efficient Black-box Explanations with Sobol-based Sensitivity Analysis (2021) \u21a9\u21a9</p> </li> </ol>"},{"location":"api/attributions/square_grad/","title":"Square Grad","text":"<p> View colab tutorial |  View source | \ud83d\udcf0 Paper</p> <p>Similar to SmoothGrad, Square-Grad is a gradient-based explanation method, which, as the name suggests, averages the square of the gradient at several points corresponding to small perturbations around the point of interest. The smoothing effect induced by the average help reducing the visual noise, and hence improve the explanations.</p> <p>More precisely, the explanation \\(\\phi\\) for an input \\(x\\) and a classifier \\(f\\) is defined as</p> \\[ \\phi = \\mathbb{E}_{\\delta ~\\sim~ \\mathcal{N}(0, \\sigma^2) }( (\\nabla_x f(x + \\delta))^2 )  \\approx \\frac{1}{N} \\sum_{i=0}^N (\\nabla_x f(x + \\delta_i))^2 \\] <p>The \\(\\sigma\\) in the formula is controlled using the <code>noise</code> parameter, and the expectation is estimated using \\(N\\) samples controlled by the <code>nb_samples</code> parameter.</p> <p>Tip</p> <p>It is recommended to have a noise level \\(\\sigma\\) at about 20% of the range of your inputs, i.e. \\(\\sigma=0.2\\) if your inputs are between \\([0, 1]\\) or \\(\\sigma=0.4\\) if your inputs are between \\([-1, 1]\\).</p>"},{"location":"api/attributions/square_grad/#example","title":"Example","text":"<pre><code>from xplique.attributions import SquareGrad\n\n# load images, labels and model\n# ...\n\nmethod = SquareGrad(model, nb_samples=50, noise=0.15)\nexplanations = method.explain(images, labels)\n</code></pre>"},{"location":"api/attributions/square_grad/#notebooks","title":"Notebooks","text":"<ul> <li>Attribution Methods: Getting started</li> <li>SquareGrad: Going Further</li> </ul>"},{"location":"api/attributions/square_grad/#SquareGrad","title":"<code>SquareGrad</code>","text":"<p>SquareGrad (or SmoothGrad^2) is an unpublished variant of classic SmoothGrad which squares each gradients of the noisy inputs before averaging. </p>"},{"location":"api/attributions/square_grad/#__init__","title":"<code>__init__(self,  model:  keras.engine.training.Model,  output_layer:  Union[str, int, None] = None,  batch_size:  Optional[int] = 32,  operator:  Optional[Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float]] = None,  nb_samples:  int = 50,  noise:  float = 0.2)</code>","text":"<p>Parameters</p> <ul> <li> <p>model            : keras.engine.training.Model </p> <ul> <li><p> The model from which we want to obtain explanations</p> </li> </ul> </li> <li> <p>output_layer            : Union[str, int, None] = None </p> <ul> <li><p> Layer to target for the outputs (e.g logits or after softmax).</p><p> If an <code>int</code> is provided it will be interpreted as a layer index.</p><p> If a <code>string</code> is provided it will look for the layer name.</p><p>  Default to the last layer.</p><p> It is recommended to use the layer before Softmax.</p> </li> </ul> </li> <li> <p>batch_size            : Optional[int] = 32 </p> <ul> <li><p> Number of inputs to explain at once, if None compute all at once.</p> </li> </ul> </li> <li> <p>operator            : Optional[Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float]] = None </p> <ul> <li><p> Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y].</p> </li> </ul> </li> <li> <p>nb_samples            : int = 50 </p> <ul> <li><p> Number of noisy samples generated for the smoothing procedure.</p> </li> </ul> </li> <li> <p>noise            : float = 0.2 </p> <ul> <li><p> Scalar, noise used as standard deviation of a normal law centered on zero.</p> </li> </ul> </li> </ul>"},{"location":"api/attributions/square_grad/#explain","title":"<code>explain(self,  inputs:  Union[tf.Dataset, tf.Tensor, numpy.ndarray],  targets:  Union[tf.Tensor, numpy.ndarray, None] = None) -&gt; tf.Tensor</code>","text":"<p>Compute SmoothGrad for a batch of samples. </p> <p>Parameters</p> <ul> <li> <p>inputs            : Union[tf.Dataset, tf.Tensor, numpy.ndarray] </p> <ul> <li><p> Dataset, Tensor or Array. Input samples to be explained.</p><p> If Dataset, targets should not be provided (included in Dataset).</p><p> Expected shape among (N, W), (N, T, W), (N, W, H, C).</p><p> More information in the documentation.</p> </li> </ul> </li> <li> <p>targets            : Union[tf.Tensor, numpy.ndarray, None] = None </p> <ul> <li><p> Tensor or Array. One-hot encoding of the model's output from which an explanation is desired. One encoding per input and only one output at a time. Therefore, the expected shape is (N, output_size).</p><p> More information in the documentation.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>explanations            : tf.Tensor </p> <ul> <li><p> Smoothed gradients, same shape as the inputs.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/attributions/vargrad/","title":"VarGrad","text":"<p> View colab tutorial |  View source | \ud83d\udcf0 Paper</p> <p>Similar to SmoothGrad, VarGrad is a gradient-based explanation method, which, as the name suggests, return the variance of the gradient at several points corresponding to small perturbations around the point of interest. The smoothing effect induced by the average help reducing the visual noise, and hence improve the explanations.</p> <p>More precisely, the explanation \\(\\phi\\) for an input \\(x\\) and a classifier \\(f\\) is defined as</p> \\[ \\phi = \\mathbb{V}_{\\delta ~\\sim~ \\mathcal{N}(0, \\sigma^2) }( \\nabla_x f(x + \\delta) ) \\approx \\frac{1}{N-1} \\sum_{i=0}^N (\\nabla_x f(x + \\delta_i) - \\hat{\\mu})^2 \\] <p>Where \\(\\hat{\\mu} = \\frac{1}{N} \\sum_{i=0}^N \\nabla_x f(x + \\delta_i)\\) is the empirical mean. The \\(\\sigma\\) in the formula is controlled using the <code>noise</code> parameter, and the expectation is estimated using \\(N\\) samples controlled by the <code>nb_samples</code> parameter.</p> <p>Tip</p> <p>It is recommended to have a noise level \\(\\sigma\\) at about 20% of the range of your inputs, i.e. \\(\\sigma=0.2\\) if your inputs are between \\([0, 1]\\) or \\(\\sigma=0.4\\) if your inputs are between \\([-1, 1]\\).</p>"},{"location":"api/attributions/vargrad/#example","title":"Example","text":"<pre><code>from xplique.attributions import VarGrad\n\n# load images, labels and model\n# ...\n\nmethod = VarGrad(model, nb_samples=50, noise=0.15)\nexplanations = method.explain(images, labels)\n</code></pre>"},{"location":"api/attributions/vargrad/#notebooks","title":"Notebooks","text":"<ul> <li>Attribution Methods: Getting started</li> <li>VarGrad: Going Further</li> </ul>"},{"location":"api/attributions/vargrad/#VarGrad","title":"<code>VarGrad</code>","text":"<p>VarGrad is a variance analog to SmoothGrad. </p>"},{"location":"api/attributions/vargrad/#__init__","title":"<code>__init__(self,  model:  keras.engine.training.Model,  output_layer:  Union[str, int, None] = None,  batch_size:  Optional[int] = 32,  operator:  Optional[Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float]] = None,  nb_samples:  int = 50,  noise:  float = 0.2)</code>","text":"<p>Parameters</p> <ul> <li> <p>model            : keras.engine.training.Model </p> <ul> <li><p> The model from which we want to obtain explanations</p> </li> </ul> </li> <li> <p>output_layer            : Union[str, int, None] = None </p> <ul> <li><p> Layer to target for the outputs (e.g logits or after softmax).</p><p> If an <code>int</code> is provided it will be interpreted as a layer index.</p><p> If a <code>string</code> is provided it will look for the layer name.</p><p>  Default to the last layer.</p><p> It is recommended to use the layer before Softmax.</p> </li> </ul> </li> <li> <p>batch_size            : Optional[int] = 32 </p> <ul> <li><p> Number of inputs to explain at once, if None compute all at once.</p> </li> </ul> </li> <li> <p>operator            : Optional[Callable[[keras.engine.training.Model, tf.Tensor, tf.Tensor], float]] = None </p> <ul> <li><p> Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y].</p> </li> </ul> </li> <li> <p>nb_samples            : int = 50 </p> <ul> <li><p> Number of noisy samples generated for the smoothing procedure.</p> </li> </ul> </li> <li> <p>noise            : float = 0.2 </p> <ul> <li><p> Scalar, noise used as standard deviation of a normal law centered on zero.</p> </li> </ul> </li> </ul>"},{"location":"api/attributions/vargrad/#explain","title":"<code>explain(self,  inputs:  Union[tf.Dataset, tf.Tensor, numpy.ndarray],  targets:  Union[tf.Tensor, numpy.ndarray, None] = None) -&gt; tf.Tensor</code>","text":"<p>Compute SmoothGrad for a batch of samples. </p> <p>Parameters</p> <ul> <li> <p>inputs            : Union[tf.Dataset, tf.Tensor, numpy.ndarray] </p> <ul> <li><p> Dataset, Tensor or Array. Input samples to be explained.</p><p> If Dataset, targets should not be provided (included in Dataset).</p><p> Expected shape among (N, W), (N, T, W), (N, W, H, C).</p><p> More information in the documentation.</p> </li> </ul> </li> <li> <p>targets            : Union[tf.Tensor, numpy.ndarray, None] = None </p> <ul> <li><p> Tensor or Array. One-hot encoding of the model's output from which an explanation is desired. One encoding per input and only one output at a time. Therefore, the expected shape is (N, output_size).</p><p> More information in the documentation.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>explanations            : tf.Tensor </p> <ul> <li><p> Smoothed gradients, same shape as the inputs.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/concepts/cav/","title":"CAV","text":"<p>CAV or Concept Activation Vector represent a high-level concept as a vector that indicate the direction to take (for activations of a layer) to maximise this concept.</p> <p>Quote</p> <p>[...] CAV for a concept is simply a vector in the direction of the values (e.g., activations) of that concept\u2019s set of examples\u2026 we derive CAVs by training a linear classifier between a concept\u2019s examples and random counter examples and then taking the vector orthogonal to the decision boundary.</p> <p>-- Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV) (2018).1</p> <p>For a layer \\(f_l\\) of a model, we seek the linear classifier \\(v_l \\in \\mathbb{R}^d\\) that separate the activations of the positive examples \\(\\{ f_l(x) : x \\in \\mathcal{P} \\}\\), and the activations of the random/negative examples \\(\\{ f_l(x) : x \\in \\mathcal{R} \\}\\).</p>"},{"location":"api/concepts/cav/#example","title":"Example","text":"<pre><code>from xplique.concepts import Cav\n\ncav_renderer = Cav(model, 'mixed4d', classifier='SGD', test_fraction=0.1)\ncav = cav_renderer(positive_examples, random_examples)\n</code></pre>"},{"location":"api/concepts/cav/#Cav","title":"<code>Cav</code>","text":"<p>Used to compute the Concept Activation Vector, which is a vector in the direction of the activations of that concept\u2019s set of examples. </p>"},{"location":"api/concepts/cav/#__init__","title":"<code>__init__(self,  model:  keras.engine.training.Model,  target_layer:  Union[str, int],  classifier:  Union[str, Callable] = 'SGD',  test_fraction:  float = 0.2,  batch_size:  int = 64,  verbose:  bool = False)</code>","text":"<p>Parameters</p> <ul> <li> <p>model            : keras.engine.training.Model </p> <ul> <li><p> Model to extract concept from.</p> </li> </ul> </li> <li> <p>target_layer            : Union[str, int] </p> <ul> <li><p> Index of the target layer or name of the layer.</p> </li> </ul> </li> <li> <p>classifier            : 'SGD' or 'SVC' or Sklearn model, optional </p> <ul> <li><p> Default implementation use SGD with hinge classifier (linear SVM), SVC use libsvm but the computation time is longer.</p> </li> </ul> </li> <li> <p>test_fraction            : float = 0.2 </p> <ul> <li><p> Fraction of the dataset used for test</p> </li> </ul> </li> <li> <p>batch_size            : int = 64 </p> <ul> <li><p> Batch size during the activations extraction</p> </li> </ul> </li> <li> <p>verbose            : bool = False </p> <ul> <li><p> If true, display information while training the classifier</p> </li> </ul> </li> </ul>"},{"location":"api/concepts/cav/#fit","title":"<code>fit(self,  positive_dataset:  tf.Tensor,  negative_dataset:  tf.Tensor) -&gt; tf.Tensor</code>","text":"<p>Compute and return the Concept Activation Vector (CAV) associated to the dataset and the layer targeted. </p> <p>Parameters</p> <ul> <li> <p>positive_dataset            : tf.Tensor </p> <ul> <li><p> Dataset of positive samples : samples containing the concept.</p> </li> </ul> </li> <li> <p>negative_dataset            : tf.Tensor </p> <ul> <li><p> Dataset of negative samples : samples without the concept</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>cav            : tf.Tensor </p> <ul> <li><p> Vector of the same shape as the layer output</p> </li> </ul> </li> </ul> <p></p> <ol> <li> <p>Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV) (2018). \u21a9</p> </li> </ol>"},{"location":"api/concepts/tcav/","title":"TCAV","text":"<p>TCAV or Testing with Concept Activation Vector consist consists in using a concept activation vector (CAV) to quantify the relationship between this concept and a class.</p> <p>This is done by using the directional derivative of the concept vector on several samples of a given class and measuring the percentage of positive (a positive directional derivative indicating that an infinitesimal addition of the concept increases the probability of the class).</p> <p>For a Concept Activation Vector \\(v_l\\) of a layer \\(f_l\\) of a model, and \\(f_{c}\\) the logit of the class \\(c\\), we measure the directional derivative \\(S_c(x) = v_l \\cdot \\frac{ \\partial{f_c(x)} } { \\partial{f_l}(x) }\\).</p> <p>The TCAV score is the percentage of elements of the class \\(c\\) for which the \\(S_c\\) is positive.</p> \\[ TCAV_c = \\frac{|x \\in \\mathcal{X}^c : S_c(x) &gt; 0 |}{ | \\mathcal{X}^c | } \\]"},{"location":"api/concepts/tcav/#example","title":"Example","text":"<pre><code>from xplique.concepts import Tcav\n\ntcav_renderer = Tcav(model, 'mixed4d') # you can also pass the layer index (e.g -1)\ntcav_score = tcav_renderer(samples, class_index, cav)\n</code></pre>"},{"location":"api/concepts/tcav/#Tcav","title":"<code>Tcav</code>","text":"<p>Used to Test a Concept Activation Vector, using the sign of the directional derivative of a concept vector relative to a class. </p>"},{"location":"api/concepts/tcav/#__init__","title":"<code>__init__(self,  model:  keras.engine.training.Model,  target_layer:  Union[str, int],  batch_size:  Optional[int] = 64)</code>","text":"<p>Parameters</p> <ul> <li> <p>model            : keras.engine.training.Model </p> <ul> <li><p> Model to extract concept from.</p> </li> </ul> </li> <li> <p>target_layer            : Union[str, int] </p> <ul> <li><p> Index of the target layer or name of the layer.</p> </li> </ul> </li> <li> <p>batch_size            : Optional[int] = 64 </p> <ul> <li><p> Batch size during the predictions.</p> </li> </ul> </li> </ul>"},{"location":"api/concepts/tcav/#directional_derivative","title":"<code>directional_derivative(multi_head_model:  keras.engine.training.Model,  inputs:  tf.Tensor,  label:  int,  cav:  tf.Tensor) -&gt; tf.Tensor</code>","text":"<p>Compute the gradient of the label relative to the activations of the CAV layer. </p> <p>Parameters</p> <ul> <li> <p>multi_head_model            : keras.engine.training.Model </p> <ul> <li><p> Model reconfigured, first output is the activations of the CAV layer, and the second output is the prediction layer.</p> </li> </ul> </li> <li> <p>inputs            : tf.Tensor </p> <ul> <li><p> Input sample on which to test the influence of the concept.</p> </li> </ul> </li> <li> <p>label            : int </p> <ul> <li><p> Index of the class to test.</p> </li> </ul> </li> <li> <p>cav            : tf.Tensor </p> <ul> <li><p> Concept Activation Vector, same shape as the activations output.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>directional_derivative            : tf.Tensor </p> <ul> <li><p> Directional derivative values of each samples.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/concepts/tcav/#score","title":"<code>score(self,  inputs:  tf.Tensor,  label:  int,  cav:  tf.Tensor) -&gt; float</code>","text":"<p>Compute and return the TCAV score of the CAV associated to class tested. </p> <p>Parameters</p> <ul> <li> <p>inputs            : tf.Tensor </p> <ul> <li><p> Input sample on which to test the influence of the concept.</p> </li> </ul> </li> <li> <p>label            : int </p> <ul> <li><p> Index of the class to test.</p> </li> </ul> </li> <li> <p>cav            : tf.Tensor </p> <ul> <li><p> Concept Activation Vector, see CAV module.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>tcav            : float </p> <ul> <li><p> Percentage of sample for which increasing the concept has a positive impact on the class logit.</p> </li> </ul> </li> </ul> <p></p> <ol> <li> <p>Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV) (2018). \u21a9</p> </li> </ol>"},{"location":"api/feature_viz/feature_viz/","title":"Feature Visualization","text":"<p>One of the specificities of neural networks is their differentiability. This characteristic allows us to compute gradients, either the gradient of a loss with respect to the parameters, or in the case we are interested in here, of a part of the network with respect to the input. This gradient then allows us to iteratively modify the input in order to maximize an objective such as a neuron, a channel or a combination of objectives.</p> <p>Quote</p> <p>If we want to understand individual features, we can search for examples where they have high values either for a neuron at an individual position, or for an entire channel. -- Feature Visualization -- How neural networks build up their understanding of images (2017)1</p> <p>More precisely, the explanation of a neuron \\(n\\) denoted as \\(\\phi^{(n)}\\) is an input \\(x* \\in \\mathcal{X}\\) such that</p> \\[ \\phi^{(n)} = \\underset{x}{arg\\ max}\\ f(x)^{(n)} - \\mathcal{R}(x) \\] <p>with \\(f(x)^{(n)}\\) the neuron score for an input \\(x\\) and \\(\\mathcal{R}(x)\\) a regularization term. In practice it turns out that preconditioning the input in a decorrelated space such as the frequency domain allows to obtain more consistent results and to better formulate the regularization (e.g. by controlling the rate of high frequency and low frequency desired).</p>"},{"location":"api/feature_viz/feature_viz/#examples","title":"Examples","text":"<p>Optimize the ten logits of a neural network (we recommend to remove the softmax activation of your network).</p> <pre><code>from xplique.features_visualizations import Objective\nfrom xplique.features_visualizations import optimize\n\n# load a model...\n\n# targeting the 10 logits of the layer 'logits'\n# we can also target a layer by its index, like -1 for the last layer\nlogits_obj = Objective.neuron(model, \"logits\", list(range(10)))\nimages, obj_names = optimize(logits_obj) # 10 images, one for each logits\n</code></pre> <p>Create a combination of multiple objectives and aggregate them</p> <pre><code>from xplique.features_visualizations import Objective\nfrom xplique.features_visualizations import optimize\n\n# load a model...\n\n# target the first logits neuron\nlogits_obj = Objective.neuron(model, \"logits\", 0)\n# target the third layer\nlayer_obj = Objective.layer(model, \"conv2d_1\")\n# target the second channel of another layer\nchannel_obj = Objective.channel(model, \"mixed4_2\", 2)\n\n# combine the objective\nobj = logits_obj * 1.0 + layer_obj * 3.0 + channel_obj * (-5.0)\nimages, obj_names = optimize(logits_obj) # 1 resulting image\n</code></pre>"},{"location":"api/feature_viz/feature_viz/#Objective","title":"<code>Objective</code>","text":"<p>Use to combine several sub-objectives into one. </p>"},{"location":"api/feature_viz/feature_viz/#__init__","title":"<code>__init__(self,  model:  keras.engine.training.Model,  layers:  List[keras.engine.base_layer.Layer],  masks:  List[tf.Tensor],  funcs:  List[Callable],  multipliers:  List[float],  names:  List[str])</code>","text":"<p>Parameters</p> <ul> <li> <p>model            : keras.engine.training.Model </p> <ul> <li><p> Model used for optimization.</p> </li> </ul> </li> <li> <p>layers            : List[keras.engine.base_layer.Layer] </p> <ul> <li><p> A list of the layers output for each sub-objectives.</p> </li> </ul> </li> <li> <p>masks            : List[tf.Tensor] </p> <ul> <li><p> A list of masks that will be applied on the targeted layer for each sub-objectives.</p> </li> </ul> </li> <li> <p>funcs            : List[Callable] </p> <ul> <li><p> A list of loss functions for each sub-objectives.</p> </li> </ul> </li> <li> <p>multipliers            : List[float] </p> <ul> <li><p> A list of multiplication factor for each sub-objectives</p> </li> </ul> </li> <li> <p>names            : List[str] </p> <ul> <li><p> A list of name for each sub-objectives</p> </li> </ul> </li> </ul>"},{"location":"api/feature_viz/feature_viz/#channel","title":"<code>channel(model:  keras.engine.training.Model,  layer:  Union[str, int],  channel_ids:  Union[int, List[int]],  multiplier:  float = 1.0,  names:  Union[str, List[str], None] = None)</code>","text":"<p>Util to build an objective to maximise a channel. </p> <p>Parameters</p> <ul> <li> <p>model            : keras.engine.training.Model </p> <ul> <li><p> Model used for optimization.</p> </li> </ul> </li> <li> <p>layer            : Union[str, int] </p> <ul> <li><p> Index or name of the targeted layer.</p> </li> </ul> </li> <li> <p>channel_ids            : Union[int, List[int]] </p> <ul> <li><p> Indexes of the channels to maximise.</p> </li> </ul> </li> <li> <p>multiplier            : float = 1.0 </p> <ul> <li><p> Multiplication factor of the objectives.</p> </li> </ul> </li> <li> <p>names            : Union[str, List[str], None] = None </p> <ul> <li><p> Names for each objectives.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>objective </p> <ul> <li><p> An objective containing a sub-objective for each channels.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/feature_viz/feature_viz/#compile","title":"<code>compile(self) -&gt; Tuple[keras.engine.training.Model,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Callable,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 List[str], Tuple]</code>","text":"<p>Compile all the sub-objectives into one and return the objects for the optimisation process. </p> <p>Return</p> <ul> <li> <p>model_reconfigured            : Tuple[keras.engine.training.Model, Callable, List[str], Tuple] </p> <ul> <li><p> Model with the outputs needed for the optimization.</p> </li> </ul> </li> <li> <p>objective_function            : Tuple[keras.engine.training.Model, Callable, List[str], Tuple] </p> <ul> <li><p> Function to call that compute the loss for the objectives.</p> </li> </ul> </li> <li> <p>names            : Tuple[keras.engine.training.Model, Callable, List[str], Tuple] </p> <ul> <li><p> Names of each objectives.</p> </li> </ul> </li> <li> <p>input_shape            : Tuple[keras.engine.training.Model, Callable, List[str], Tuple] </p> <ul> <li><p> Shape of the input, one sample for each optimization.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/feature_viz/feature_viz/#direction","title":"<code>direction(model:  keras.engine.training.Model,  layer:  Union[str, int],  vectors:  Union[tf.Tensor, List[tf.Tensor]],  multiplier:  float = 1.0,  cossim_pow:  float = 2.0,  names:  Union[str, List[str], None] = None)</code>","text":"<p>Util to build an objective to maximise a direction of a layer. </p> <p>Parameters</p> <ul> <li> <p>model            : keras.engine.training.Model </p> <ul> <li><p> Model used for optimization.</p> </li> </ul> </li> <li> <p>layer            : Union[str, int] </p> <ul> <li><p> Index or name of the targeted layer.</p> </li> </ul> </li> <li> <p>vectors            : Union[tf.Tensor, List[tf.Tensor]] </p> <ul> <li><p> Direction(s) to optimize.</p> </li> </ul> </li> <li> <p>multiplier            : float = 1.0 </p> <ul> <li><p> Multiplication factor of the objective.</p> </li> </ul> </li> <li> <p>cossim_pow            : float = 2.0 </p> <ul> <li><p> Power of the cosine similarity, higher value encourage the objective to care more about the angle of the activations.</p> </li> </ul> </li> <li> <p>names            : Union[str, List[str], None] = None </p> <ul> <li><p> A name for each objectives.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>objective </p> <ul> <li><p> An objective ready to be compiled</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/feature_viz/feature_viz/#layer","title":"<code>layer(model:  keras.engine.training.Model,  layer:  Union[str, int],  reducer:  str = 'magnitude',  multiplier:  float = 1.0,  name:  Optional[str] = None)</code>","text":"<p>Util to build an objective to maximise a layer. </p> <p>Parameters</p> <ul> <li> <p>model            : keras.engine.training.Model </p> <ul> <li><p> Model used for optimization.</p> </li> </ul> </li> <li> <p>layer            : Union[str, int] </p> <ul> <li><p> Index or name of the targeted layer.</p> </li> </ul> </li> <li> <p>reducer            : str = 'magnitude' </p> <ul> <li><p> Type of reduction to apply, 'mean' will optimize the mean value of the layer, 'magnitude' will optimize the mean of the absolute values.</p> </li> </ul> </li> <li> <p>multiplier            : float = 1.0 </p> <ul> <li><p> Multiplication factor of the objective.</p> </li> </ul> </li> <li> <p>name            : Optional[str] = None </p> <ul> <li><p> A name for the objective.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>objective </p> <ul> <li><p> An objective ready to be compiled</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/feature_viz/feature_viz/#neuron","title":"<code>neuron(model:  keras.engine.training.Model,  layer:  Union[str, int],  neurons_ids:  Union[int, List[int]],  multiplier:  float = 1.0,  names:  Union[str, List[str], None] = None)</code>","text":"<p>Util to build an objective to maximise a neuron. </p> <p>Parameters</p> <ul> <li> <p>model            : keras.engine.training.Model </p> <ul> <li><p> Model used for optimization.</p> </li> </ul> </li> <li> <p>layer            : Union[str, int] </p> <ul> <li><p> Index or name of the targeted layer.</p> </li> </ul> </li> <li> <p>neurons_ids            : Union[int, List[int]] </p> <ul> <li><p> Indexes of the neurons to maximise.</p> </li> </ul> </li> <li> <p>multiplier            : float = 1.0 </p> <ul> <li><p> Multiplication factor of the objectives.</p> </li> </ul> </li> <li> <p>names            : Union[str, List[str], None] = None </p> <ul> <li><p> Names for each objectives.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>objective </p> <ul> <li><p> An objective containing a sub-objective for each neurons.</p> </li> </ul> </li> </ul> <p></p> <ol> <li> <p>Feature Visualization -- How neural networks build up their understanding of images (2017) \u21a9</p> </li> </ol>"},{"location":"api/metrics/api_metrics/","title":"API: Metrics","text":""},{"location":"api/metrics/api_metrics/#context","title":"Context","text":"<p>As the XAI field continues on being trendy, the quantity of materials at disposal to explain DL models keeps on growing. Especially, there is an increasing need to benchmark and evaluate those different approaches. Mainly, there is an urge to evaluate the quality of explanations provided by attribution methods.</p> <p>Info</p> <p>Note that, even though some work exists for other tasks, this challenge has been mainly tackled in the context of Computer Vision tasks.</p> <p>As pointed out by Petsiuk et al. most explanations approaches are used to be evaluated in a human-centred way.  For instance, an attribution method was considered as good if it pointed out the same relevant pixels as the ones highlighted by human users. While this kind of evaluation allows giving some user trust it can easily be biased. Therefore, the authors introduced two automatic evaluation metrics that rely solely on the drop or rise in the probability of a class as important pixels (defined by the saliency map) are removed or added. Those are not the only available metrics and we propose here to present the API we used as common ground and then to dive into more specifity.</p>"},{"location":"api/metrics/api_metrics/#common-api","title":"Common API","text":"<p>All metrics inherits from the base class <code>BaseAttributionMetric</code> which has the following <code>__init__</code> arguments:</p> <ul> <li>- <code>model</code>: The model from which we want to obtain explanations</li> <li> <p>- <code>inputs</code>: Input samples to be explained</p> <p>Warning</p> <p>Inputs should be the same as defined in the attribution's API Description</p> </li> <li> <p>- <code>targets</code>: One-hot encoding of the model's output from which an explanation is desired</p> <p>Warning</p> <p>Idem</p> </li> <li> <p>- <code>batch_size</code></p> </li> </ul> <p>Then we can distinguish two category of metrics:</p> <ul> <li>- Those which only need the attribution ouputs of an explainer: <code>ExplanationMetric</code></li> <li>- Those which need the explainer: <code>ExplainerMetric</code></li> </ul>"},{"location":"api/metrics/api_metrics/#explanationmetric","title":"<code>ExplanationMetric</code>","text":"<p>Those metrics are agnostic of the explainer used and rely only on the attributions mappings it gives.</p> <p>Tip</p> <p>Therefore, you can use them with other explainer than those provided in Xplique!</p> <p>All metrics inheriting from this class have another argument in their <code>__init__</code> method:</p> <ul> <li>- <code>operator</code>: Optionnal function wrapping the model. It can be seen as a metric which allow to evaluate model evolution. For more details, see the attribution's API Description</li> </ul> <p>All metrics inheriting from this class have to define a method <code>evaluate</code> which will take as input the <code>attributions</code> given by an explainer. Those attributions should correspond to the <code>model</code>, <code>inputs</code> and <code>targets</code> used to build the metric object.</p> <p>Especially, all Fidelity metrics inherit from this class:</p> Metric Name (Fidelity) Notebook MuFidelity Insertion Deletion"},{"location":"api/metrics/api_metrics/#explainermetric","title":"<code>ExplainerMetric</code>","text":"<p>Those metrics will not assess the quality of the explanations provided but (also) the explainer itself.</p> <p>All metrics inheriting from this class have to define a method <code>evaluate</code> which will take as input the <code>explainer</code> evaluated.</p> <p>Info</p> <p>It is even more important that <code>inputs</code> and <code>targets</code> are the same as defined in the attribution's API Description</p> <p>Currently, there is only one Stability metric inheriting from this class:</p> Metric Name (Stability) Notebook AverageStability (WIP)"},{"location":"api/metrics/api_metrics/#other-metrics","title":"Other Metrics","text":"<p>A Representatibity metric: MeGe is also available. Documentation about it should be added soon.</p>"},{"location":"api/metrics/api_metrics/#notebooks","title":"Notebooks","text":"<ul> <li>Metrics: Getting started </li> </ul>"},{"location":"api/metrics/avg_stability/","title":"Average Stability","text":"<p>Average Stability is a Stability metric measuring how similar are explanations of similar inputs.</p> <p>Quote</p> <p>[...]  We want to ensure that, if inputs are near each other and their model outputs are similar, then their explanations should be close to each other.</p> <p>-- Evaluating and Aggregating Feature-based Model Explanations (2020)1</p> <p>Formally, given a predictor \\(f\\), an explanation function \\(g\\), a point \\(x\\), a radius \\(r\\) and a two distance metric: \\(\\rho\\) over the inputs and \\(D\\) over the explanations, the AverageStability is defined as:</p> \\[ S = \\underset{z : \\rho(x, z) \\leq r}{\\int} D(g(f, x), g(f, z))\\ dz \\] <p>Info</p> <p>The better the method, the smaller the score.</p>"},{"location":"api/metrics/avg_stability/#example","title":"Example","text":"<pre><code>from xplique.metrics import AverageStability\nfrom xplique.attributions import Saliency\n\n# load images, labels and model\n# ...\nexplainer = Saliency(model)\n\nmetric = AverageStability(model, inputs, labels)\nscore = metric.evaluate(explainer)\n</code></pre>"},{"location":"api/metrics/avg_stability/#AverageStability","title":"<code>AverageStability</code>","text":"<p>Used to compute the average sensitivity metric (or stability). This metric ensure that close inputs with similar predictions yields similar explanations. For each inputs we randomly sample noise to add to the inputs and compute the explanation for the noisy inputs. We then get the average distance between the original explanations and the noisy explanations. </p>"},{"location":"api/metrics/avg_stability/#__init__","title":"<code>__init__(self,  model:  Callable,  inputs:  Union[tf.Dataset, tf.Tensor, numpy.ndarray],  targets:  Union[tf.Tensor, numpy.ndarray, None] = None,  batch_size:  Optional[int] = 64,  radius:  float = 0.1,  distance:  Union[str, Callable] = 'l2',  nb_samples:  int = 20)</code>","text":"<p>Parameters</p> <ul> <li> <p>model            : Callable </p> <ul> <li><p> Model used for computing metric.</p> </li> </ul> </li> <li> <p>inputs            : Union[tf.Dataset, tf.Tensor, numpy.ndarray] </p> <ul> <li><p> Input samples under study.</p> </li> </ul> </li> <li> <p>targets            : Union[tf.Tensor, numpy.ndarray, None] = None </p> <ul> <li><p> One-hot encoded labels or regression target (e.g {+1, -1}), one for each sample.</p> </li> </ul> </li> <li> <p>batch_size            : Optional[int] = 64 </p> <ul> <li><p> Number of samples to explain at once, if None compute all at once.</p> </li> </ul> </li> <li> <p>radius            : float = 0.1 </p> <ul> <li><p> Maximum value of the uniform noise added to the inputs before recalculating their explanations.</p> </li> </ul> </li> <li> <p>distance            : Union[str, Callable] = 'l2' </p> <ul> <li><p> Distance metric between the explanations.</p> </li> </ul> </li> <li> <p>nb_samples            : int = 20 </p> <ul> <li><p> Number of different neighbors points to try on each input to measure the stability.</p> </li> </ul> </li> </ul>"},{"location":"api/metrics/avg_stability/#evaluate","title":"<code>evaluate(self,  explainer:  Callable,  base_explanations:  Union[tf.Tensor, numpy.ndarray, None] = None) -&gt; float</code>","text":"<p>Evaluate the fidelity score. </p> <p>Parameters</p> <ul> <li> <p>explainer            : Callable </p> <ul> <li><p> Explainer or Explanations associated to each inputs.</p> </li> </ul> </li> <li> <p>base_explanations            : Union[tf.Tensor, numpy.ndarray, None] = None </p> <ul> <li><p> Explanation for the inputs under study. Calculates them automatically if they are not provided.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>stability_score            : float </p> <ul> <li><p> Average distance between the explanations</p> </li> </ul> </li> </ul> <p></p> <p>Warning</p> <p>AverageStability will compute several time explanations for all the inputs (pertubed more or less severly). Thus, it might be very long to compute (especially if the explainer is already time consumming).</p> <ol> <li> <p>Evaluating and Aggregating Feature-based Model Explanations (2020) \u21a9</p> </li> </ol>"},{"location":"api/metrics/deletion/","title":"Deletion","text":"<p>The Deletion Fidelity metric measures how well a saliency-map\u2013based explanation localizes the important features.</p> <p>Quote</p> <p>The deletion metric measures the drop in the probability of a class as important pixels (given by the saliency map) are gradually removed from the image. A sharp drop, and thus a small area under the probability curve, are indicative of a good explanation.</p> <p>-- RISE: Randomized Input Sampling for Explanation of Black-box Models (2018)1</p>"},{"location":"api/metrics/deletion/#score-interpretation","title":"Score interpretation","text":"<p>The interpretation of the score depends on your <code>operator</code>, which represents the metrics you use to evaluate your model. For metrics where the score increases with the performance of the model (such as accuracy).   If explanations are accurate, the score will quickly fall to the score of a random model.   Thus, in this case, a lower score represent a more accurate explanation.</p> <p>For metrics where the score decreases with the performance of the model (such as losses).    If explanations are accurate, the score will quickly rise to the score of a random model.   Thus, in this case, a higher score represent a more accurate explanation.</p>"},{"location":"api/metrics/deletion/#remarks","title":"Remarks","text":"<p>This metric only evaluate the order of importance between features.</p> <p>The parameters metric, steps and max_percentage_perturbed may drastically change the score :</p> <ul> <li> <p>For inputs with many features, increasing the number of steps will allow you to capture more efficiently the difference between attributions methods.</p> </li> <li> <p>The order of importance of features with low importance may not matter, hence, decreasing the max_percentage_perturbed, may make the score more relevant.</p> </li> </ul> <p>Sometimes, attributions methods also returns negative attributions, for those methods, do not take the absolute value before computing insertion and deletion metrics. Otherwise, negative attributions may have higher absolute values, and the order of importance between features will change. Therefore, take those previous remarks into account to get a relevant score.</p>"},{"location":"api/metrics/deletion/#example","title":"Example","text":"<pre><code>from xplique.metrics import Deletion\nfrom xplique.attributions import Saliency\n\n# load images, targets and model\n# ...\nexplainer = Saliency(model)\nexplanations = explainer(inputs, targets)\n\nmetric = Deletion(model, inputs, targets)\nscore = metric.evaluate(explanations)\n</code></pre>"},{"location":"api/metrics/deletion/#Deletion","title":"<code>Deletion</code>","text":"<p>The deletion metric measures the drop in the probability of a class as important pixels (given by the saliency map) are gradually removed from the image. A sharp drop, and thus a small area under the probability curve, are indicative of a good explanation. </p>"},{"location":"api/metrics/deletion/#__init__","title":"<code>__init__(self,  model:  keras.engine.training.Model,  inputs:  Union[tf.Dataset, tf.Tensor, numpy.ndarray],  targets:  Union[tf.Tensor, numpy.ndarray, None] = None,  batch_size:  Optional[int] = 64,  baseline_mode:  Union[float, Callable] = 0.0,  steps:  int = 10,  max_percentage_perturbed:  float = 1.0,  operator:  Optional[Callable] = None)</code>","text":"<p>Parameters</p> <ul> <li> <p>model            : keras.engine.training.Model </p> <ul> <li><p> Model used for computing metric.</p> </li> </ul> </li> <li> <p>inputs            : Union[tf.Dataset, tf.Tensor, numpy.ndarray] </p> <ul> <li><p> Input samples under study.</p> </li> </ul> </li> <li> <p>targets            : Union[tf.Tensor, numpy.ndarray, None] = None </p> <ul> <li><p> One-hot encoded labels or regression target (e.g {+1, -1}), one for each sample.</p> </li> </ul> </li> <li> <p>batch_size            : Optional[int] = 64 </p> <ul> <li><p> Number of samples to explain at once, if None compute all at once.</p> </li> </ul> </li> <li> <p>baseline_mode            : Union[float, Callable] = 0.0 </p> <ul> <li><p> Value of the baseline state, will be called with the inputs if it is a function.</p> </li> </ul> </li> <li> <p>steps            : int = 10 </p> <ul> <li><p> Number of steps between the start and the end state.</p><p> Can be set to -1 for all possible steps to be computed.</p> </li> </ul> </li> <li> <p>max_percentage_perturbed            : float = 1.0 </p> <ul> <li><p> Maximum percentage of the input perturbed.</p> </li> </ul> </li> <li> <p>operator            : Optional[Callable] = None </p> <ul> <li><p> Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y].</p> </li> </ul> </li> </ul>"},{"location":"api/metrics/deletion/#detailed_evaluate","title":"<code>detailed_evaluate(self,  explanations:  Union[tf.Tensor, numpy.ndarray]) -&gt; Dict[int, float]</code>","text":"<p>Evaluate model performance for successive perturbations of an input. Used to compute causal score. </p> <p>Parameters</p> <ul> <li> <p>explanations            : Union[tf.Tensor, numpy.ndarray] </p> <ul> <li><p> Explanation for the inputs, labels to evaluate.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>causal_score_dict            : Dict[int, float] </p> <ul> <li><p> Dictionary of scores obtain for different perturbations Keys are the steps, i.e the number of features perturbed Values are the scores, the score of the model     on the inputs with the corresponding number of features perturbed</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/metrics/deletion/#evaluate","title":"<code>evaluate(self,  explanations:  Union[tf.Tensor, numpy.ndarray]) -&gt; float</code>","text":"<p>Evaluate the causal score. </p> <p>Parameters</p> <ul> <li> <p>explanations            : Union[tf.Tensor, numpy.ndarray] </p> <ul> <li><p> Explanation for the inputs, labels to evaluate.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>causal_score            : float </p> <ul> <li><p> Metric score, area over the deletion (lower is better) or insertion (higher is better) curve.</p> </li> </ul> </li> </ul> <p></p> <ol> <li> <p>RISE: Randomized Input Sampling for Explanation of Black-box Models (2018) \u21a9</p> </li> </ol>"},{"location":"api/metrics/insertion/","title":"Insertion","text":"<p>The Insertion Fidelity metric measures how well a saliency-map\u2013based explanation can find elements that are minimal for the predictions.</p> <p>Quote</p> <p>The insertion metric, on the other hand, captures the importance of the pixels in terms of their ability to synthesize an image and is measured by the rise in the probability of the class of interest as pixels are added according to the generated importance map.</p> <p>-- RISE: Randomized Input Sampling for Explanation of Black-box Models (2018)1</p>"},{"location":"api/metrics/insertion/#score-interpretation","title":"Score interpretation","text":"<p>The interpretation of the score depends on your <code>operator</code>, which represents the metrics you use to evaluate your model. For metrics where the score increases with the performance of the model (such as accuracy).   If explanations are accurate, the score will quickly rise to the score on non-perturbed input.   Thus, in this case, a higher score represent a more accurate explanation.</p> <p>For metrics where the score decreases with the performance of the model (such as losses).    If explanations are accurate, the score will quickly fall to the score on non-perturbed input.   Thus, in this case, a lower score represent a more accurate explanation.</p>"},{"location":"api/metrics/insertion/#remarks","title":"Remarks","text":"<p>This metric only evaluate the order of importance between features.</p> <p>The parameters metric, steps and max_percentage_perturbed may drastically change the score :</p> <ul> <li> <p>For inputs with many features, increasing the number of steps will allow you to capture more efficiently the difference between attributions methods.</p> </li> <li> <p>The order of importance of features with low importance may not matter, hence, decreasing the max_percentage_perturbed, may make the score more relevant.</p> </li> </ul> <p>Sometimes, attributions methods also returns negative attributions, for those methods, do not take the absolute value before computing insertion and deletion metrics. Otherwise, negative attributions may have higher absolute values, and the order of importance between features will change. Therefore, take those previous remarks into account to get a relevant score.</p>"},{"location":"api/metrics/insertion/#example","title":"Example","text":"<pre><code>from xplique.metrics import Insertion\nfrom xplique.attributions import Saliency\n\n# load images, labels and model\n# ...\nexplainer = Saliency(model)\nexplanations = explainer(inputs, labels)\n\nmetric = Insertion(model, inputs, labels)\nscore = metric.evaluate(explanations)\n</code></pre>"},{"location":"api/metrics/insertion/#Insertion","title":"<code>Insertion</code>","text":"<p>The insertion metric, on the other hand, captures the importance of the pixels in terms of their ability to synthesize an image and is measured by the rise in the probability of the class of interest as pixels are added according to the generated importance map. </p>"},{"location":"api/metrics/insertion/#__init__","title":"<code>__init__(self,  model:  keras.engine.training.Model,  inputs:  Union[tf.Dataset, tf.Tensor, numpy.ndarray],  targets:  Union[tf.Tensor, numpy.ndarray, None] = None,  batch_size:  Optional[int] = 64,  baseline_mode:  Union[float, Callable] = 0.0,  steps:  int = 10,  max_percentage_perturbed:  float = 1.0,  operator:  Optional[Callable] = None)</code>","text":"<p>Parameters</p> <ul> <li> <p>model            : keras.engine.training.Model </p> <ul> <li><p> Model used for computing metric.</p> </li> </ul> </li> <li> <p>inputs            : Union[tf.Dataset, tf.Tensor, numpy.ndarray] </p> <ul> <li><p> Input samples under study.</p> </li> </ul> </li> <li> <p>targets            : Union[tf.Tensor, numpy.ndarray, None] = None </p> <ul> <li><p> One-hot encoded labels or regression target (e.g {+1, -1}), one for each sample.</p> </li> </ul> </li> <li> <p>batch_size            : Optional[int] = 64 </p> <ul> <li><p> Number of samples to explain at once, if None compute all at once.</p> </li> </ul> </li> <li> <p>baseline_mode            : Union[float, Callable] = 0.0 </p> <ul> <li><p> Value of the baseline state, will be called with the inputs if it is a function.</p> </li> </ul> </li> <li> <p>steps            : int = 10 </p> <ul> <li><p> Number of steps between the start and the end state.</p><p> Can be set to -1 for all possible steps to be computed.</p> </li> </ul> </li> <li> <p>max_percentage_perturbed            : float = 1.0 </p> <ul> <li><p> Maximum percentage of the input perturbed.</p> </li> </ul> </li> <li> <p>operator            : Optional[Callable] = None </p> <ul> <li><p> Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y].</p> </li> </ul> </li> </ul>"},{"location":"api/metrics/insertion/#detailed_evaluate","title":"<code>detailed_evaluate(self,  explanations:  Union[tf.Tensor, numpy.ndarray]) -&gt; Dict[int, float]</code>","text":"<p>Evaluate model performance for successive perturbations of an input. Used to compute causal score. </p> <p>Parameters</p> <ul> <li> <p>explanations            : Union[tf.Tensor, numpy.ndarray] </p> <ul> <li><p> Explanation for the inputs, labels to evaluate.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>causal_score_dict            : Dict[int, float] </p> <ul> <li><p> Dictionary of scores obtain for different perturbations Keys are the steps, i.e the number of features perturbed Values are the scores, the score of the model     on the inputs with the corresponding number of features perturbed</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/metrics/insertion/#evaluate","title":"<code>evaluate(self,  explanations:  Union[tf.Tensor, numpy.ndarray]) -&gt; float</code>","text":"<p>Evaluate the causal score. </p> <p>Parameters</p> <ul> <li> <p>explanations            : Union[tf.Tensor, numpy.ndarray] </p> <ul> <li><p> Explanation for the inputs, labels to evaluate.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>causal_score            : float </p> <ul> <li><p> Metric score, area over the deletion (lower is better) or insertion (higher is better) curve.</p> </li> </ul> </li> </ul> <p></p> <ol> <li> <p>RISE: Randomized Input Sampling for Explanation of Black-box Models (2018) \u21a9</p> </li> </ol>"},{"location":"api/metrics/mu_fidelity/","title":"MuFidelity","text":"<p>MuFidelity is a fidelity metric measuring the correlation between important variables defined by the explanation method and the decline in the model score when these variables are reset to a baseline state.</p> <p>Quote</p> <p>[...]  when we set particular features \\(x_s\\) to a baseline value \\(x_0\\) the change in predictor\u2019s output should be proportional to the sum of attribution scores.</p> <p>-- Evaluating and Aggregating Feature-based Model Explanations (2020)1</p> <p>Formally, given a predictor \\(f\\), an explanation function \\(g\\), a point \\(x \\in \\mathbb{R}^n\\) and a subset size \\(k\\) the MuFidelity metric is defined as:</p> \\[ \\mu F = \\underset{S \\subseteq \\{1, ..., d\\} \\\\ |S| = k}{Corr}( \\sum_{i \\in S} g(f, x)_i, f(x) - f(x_{[x_i = x_0 | i \\in S]})) \\] <p>Info</p> <p>The better the method, the higher the score.</p>"},{"location":"api/metrics/mu_fidelity/#example","title":"Example","text":"<pre><code>from xplique.metrics import MuFidelity\nfrom xplique.attributions import Saliency\n\n# load images, labels and model\n# ...\nexplainer = Saliency(model)\nexplanations = explainer(inputs, lablels)\n\nmetric = MuFidelity(model, inputs, labels)\nscore = metric.evaluate(explainations)\n</code></pre>"},{"location":"api/metrics/mu_fidelity/#MuFidelity","title":"<code>MuFidelity</code>","text":"<p>Used to compute the fidelity correlation metric. This metric ensure there is a correlation between a random subset of pixels and their attribution score. For each random subset created, we set the pixels of the subset at a baseline state and obtain the prediction score. This metric measures the correlation between the drop in the score and the importance of the explanation. </p>"},{"location":"api/metrics/mu_fidelity/#__init__","title":"<code>__init__(self,  model:  Callable,  inputs:  Union[tf.Dataset, tf.Tensor, numpy.ndarray],  targets:  Union[tf.Tensor, numpy.ndarray, None] = None,  batch_size:  Optional[int] = 64,  grid_size:  Optional[int] = 9,  subset_percent:  float = 0.2,  baseline_mode:  Union[Callable, float] = 0.0,  nb_samples:  int = 200,  operator:  Optional[Callable] = None)</code>","text":"<p>Parameters</p> <ul> <li> <p>model            : Callable </p> <ul> <li><p> Model used for computing metric.</p> </li> </ul> </li> <li> <p>inputs            : Union[tf.Dataset, tf.Tensor, numpy.ndarray] </p> <ul> <li><p> Input samples under study.</p> </li> </ul> </li> <li> <p>targets            : Union[tf.Tensor, numpy.ndarray, None] = None </p> <ul> <li><p> One-hot encoded labels or regression target (e.g {+1, -1}), one for each sample.</p> </li> </ul> </li> <li> <p>batch_size            : Optional[int] = 64 </p> <ul> <li><p> Number of samples to explain at once, if None compute all at once.</p> </li> </ul> </li> <li> <p>grid_size            : Optional[int] = 9 </p> <ul> <li><p> If none, compute the original metric, else cut the image in (grid_size, grid_size) and each element of the subset will be a super pixel representing one element of the grid.</p><p> You should use this when dealing with medium / large size images.</p> </li> </ul> </li> <li> <p>subset_percent            : float = 0.2 </p> <ul> <li><p> Percent of the image that will be set to baseline.</p> </li> </ul> </li> <li> <p>baseline_mode            : Union[Callable, float] = 0.0 </p> <ul> <li><p> Value of the baseline state, will be called with the a single input if it is a function.</p> </li> </ul> </li> <li> <p>nb_samples            : int = 200 </p> <ul> <li><p> Number of different subsets to try on each input to measure the correlation.</p> </li> </ul> </li> <li> <p>operator            : Optional[Callable] = None </p> <ul> <li><p> Function g to explain, g take 3 parameters (f, x, y) and should return a scalar, with f the model, x the inputs and y the targets. If None, use the standard operator g(f, x, y) = f(x)[y].</p> </li> </ul> </li> </ul>"},{"location":"api/metrics/mu_fidelity/#evaluate","title":"<code>evaluate(self,  explanations:  Union[tf.Tensor, numpy.ndarray]) -&gt; float</code>","text":"<p>Evaluate the fidelity score. </p> <p>Parameters</p> <ul> <li> <p>explanations            : Union[tf.Tensor, numpy.ndarray] </p> <ul> <li><p> Explanation for the inputs, labels to evaluate.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>fidelity_score            : float </p> <ul> <li><p> Metric score, average correlation between the drop in score when variables are set to a baseline state and the importance of these variables according to the explanations.</p> </li> </ul> </li> </ul> <p></p> <ol> <li> <p>Evaluating and Aggregating Feature-based Model Explanations (2020) \u21a9</p> </li> </ol>"}]}