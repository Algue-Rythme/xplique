{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\ud83e\udd8a Xplique (pronounced \\\u025bks.plik\\ ) is a Python toolkit dedicated to explainability, currently based on Tensorflow. The goal of this library is to gather the state of the art of Explainable AI to help you understand your complex neural network models. Explore Xplique docs \u00bb Attributions \u00b7 Concept \u00b7 Feature Visualization \u00b7 Metrics The library is composed of several modules, the Attributions Methods module implements various methods (e.g Saliency, Grad-CAM, Integrated-Gradients...), with explanations, examples and links to official papers. The Feature Visualization module allows to see how neural networks build their understanding of images by finding inputs that maximize neurons, channels, layers or compositions of these elements. The Concepts module allows you to extract human concepts from a model and to test their usefulness with respect to a class. Finally, the Metrics module covers the current metrics used in explainability. Used in conjunction with the Attribution Methods module, it allows you to test the different methods or evaluate the explanations of a model. \ud83d\udcda Table of contents \u00b6 \ud83d\udcda Table of contents \ud83d\ude80 Quick Start \ud83d\udd25 Tutorials \ud83d\udce6 What's Included \ud83d\udcde Callable \ud83d\udc4d Contributing \ud83d\udc40 See Also \ud83d\ude4f Acknowledgments \ud83d\udc68\u200d\ud83c\udf93 Creator \ud83d\udcdd License \ud83d\udd25 Tutorials \u00b6 We propose some Hands-on tutorials to get familiar with the library and its api: Attribution Methods : Getting started Attribution Methods : Tabular data and Regression Attribution Methods : Metrics Concepts Methods : Testing with Concept Activation Vectors Feature Visualization : Getting started You can find a certain number of other practical tutorials just here . This section is actively developed and more contents will be included. We will try to cover all the possible usage of the library, feel free to contact us if you have any suggestions or recommandations towards tutorials you would like to see. \ud83d\ude80 Quick Start \u00b6 Xplique requires a version of python higher than 3.6 and several libraries including Tensorflow and Numpy. Installation can be done using Pypi: pip install xplique Now that Xplique is installed, here are 4 basic examples of what you can do with the available modules. Attributions Methods \u00b6 let's start with a simple example, by computing Grad-CAM for several images (or a complete dataset) on a trained model. from xplique.attributions import GradCAM # load images, labels and model # ... explainer = GradCAM ( model ) explanations = explainer . explain ( images , labels ) # or just `explainer(images, labels)` Attributions Metrics \u00b6 In order to measure if the explanations provided by our method are faithful (it reflects well the functioning of the model) we can use a fidelity metric such as Deletion from xplique.attributions import GradCAM from xplique.metrics import Deletion # load images, labels and model # ... explainer = GradCAM ( model ) explanations = explainer ( inputs , labels ) metric = Deletion ( model , inputs , labels ) score_grad_cam = metric ( explanations ) Concepts Extraction \u00b6 Concerning the concept-based methods, we can for example extract a concept vector from a layer of a model. In order to do this, we use two datasets, one containing inputs containing the concept: positive_samples , the other containing other entries which do not contain the concept: negative_samples . from xplique.concepts import Cav # load a model, samples that contain a concept # (positive) and samples who don't (negative) # ... extractor = Cav ( model , 'mixed3' ) concept_vector = extractor ( positive_samples , negative_samples ) Feature Visualization \u00b6 Finally, in order to find an image that maximizes a neuron and at the same time a layer, we build two objectives that we combine together. We then call the optimizer which returns our images from xplique.features_visualizations import Objective from xplique.features_visualizations import optimize # load a model... neuron_obj = Objective . neuron ( model , \"logits\" , 200 ) channel_obj = Objective . layer ( model , \"mixed3\" , 10 ) obj = neuron_obj + 2.0 * channel_obj images , obj_names = optimize ( obj ) \ud83d\udce6 What's Included \u00b6 All the attributions method presented below handle both Classification and Regression tasks. Attribution Method Type of Model Source Tabular Data Images Time-Series Deconvolution TF Paper \u2714 \u2714 WIP Grad-CAM TF Paper \u2714 WIP Grad-CAM++ TF Paper \u2714 WIP Gradient Input TF Paper \u2714 \u2714 WIP Guided Backprop TF Paper \u2714 \u2714 WIP Integrated Gradients TF Paper \u2714 \u2714 WIP Kernel SHAP Callable* Paper \u2714 \u2714 WIP Lime Callable* Paper \u2714 \u2714 WIP Occlusion Callable* Paper \u2714 \u2714 WIP Rise Callable* Paper WIP \u2714 WIP Saliency TF Paper \u2714 \u2714 WIP SmoothGrad TF Paper \u2714 \u2714 WIP SquareGrad TF Paper \u2714 \u2714 WIP VarGrad TF Paper \u2714 \u2714 WIP * : See the Callable documentation Attribution Metrics Type of Model Property Source MuFidelity TF Fidelity Paper Deletion TF Fidelity Paper Insertion TF Fidelity Paper Average Stability TF Stability Paper MeGe TF Representativity Paper ReCo TF Consistency Paper (WIP) e-robustness Concepts method Type of Model Source Concept Activation Vector (CAV) TF Paper Testing CAV (TCAV) TF Paper (WIP) Robust TCAV (WIP) Automatic Concept Extraction (ACE) Feature Visualization (Paper) Type of Model Details Neurons TF Optimizes for specific neurons Layer TF Optimizes for specific layers Channel TF Optimizes for specific channels Direction TF Optimizes for specific vector Fourrier Preconditioning TF Optimize in Fourier basis (see preconditioning ) Objective combination TF Allows to combine objectives methods with TF need a Tensorflow model. \ud83d\udc4d Contributing \u00b6 Feel free to propose your ideas or come and contribute with us on the Xplique toolbox! We have a specific document where we describe in a simple way how to make your first pull request: just here . \ud83d\udc40 See Also \u00b6 This library is one approach of many to explain your model. We don't expect it to be the perfect solution; we create it to explore one point in the space of possibilities. Other tools to explain your model include: Lucid the wonderful library specialized in feature visualization from OpenAI. Captum the Pytorch library for Interpretability research Tf-explain that implement multiples attribution methods and propose callbacks API for tensorflow. Alibi Explain for model inspection and interpretation SHAP a very popular library to compute local explanations using the classic Shapley values from game theory and their related extensions To learn more about Explainable AI in general, see: Interpretable Machine Learning an excellent book by Christophe Molnar. Interpretability Beyond Feature Attribution by Been Kim. A Roadmap for the Rigorous Science of Interpretability by Finale Doshi-Velez. DEEL White paper a summary of the DEEL team on the challenges of certifiable AI and the role of explainability for this purpose \ud83d\ude4f Acknowledgments \u00b6 This project received funding from the French \u201dInvesting for the Future \u2013 PIA3\u201d program within the Artificial and Natural Intelligence Toulouse Institute (ANITI). The authors gratefully acknowledge the support of the DEEL project. \ud83d\udc68\u200d\ud83c\udf93 Creator \u00b6 This library was started as a side-project by Thomas FEL who is currently a graduate student at the Artificial and Natural Intelligence Toulouse Institute under the direction of Thomas SERRE . His thesis work focuses on explainability for deep neural networks. He then received help from some members of the DEEL team to enhance the library namely from Justin PLAKOO and Lucas HERVIER . \ud83d\udcdd License \u00b6 The package is released under MIT license .","title":"Home"},{"location":"#table-of-contents","text":"\ud83d\udcda Table of contents \ud83d\ude80 Quick Start \ud83d\udd25 Tutorials \ud83d\udce6 What's Included \ud83d\udcde Callable \ud83d\udc4d Contributing \ud83d\udc40 See Also \ud83d\ude4f Acknowledgments \ud83d\udc68\u200d\ud83c\udf93 Creator \ud83d\udcdd License","title":"\ud83d\udcda Table of contents"},{"location":"#tutorials","text":"We propose some Hands-on tutorials to get familiar with the library and its api: Attribution Methods : Getting started Attribution Methods : Tabular data and Regression Attribution Methods : Metrics Concepts Methods : Testing with Concept Activation Vectors Feature Visualization : Getting started You can find a certain number of other practical tutorials just here . This section is actively developed and more contents will be included. We will try to cover all the possible usage of the library, feel free to contact us if you have any suggestions or recommandations towards tutorials you would like to see.","title":"\ud83d\udd25 Tutorials"},{"location":"#quick-start","text":"Xplique requires a version of python higher than 3.6 and several libraries including Tensorflow and Numpy. Installation can be done using Pypi: pip install xplique Now that Xplique is installed, here are 4 basic examples of what you can do with the available modules.","title":"\ud83d\ude80 Quick Start"},{"location":"#whats-included","text":"All the attributions method presented below handle both Classification and Regression tasks. Attribution Method Type of Model Source Tabular Data Images Time-Series Deconvolution TF Paper \u2714 \u2714 WIP Grad-CAM TF Paper \u2714 WIP Grad-CAM++ TF Paper \u2714 WIP Gradient Input TF Paper \u2714 \u2714 WIP Guided Backprop TF Paper \u2714 \u2714 WIP Integrated Gradients TF Paper \u2714 \u2714 WIP Kernel SHAP Callable* Paper \u2714 \u2714 WIP Lime Callable* Paper \u2714 \u2714 WIP Occlusion Callable* Paper \u2714 \u2714 WIP Rise Callable* Paper WIP \u2714 WIP Saliency TF Paper \u2714 \u2714 WIP SmoothGrad TF Paper \u2714 \u2714 WIP SquareGrad TF Paper \u2714 \u2714 WIP VarGrad TF Paper \u2714 \u2714 WIP * : See the Callable documentation Attribution Metrics Type of Model Property Source MuFidelity TF Fidelity Paper Deletion TF Fidelity Paper Insertion TF Fidelity Paper Average Stability TF Stability Paper MeGe TF Representativity Paper ReCo TF Consistency Paper (WIP) e-robustness Concepts method Type of Model Source Concept Activation Vector (CAV) TF Paper Testing CAV (TCAV) TF Paper (WIP) Robust TCAV (WIP) Automatic Concept Extraction (ACE) Feature Visualization (Paper) Type of Model Details Neurons TF Optimizes for specific neurons Layer TF Optimizes for specific layers Channel TF Optimizes for specific channels Direction TF Optimizes for specific vector Fourrier Preconditioning TF Optimize in Fourier basis (see preconditioning ) Objective combination TF Allows to combine objectives methods with TF need a Tensorflow model.","title":"\ud83d\udce6 What's Included"},{"location":"#contributing","text":"Feel free to propose your ideas or come and contribute with us on the Xplique toolbox! We have a specific document where we describe in a simple way how to make your first pull request: just here .","title":"\ud83d\udc4d Contributing"},{"location":"#see-also","text":"This library is one approach of many to explain your model. We don't expect it to be the perfect solution; we create it to explore one point in the space of possibilities. Other tools to explain your model include: Lucid the wonderful library specialized in feature visualization from OpenAI. Captum the Pytorch library for Interpretability research Tf-explain that implement multiples attribution methods and propose callbacks API for tensorflow. Alibi Explain for model inspection and interpretation SHAP a very popular library to compute local explanations using the classic Shapley values from game theory and their related extensions To learn more about Explainable AI in general, see: Interpretable Machine Learning an excellent book by Christophe Molnar. Interpretability Beyond Feature Attribution by Been Kim. A Roadmap for the Rigorous Science of Interpretability by Finale Doshi-Velez. DEEL White paper a summary of the DEEL team on the challenges of certifiable AI and the role of explainability for this purpose","title":"\ud83d\udc40 See Also"},{"location":"#acknowledgments","text":"This project received funding from the French \u201dInvesting for the Future \u2013 PIA3\u201d program within the Artificial and Natural Intelligence Toulouse Institute (ANITI). The authors gratefully acknowledge the support of the DEEL project.","title":"\ud83d\ude4f Acknowledgments"},{"location":"#creator","text":"This library was started as a side-project by Thomas FEL who is currently a graduate student at the Artificial and Natural Intelligence Toulouse Institute under the direction of Thomas SERRE . His thesis work focuses on explainability for deep neural networks. He then received help from some members of the DEEL team to enhance the library namely from Justin PLAKOO and Lucas HERVIER .","title":"\ud83d\udc68\u200d\ud83c\udf93 Creator"},{"location":"#license","text":"The package is released under MIT license .","title":"\ud83d\udcdd License"},{"location":"callable/","text":"\ud83d\udcde Callable or Models handle by BlackBox Attribution methods \u00b6 The model can be something else than a tf.keras.Model if it respects one of the following condition: - model(inputs: np.ndarray) return either a np.ndarray or a tf.Tensor of shape (N, L) (N, L) where N N is the number of samples and L L the number of targets - The model has a scikit-learn API and has a predict_proba function - The model is a xgboost.XGBModel from the XGBoost python library - The model is a TF Lite model . Note this feature is experimental. On the other hand, a PyTorch model can be used with method having Callable as type of model. In order to makes it work you should write a wrapper as follow: class TemplateTorchWrapper ( nn . Module ): def __init__ ( self , torch_model ): super ( TemplateTorchWrapper , self ) . __init__ () self . model = torch_model def __call__ ( self , inputs ): # transform your numpy inputs to torch torch_inputs = self . _transform_np_inputs ( inputs ) # mak predictions with torch . no_grad (): outputs = self . model ( torch_inputs ) # convert to numpy outputs = outputs . detach () . numpy () # convert to tf.Tensor outputs = tf . cast ( outputs , tf . float32 ) return outputs def _transform_np_inputs ( self , np_inputs ): # include in this function all transformation # needed for your torch model to work, here # for example we swap from channels last to # channels first np_inputs = np . swapaxes ( np_inputs , - 1 , 1 ) torch_inputs = torch . Tensor ( np_inputs ) return torch_inputs wrapped_model = TemplateTorchWrapper ( torch_model ) explanations = explainer . explain ( images , labels ) As a matter of fact, if the instance of your model doesn't belong to [ tf.keras.Model , tf.lite.Interpreter , sklearn.base.BaseEstimator , xgboost.XGBModel ] when the explainer will need to make inference the following will happen: # inputs are automatically transform to tf.Tensor when using an explainer pred = model ( inputs . numpy ()) pred = tf . cast ( pred , dtype = tf . float32 ) scores = tf . reduce_sum ( pred * targets , axis =- 1 ) Knowing that, you are free to wrap your model to make it work with our API!","title":"Callable"},{"location":"callable/#callable-or-models-handle-by-blackbox-attribution-methods","text":"The model can be something else than a tf.keras.Model if it respects one of the following condition: - model(inputs: np.ndarray) return either a np.ndarray or a tf.Tensor of shape (N, L) (N, L) where N N is the number of samples and L L the number of targets - The model has a scikit-learn API and has a predict_proba function - The model is a xgboost.XGBModel from the XGBoost python library - The model is a TF Lite model . Note this feature is experimental. On the other hand, a PyTorch model can be used with method having Callable as type of model. In order to makes it work you should write a wrapper as follow: class TemplateTorchWrapper ( nn . Module ): def __init__ ( self , torch_model ): super ( TemplateTorchWrapper , self ) . __init__ () self . model = torch_model def __call__ ( self , inputs ): # transform your numpy inputs to torch torch_inputs = self . _transform_np_inputs ( inputs ) # mak predictions with torch . no_grad (): outputs = self . model ( torch_inputs ) # convert to numpy outputs = outputs . detach () . numpy () # convert to tf.Tensor outputs = tf . cast ( outputs , tf . float32 ) return outputs def _transform_np_inputs ( self , np_inputs ): # include in this function all transformation # needed for your torch model to work, here # for example we swap from channels last to # channels first np_inputs = np . swapaxes ( np_inputs , - 1 , 1 ) torch_inputs = torch . Tensor ( np_inputs ) return torch_inputs wrapped_model = TemplateTorchWrapper ( torch_model ) explanations = explainer . explain ( images , labels ) As a matter of fact, if the instance of your model doesn't belong to [ tf.keras.Model , tf.lite.Interpreter , sklearn.base.BaseEstimator , xgboost.XGBModel ] when the explainer will need to make inference the following will happen: # inputs are automatically transform to tf.Tensor when using an explainer pred = model ( inputs . numpy ()) pred = tf . cast ( pred , dtype = tf . float32 ) scores = tf . reduce_sum ( pred * targets , axis =- 1 ) Knowing that, you are free to wrap your model to make it work with our API!","title":"\ud83d\udcde Callable or Models handle by BlackBox Attribution methods"},{"location":"contributing/","text":"Contributing \ud83d\ude4f \u00b6 Thanks for taking the time to contribute! \ud83c\udf89\ud83d\udc4d From opening a bug report to creating a pull request: every contribution is appreciated and welcome. If you're planning to implement a new feature or change the api please create an issue first. This way we can ensure that your precious work is not in vain. Setup with make \u2699\ufe0f \u00b6 Clone the repo git clone https://github.com/deel-ai/xplique.git . Go to your freshly downloaded repo cd xplique Create a virtual environment and install the necessary dependencies for development make prepare-dev && source xplique_dev_env/bin/activate . You are ready to install the library pip install -e . or run the test suite make test . Welcome to the team \ud83d\udd25\ud83d\ude80 ! Setup without make \u2699\ufe0f \u00b6 Clone the repo git clone https://github.com/deel-ai/xplique.git . Go to your freshly downloaded repo cd xplique Install virtualenv with pip : pip install virtualenv Or with conda : conda install -c conda-forge virtualenv Create a new virtual environment venv xplique_dev_env Activate your new environment . xplique_dev_env/bin/activate Depending on your machine, this operation might be slightly different. For instance, on Windows you should probably do (with cmd.exe): ~/xplique> path\\to\\xplique_dev_env\\bin\\activate.bat Or with Powershell: PS ~/xplique> path\\to\\xplique_dev_env\\bin\\Activate.ps1 Anyway, if you suceed you should see your virtual environment name in front of any other command: (xplique_dev_env) :~/xplique$ You can now install all necessary packages, with pip: pip install -r requirements.txt pip install -r requirements_dev.txt Or with conda: conda install --file requirements.txt conda install --file requirements_dev.txt You are ready to install the library: pip install -e Or run the test suite: tox You are now ready to code and to be part of the team \ud83d\udd25\ud83d\ude80 ! Tests \u2705 \u00b6 A pretty fair question would be to know what is make test doing ? It is actually just a command which activate your virtual environment and launch the tox command. So basically, if you do not succeed to use make just activate your virtual env and do tox ! tox on the otherhand will do the following: - run pytest on the tests folder with python 3.6, python 3.7 and python 3.8 Note: If you do not have those 3 interpreters the tests would be only performs with your current interpreter - run pylint on the xplique main files, also with python 3.6, python 3.7 and python 3.8 Note: It is possible that pylint throw false-positive errors. If the linting test failed please check first pylint output to point out the reasons. Please, make sure you run all the tests at least once before opening a pull request. A word toward Pylint for those that don't know it: Pylint is a Python static code analysis tool which looks for programming errors, helps enforcing a coding standard, sniffs for code smells and offers simple refactoring suggestions. Basically, it will check that your code follow a certain number of convention. Any Pull Request will go through a Github workflow ensuring that your code respect the Pylint conventions (most of them at least). Submitting Changes \ud83d\udd03 \u00b6 After getting some feedback, push to your fork and submit a pull request. We may suggest some changes or improvements or alternatives, but for small changes your pull request should be accepted quickly. Something that will increase the chance that your pull request is accepted: Write tests and ensure that the existing ones pass. If make test is succesful, you have fair chances to pass the CI workflows (linting and test) Follow the existing coding style. Write a good commit message (we follow a lowercase convention). For a major fix/feature make sure your PR has an issue and if it doesn't, please create one. This would help discussion with the community, and polishing ideas in case of a new feature. Documentation \ud83d\udcda \u00b6 Xplique is a small library but documentation is often a huge time sink for users. That's why we greatly appreciate any time spent fixing typos or clarifying sections in the documentation. To setup a local live-server to update the documentation: make serve-doc or activate your virtual env and: CUDA_VISIBLE_DEVICES=-1 mkdocs serve","title":"Contributing"},{"location":"contributing/#contributing","text":"Thanks for taking the time to contribute! \ud83c\udf89\ud83d\udc4d From opening a bug report to creating a pull request: every contribution is appreciated and welcome. If you're planning to implement a new feature or change the api please create an issue first. This way we can ensure that your precious work is not in vain.","title":"Contributing \ud83d\ude4f"},{"location":"contributing/#setup-with-make","text":"Clone the repo git clone https://github.com/deel-ai/xplique.git . Go to your freshly downloaded repo cd xplique Create a virtual environment and install the necessary dependencies for development make prepare-dev && source xplique_dev_env/bin/activate . You are ready to install the library pip install -e . or run the test suite make test . Welcome to the team \ud83d\udd25\ud83d\ude80 !","title":"Setup with make \u2699\ufe0f"},{"location":"contributing/#setup-without-make","text":"Clone the repo git clone https://github.com/deel-ai/xplique.git . Go to your freshly downloaded repo cd xplique Install virtualenv with pip : pip install virtualenv Or with conda : conda install -c conda-forge virtualenv Create a new virtual environment venv xplique_dev_env Activate your new environment . xplique_dev_env/bin/activate Depending on your machine, this operation might be slightly different. For instance, on Windows you should probably do (with cmd.exe): ~/xplique> path\\to\\xplique_dev_env\\bin\\activate.bat Or with Powershell: PS ~/xplique> path\\to\\xplique_dev_env\\bin\\Activate.ps1 Anyway, if you suceed you should see your virtual environment name in front of any other command: (xplique_dev_env) :~/xplique$ You can now install all necessary packages, with pip: pip install -r requirements.txt pip install -r requirements_dev.txt Or with conda: conda install --file requirements.txt conda install --file requirements_dev.txt You are ready to install the library: pip install -e Or run the test suite: tox You are now ready to code and to be part of the team \ud83d\udd25\ud83d\ude80 !","title":"Setup without make \u2699\ufe0f"},{"location":"contributing/#tests","text":"A pretty fair question would be to know what is make test doing ? It is actually just a command which activate your virtual environment and launch the tox command. So basically, if you do not succeed to use make just activate your virtual env and do tox ! tox on the otherhand will do the following: - run pytest on the tests folder with python 3.6, python 3.7 and python 3.8 Note: If you do not have those 3 interpreters the tests would be only performs with your current interpreter - run pylint on the xplique main files, also with python 3.6, python 3.7 and python 3.8 Note: It is possible that pylint throw false-positive errors. If the linting test failed please check first pylint output to point out the reasons. Please, make sure you run all the tests at least once before opening a pull request. A word toward Pylint for those that don't know it: Pylint is a Python static code analysis tool which looks for programming errors, helps enforcing a coding standard, sniffs for code smells and offers simple refactoring suggestions. Basically, it will check that your code follow a certain number of convention. Any Pull Request will go through a Github workflow ensuring that your code respect the Pylint conventions (most of them at least).","title":"Tests \u2705"},{"location":"contributing/#submitting-changes","text":"After getting some feedback, push to your fork and submit a pull request. We may suggest some changes or improvements or alternatives, but for small changes your pull request should be accepted quickly. Something that will increase the chance that your pull request is accepted: Write tests and ensure that the existing ones pass. If make test is succesful, you have fair chances to pass the CI workflows (linting and test) Follow the existing coding style. Write a good commit message (we follow a lowercase convention). For a major fix/feature make sure your PR has an issue and if it doesn't, please create one. This would help discussion with the community, and polishing ideas in case of a new feature.","title":"Submitting Changes \ud83d\udd03"},{"location":"contributing/#documentation","text":"Xplique is a small library but documentation is often a huge time sink for users. That's why we greatly appreciate any time spent fixing typos or clarifying sections in the documentation. To setup a local live-server to update the documentation: make serve-doc or activate your virtual env and: CUDA_VISIBLE_DEVICES=-1 mkdocs serve","title":"Documentation \ud83d\udcda"},{"location":"tutorials/","text":"Tutorials: Notebooks \ud83d\udcd4 \u00b6 We propose here several tutorials to discover the different functionnalities that the library has to offer. We decided to host those tutorials on Google Colab mainly because you will be able to play the notebooks with a GPU which should greatly improve your User eXperience. Here is the lists of the availables tutorial for now: Getting Started \u00b6 Tutorial Name Notebook Getting Started Tabular data and Regression Metrics Concept Activation Vectors Feature Visualization Attributions \u00b6 Category Tutorial Name Notebook BlackBox Rise BlackBox Occlusion BlackBox Lime BlackBox KernelShap WhiteBox Saliency WhiteBox DeconvNet WhiteBox GradientInput WhiteBox GuidedBackpropagation WhiteBox GradCAM WhiteBox GradCAM++ WhiteBox IntegratedGradients WhiteBox SmoothGrad WhiteBox SquareGrad WhiteBox VarGrad Tabular Data Regression Tabular Data Metrics \u00b6 Category Tutorial Name Notebook Fidelity MuFidelity Fidelity Insertion Fidelity Deletion Stability AverageStability (WIP) Concepts extraction \u00b6 WIP Features Visualizations \u00b6 WIP","title":"Tutorials: Notebooks \ud83d\udcd4"},{"location":"tutorials/#tutorials-notebooks","text":"We propose here several tutorials to discover the different functionnalities that the library has to offer. We decided to host those tutorials on Google Colab mainly because you will be able to play the notebooks with a GPU which should greatly improve your User eXperience. Here is the lists of the availables tutorial for now:","title":"Tutorials: Notebooks \ud83d\udcd4"},{"location":"tutorials/#getting-started","text":"Tutorial Name Notebook Getting Started Tabular data and Regression Metrics Concept Activation Vectors Feature Visualization","title":"Getting Started"},{"location":"tutorials/#attributions","text":"Category Tutorial Name Notebook BlackBox Rise BlackBox Occlusion BlackBox Lime BlackBox KernelShap WhiteBox Saliency WhiteBox DeconvNet WhiteBox GradientInput WhiteBox GuidedBackpropagation WhiteBox GradCAM WhiteBox GradCAM++ WhiteBox IntegratedGradients WhiteBox SmoothGrad WhiteBox SquareGrad WhiteBox VarGrad Tabular Data Regression Tabular Data","title":"Attributions"},{"location":"tutorials/#metrics","text":"Category Tutorial Name Notebook Fidelity MuFidelity Fidelity Insertion Fidelity Deletion Stability AverageStability (WIP)","title":"Metrics"},{"location":"tutorials/#concepts-extraction","text":"WIP","title":"Concepts extraction"},{"location":"tutorials/#features-visualizations","text":"WIP","title":"Features Visualizations"},{"location":"api/avg_stability/","text":"Average Stability \u00b6 Average Stability is a Stability metric measuring how similar are explanations of similar inputs. Quote [...] We want to ensure that, if inputs are near each other and their model outputs are similar, then their explanations should be close to each other. -- Evaluating and Aggregating Feature-based Model Explanations (2020) 1 Formally, given a predictor f f , an explanation function g g , a point x x , a radius r r and a two distance metric: \\rho \\rho over the inputs and D D over the explanations, the AverageStability is defined as: S = \\underset{z : \\rho(x, z) \\leq r}{\\int} D(g(f, x), g(f, z))\\ dz S = \\underset{z : \\rho(x, z) \\leq r}{\\int} D(g(f, x), g(f, z))\\ dz The better the method, the smaller the score. Example \u00b6 from xplique.metrics import AverageStability from xplique.attributions import Saliency # load images, labels and model # ... explainer = Saliency ( model ) metric = AverageStability ( model , inputs , labels ) score = metric . evaluate ( explainer ) AverageStability \u00b6 Used to compute the average sensitivity metric (or stability). This metric ensure that close inputs with similar predictions yields similar explanations. For each inputs we randomly sample noise to add to the inputs and compute the explanation for the noisy inputs. We then get the average distance between the original explanations and the noisy explanations. __init__( self , model : Callable , inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] , targets : Union[tf.Tensor, numpy.ndarray, None] = None , batch_size : Union[int, None] = 64 , radius : float = 0.1 , distance : str = 'l2' , nb_samples : int = 200) \u00b6 Parameters model : Callable Model used for computing metric. inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] Input samples under study. targets : Union[tf.Tensor, numpy.ndarray, None] = None One-hot encoded labels or regression target (e.g {+1, -1}), one for each sample. batch_size : Union[int, None] = 64 Number of samples to explain at once, if None compute all at once. radius : float = 0.1 Radius defining the neighborhood of the inputs with respect to l1 distance. distance : str = 'l2' Distance metric between the explanations. nb_samples : int = 200 Number of different neighbors points to try on each input to measure the stability. evaluate( self , explainer : Callable) -> float \u00b6 Evaluate the fidelity score. Parameters explainer : Callable Explainer or Explanations associated to each inputs. Return stability_score : float Average distance between the explanations Evaluating and Aggregating Feature-based Model Explanations (2020) \u21a9","title":"AverageStability"},{"location":"api/avg_stability/#average-stability","text":"Average Stability is a Stability metric measuring how similar are explanations of similar inputs. Quote [...] We want to ensure that, if inputs are near each other and their model outputs are similar, then their explanations should be close to each other. -- Evaluating and Aggregating Feature-based Model Explanations (2020) 1 Formally, given a predictor f f , an explanation function g g , a point x x , a radius r r and a two distance metric: \\rho \\rho over the inputs and D D over the explanations, the AverageStability is defined as: S = \\underset{z : \\rho(x, z) \\leq r}{\\int} D(g(f, x), g(f, z))\\ dz S = \\underset{z : \\rho(x, z) \\leq r}{\\int} D(g(f, x), g(f, z))\\ dz The better the method, the smaller the score.","title":"Average Stability"},{"location":"api/avg_stability/#example","text":"from xplique.metrics import AverageStability from xplique.attributions import Saliency # load images, labels and model # ... explainer = Saliency ( model ) metric = AverageStability ( model , inputs , labels ) score = metric . evaluate ( explainer )","title":"Example"},{"location":"api/avg_stability/#AverageStability","text":"Used to compute the average sensitivity metric (or stability). This metric ensure that close inputs with similar predictions yields similar explanations. For each inputs we randomly sample noise to add to the inputs and compute the explanation for the noisy inputs. We then get the average distance between the original explanations and the noisy explanations.","title":"AverageStability"},{"location":"api/cav/","text":"CAV \u00b6 CAV or Concept Activation Vector represent a high-level concept as a vector that indicate the direction to take (for activations of a layer) to maximise this concept. Quote [...] CAV for a concept is simply a vector in the direction of the values (e.g., activations) of that concept\u2019s set of examples\u2026 we derive CAVs by training a linear classifier between a concept\u2019s examples and random counter examples and then taking the vector orthogonal to the decision boundary. -- Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV) (2018). 1 For a layer f_l f_l of a model, we seek the linear classifier v_l \\in \\mathbb{R}^d v_l \\in \\mathbb{R}^d that separate the activations of the positive examples \\{ f_l(x) : x \\in \\mathcal{P} \\} \\{ f_l(x) : x \\in \\mathcal{P} \\} , and the activations of the random/negative examples \\{ f_l(x) : x \\in \\mathcal{R} \\} \\{ f_l(x) : x \\in \\mathcal{R} \\} . Example \u00b6 from xplique.concepts import Cav cav_renderer = Cav ( model , 'mixed4d' , classifier = 'SGD' , test_fraction = 0.1 ) cav = cav_renderer ( positive_examples , random_examples ) Cav \u00b6 Used to compute the Concept Activation Vector, which is a vector in the direction of the activations of that concept\u2019s set of examples. __init__( self , model : tf.keras.Model , target_layer : Union[str, int] , classifier : Union[str, Callable] = 'SGD' , test_fraction : float = 0.2 , batch_size : int = 64 , verbose : bool = False) \u00b6 Parameters model : tf.keras.Model Model to extract concept from. target_layer : Union[str, int] Index of the target layer or name of the layer. classifier : 'SGD' or 'SVC' or Sklearn model, optional Default implementation use SGD with hinge classifier (linear SVM), SVC use libsvm but the computation time is longer. test_fraction : float = 0.2 Fraction of the dataset used for test batch_size : int = 64 Batch size during the activations extraction verbose : bool = False If true, display information while training the classifier fit( self , positive_dataset : tf.Tensor , negative_dataset : tf.Tensor) -> tf.Tensor \u00b6 Compute and return the Concept Activation Vector (CAV) associated to the dataset and the layer targeted. Parameters positive_dataset : tf.Tensor Dataset of positive samples : samples containing the concept. negative_dataset : tf.Tensor Dataset of negative samples : samples without the concept Return cav : tf.Tensor Vector of the same shape as the layer output Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV) (2018). \u21a9","title":"Cav"},{"location":"api/cav/#cav","text":"CAV or Concept Activation Vector represent a high-level concept as a vector that indicate the direction to take (for activations of a layer) to maximise this concept. Quote [...] CAV for a concept is simply a vector in the direction of the values (e.g., activations) of that concept\u2019s set of examples\u2026 we derive CAVs by training a linear classifier between a concept\u2019s examples and random counter examples and then taking the vector orthogonal to the decision boundary. -- Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV) (2018). 1 For a layer f_l f_l of a model, we seek the linear classifier v_l \\in \\mathbb{R}^d v_l \\in \\mathbb{R}^d that separate the activations of the positive examples \\{ f_l(x) : x \\in \\mathcal{P} \\} \\{ f_l(x) : x \\in \\mathcal{P} \\} , and the activations of the random/negative examples \\{ f_l(x) : x \\in \\mathcal{R} \\} \\{ f_l(x) : x \\in \\mathcal{R} \\} .","title":"CAV"},{"location":"api/cav/#example","text":"from xplique.concepts import Cav cav_renderer = Cav ( model , 'mixed4d' , classifier = 'SGD' , test_fraction = 0.1 ) cav = cav_renderer ( positive_examples , random_examples )","title":"Example"},{"location":"api/cav/#Cav","text":"Used to compute the Concept Activation Vector, which is a vector in the direction of the activations of that concept\u2019s set of examples.","title":"Cav"},{"location":"api/deconvnet/","text":"Deconvnet \u00b6 Example \u00b6 from xplique.attributions import DeconvNet # load images, labels and model # ... method = DeconvNet ( model ) explanations = method . explain ( images , labels ) DeconvNet \u00b6 Used to compute the DeconvNet method, which modifies the classic Saliency procedure on ReLU's non linearities, allowing only the positive gradients (even from negative inputs) to pass through. __init__( self , model : tf.keras.Model , output_layer : Union[str, int, None] = -1 , batch_size : Union[int, None] = 32) \u00b6 Parameters model : tf.keras.Model Model used for computing explanations. output_layer : Union[str, int, None] = -1 Layer to target for the output (e.g logits or after softmax), if int, will be be interpreted as layer index, if string will look for the layer name. Default to the last layer, it is recommended to use the layer before Softmax. batch_size : Union[int, None] = 32 Number of samples to explain at once, if None compute all at once. explain( self , inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] , targets : Union[tf.Tensor, numpy.ndarray, None] = None) -> tf.Tensor \u00b6 Compute DeconvNet for a batch of samples. Accept Tensor, numpy array or tf.data.Dataset (in that case targets is None) Parameters inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] Input samples to be explained. targets : Union[tf.Tensor, numpy.ndarray, None] = None One-hot encoded labels or regression target (e.g {+1, -1}), one for each sample. Return explanations : tf.Tensor Deconv maps.","title":"DeconvNet"},{"location":"api/deconvnet/#deconvnet","text":"","title":"Deconvnet"},{"location":"api/deconvnet/#example","text":"from xplique.attributions import DeconvNet # load images, labels and model # ... method = DeconvNet ( model ) explanations = method . explain ( images , labels )","title":"Example"},{"location":"api/deconvnet/#DeconvNet","text":"Used to compute the DeconvNet method, which modifies the classic Saliency procedure on ReLU's non linearities, allowing only the positive gradients (even from negative inputs) to pass through.","title":"DeconvNet"},{"location":"api/deletion/","text":"Deletion \u00b6 The Deletion Fidelity metric measures how well a saliency-map\u2013based explanation of an image classification result localizes the important pixels. Quote The deletion metric measures the drop in the probability of a class as important pixels (given by the saliency map) are gradually removed from the image. A sharp drop, and thus a small area under the probability curve, are indicative of a good explanation. -- RISE: Randomized Input Sampling for Explanation of Black-box Models (2018) 1 The better the method, the smaller the score. Example \u00b6 from xplique.metrics import Deletion from xplique.attributions import Saliency # load images, targets and model # ... explainer = Saliency ( model ) explanations = explainer ( inputs , targets ) metric = Deletion ( model , inputs , targets ) score = metric . evaluate ( explanations ) Deletion \u00b6 The deletion metric measures the drop in the probability of a class as important pixels (given by the saliency map) are gradually removed from the image. A sharp drop, and thus a small area under the probability curve, are indicative of a good explanation. __init__( self , model : tf.keras.Model , inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] , targets : Union[tf.Tensor, numpy.ndarray, None] = None , batch_size : Union[int, None] = 64 , baseline_mode : Union[float, Callable] = 0.0 , steps : int = 10) \u00b6 Parameters model : tf.keras.Model Model used for computing metric. inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] Input samples under study. targets : Union[tf.Tensor, numpy.ndarray, None] = None One-hot encoded labels or regression target (e.g {+1, -1}), one for each sample. batch_size : Union[int, None] = 64 Number of samples to explain at once, if None compute all at once. baseline_mode : Union[float, Callable] = 0.0 Value of the baseline state, will be called with the inputs if it is a function. steps : int = 10 Number of steps between the start and the end state. evaluate( self , explanations : Union[tf.Tensor, numpy.ndarray]) -> float \u00b6 Evaluate the causal score. Parameters explanations : Union[tf.Tensor, numpy.ndarray] Explanation for the inputs, labels to evaluate. Return causal_score : float Metric score, area over the deletion (lower is better) or insertion (higher is better) curve. RISE: Randomized Input Sampling for Explanation of Black-box Models (2018) \u21a9","title":"Deletion"},{"location":"api/deletion/#deletion","text":"The Deletion Fidelity metric measures how well a saliency-map\u2013based explanation of an image classification result localizes the important pixels. Quote The deletion metric measures the drop in the probability of a class as important pixels (given by the saliency map) are gradually removed from the image. A sharp drop, and thus a small area under the probability curve, are indicative of a good explanation. -- RISE: Randomized Input Sampling for Explanation of Black-box Models (2018) 1 The better the method, the smaller the score.","title":"Deletion"},{"location":"api/deletion/#example","text":"from xplique.metrics import Deletion from xplique.attributions import Saliency # load images, targets and model # ... explainer = Saliency ( model ) explanations = explainer ( inputs , targets ) metric = Deletion ( model , inputs , targets ) score = metric . evaluate ( explanations )","title":"Example"},{"location":"api/deletion/#Deletion","text":"The deletion metric measures the drop in the probability of a class as important pixels (given by the saliency map) are gradually removed from the image. A sharp drop, and thus a small area under the probability curve, are indicative of a good explanation.","title":"Deletion"},{"location":"api/feature_viz/","text":"Feature Visualization \u00b6 One of the specificities of neural networks is their differentiability. This characteristic allows us to compute gradients, either the gradient of a loss with respect to the parameters, or in the case we are interested in here, of a part of the network with respect to the input. This gradient then allows us to iteratively modify the input in order to maximize an objective such as a neuron, a channel or a combination of objectives. Quote If we want to understand individual features, we can search for examples where they have high values either for a neuron at an individual position, or for an entire channel. -- Feature Visualization -- How neural networks build up their understanding of images (2017) 1 More precisely, the explanation of a neuron n n denoted as \\phi^{(n)} \\phi^{(n)} is an input x* \\in \\mathcal{X} x* \\in \\mathcal{X} such that \\phi^{(n)} = \\underset{x}{arg\\ max}\\ f(x)^{(n)} - \\mathcal{R}(x) \\phi^{(n)} = \\underset{x}{arg\\ max}\\ f(x)^{(n)} - \\mathcal{R}(x) with f(x)^{(n)} f(x)^{(n)} the neuron score for an input x x and \\mathcal{R}(x) \\mathcal{R}(x) a regularization term. In practice it turns out that preconditioning the input in a decorrelated space such as the frequency domain allows to obtain more consistent results and to better formulate the regularization (e.g. by controlling the rate of high frequency and low frequency desired). Examples \u00b6 Optimize the ten logits of a neural network (we recommend to remove the softmax activation of your network). from xplique.features_visualizations import Objective from xplique.features_visualizations import optimize # load a model... # targeting the 10 logits of the layer 'logits' # we can also target a layer by its index, like -1 for the last layer logits_obj = Objective . neuron ( model , \"logits\" , list ( range ( 10 ))) images , obj_names = optimize ( logits_obj ) # 10 images, one for each logits Create a combination of multiple objectives and aggregate them from xplique.features_visualizations import Objective from xplique.features_visualizations import optimize # load a model... # target the first logits neuron logits_obj = Objective . neuron ( model , \"logits\" , 0 ) # target the third layer layer_obj = Objective . layer ( model , \"conv2d_1\" ) # target the second channel of another layer channel_obj = Objective . channel ( model , \"mixed4_2\" , 2 ) # combine the objective obj = logits_obj * 1.0 + layer_obj * 3.0 + channel_obj * ( - 5.0 ) images , obj_names = optimize ( logits_obj ) # 1 resulting image Objective \u00b6 Use to combine several sub-objectives into one. __init__( self , model : tf.keras.Model , layers : List[tf.keras.layers.Layer] , masks : List[tf.Tensor] , funcs : List[Callable] , multipliers : List[float] , names : List[str]) \u00b6 Parameters model : tf.keras.Model Model used for optimization. layers : List[tf.keras.layers.Layer] A list of the layers output for each sub-objectives. masks : List[tf.Tensor] A list of masks that will be applied on the targeted layer for each sub-objectives. funcs : List[Callable] A list of loss functions for each sub-objectives. multipliers : List[float] A list of multiplication factor for each sub-objectives names : List[str] A list of name for each sub-objectives channel( model : tf.keras.Model , layer : Union[str, int] , channel_ids : Union[int, List[int]] , multiplier : float = 1.0 , names : Union[str, List[str], None] = None) \u00b6 Util to build an objective to maximise a channel. Parameters model : tf.keras.Model Model used for optimization. layer : Union[str, int] Index or name of the targeted layer. channel_ids : Union[int, List[int]] Indexes of the channels to maximise. multiplier : float = 1.0 Multiplication factor of the objectives. names : Union[str, List[str], None] = None Names for each objectives. Return objective An objective containing a sub-objective for each channels. compile( self ) -> Tuple[tf.keras.Model, Callable, List[str], Tuple] \u00b6 Compile all the sub-objectives into one and return the objects for the optimisation process. Return model_reconfigured : Tuple[tf.keras.Model, Callable, List[str], Tuple] Model with the outputs needed for the optimization. objective_function : Tuple[tf.keras.Model, Callable, List[str], Tuple] Function to call that compute the loss for the objectives. names : Tuple[tf.keras.Model, Callable, List[str], Tuple] Names of each objectives. input_shape : Tuple[tf.keras.Model, Callable, List[str], Tuple] Shape of the input, one sample for each optimization. direction( model : tf.keras.Model , layer : Union[str, int] , vectors : Union[tf.Tensor, List[tf.Tensor]] , multiplier : float = 1.0 , cossim_pow : float = 2.0 , names : Union[str, List[str], None] = None) \u00b6 Util to build an objective to maximise a direction of a layer. Parameters model : tf.keras.Model Model used for optimization. layer : Union[str, int] Index or name of the targeted layer. vectors : Union[tf.Tensor, List[tf.Tensor]] Direction(s) to optimize. multiplier : float = 1.0 Multiplication factor of the objective. cossim_pow : float = 2.0 Power of the cosine similarity, higher value encourage the objective to care more about the angle of the activations. names : Union[str, List[str], None] = None A name for each objectives. Return objective An objective ready to be compiled layer( model : tf.keras.Model , layer : Union[str, int] , reducer : str = 'magnitude' , multiplier : float = 1.0 , name : Union[str, None] = None) \u00b6 Util to build an objective to maximise a layer. Parameters model : tf.keras.Model Model used for optimization. layer : Union[str, int] Index or name of the targeted layer. reducer : str = 'magnitude' Type of reduction to apply, 'mean' will optimize the mean value of the layer, 'magnitude' will optimize the mean of the absolute values. multiplier : float = 1.0 Multiplication factor of the objective. name : Union[str, None] = None A name for the objective. Return objective An objective ready to be compiled neuron( model : tf.keras.Model , layer : Union[str, int] , neurons_ids : Union[int, List[int]] , multiplier : float = 1.0 , names : Union[str, List[str], None] = None) \u00b6 Util to build an objective to maximise a neuron. Parameters model : tf.keras.Model Model used for optimization. layer : Union[str, int] Index or name of the targeted layer. neurons_ids : Union[int, List[int]] Indexes of the neurons to maximise. multiplier : float = 1.0 Multiplication factor of the objectives. names : Union[str, List[str], None] = None Names for each objectives. Return objective An objective containing a sub-objective for each neurons. Feature Visualization -- How neural networks build up their understanding of images (2017) \u21a9","title":"Feature visualization"},{"location":"api/feature_viz/#feature-visualization","text":"One of the specificities of neural networks is their differentiability. This characteristic allows us to compute gradients, either the gradient of a loss with respect to the parameters, or in the case we are interested in here, of a part of the network with respect to the input. This gradient then allows us to iteratively modify the input in order to maximize an objective such as a neuron, a channel or a combination of objectives. Quote If we want to understand individual features, we can search for examples where they have high values either for a neuron at an individual position, or for an entire channel. -- Feature Visualization -- How neural networks build up their understanding of images (2017) 1 More precisely, the explanation of a neuron n n denoted as \\phi^{(n)} \\phi^{(n)} is an input x* \\in \\mathcal{X} x* \\in \\mathcal{X} such that \\phi^{(n)} = \\underset{x}{arg\\ max}\\ f(x)^{(n)} - \\mathcal{R}(x) \\phi^{(n)} = \\underset{x}{arg\\ max}\\ f(x)^{(n)} - \\mathcal{R}(x) with f(x)^{(n)} f(x)^{(n)} the neuron score for an input x x and \\mathcal{R}(x) \\mathcal{R}(x) a regularization term. In practice it turns out that preconditioning the input in a decorrelated space such as the frequency domain allows to obtain more consistent results and to better formulate the regularization (e.g. by controlling the rate of high frequency and low frequency desired).","title":"Feature Visualization"},{"location":"api/feature_viz/#examples","text":"Optimize the ten logits of a neural network (we recommend to remove the softmax activation of your network). from xplique.features_visualizations import Objective from xplique.features_visualizations import optimize # load a model... # targeting the 10 logits of the layer 'logits' # we can also target a layer by its index, like -1 for the last layer logits_obj = Objective . neuron ( model , \"logits\" , list ( range ( 10 ))) images , obj_names = optimize ( logits_obj ) # 10 images, one for each logits Create a combination of multiple objectives and aggregate them from xplique.features_visualizations import Objective from xplique.features_visualizations import optimize # load a model... # target the first logits neuron logits_obj = Objective . neuron ( model , \"logits\" , 0 ) # target the third layer layer_obj = Objective . layer ( model , \"conv2d_1\" ) # target the second channel of another layer channel_obj = Objective . channel ( model , \"mixed4_2\" , 2 ) # combine the objective obj = logits_obj * 1.0 + layer_obj * 3.0 + channel_obj * ( - 5.0 ) images , obj_names = optimize ( logits_obj ) # 1 resulting image","title":"Examples"},{"location":"api/feature_viz/#Objective","text":"Use to combine several sub-objectives into one.","title":"Objective"},{"location":"api/grad_cam/","text":"Grad-CAM \u00b6 Grad-CAM is a technique for producing visual explanations that can be used on Convolutional Neural Netowrk (CNN) which uses both gradients and the feature maps of the last convolutional layer. Quote Grad-CAM uses the gradients of any target concept (say logits for \u201cdog\u201d or even a caption), flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. -- Visual Explanations from Deep Networks via Gradient-based Localization (2016). More precisely, to obtain the localization map for a class L_c L_c , we need to compute the weights \\alpha_k^c \\alpha_k^c associated to each of the feature map activation A^k A^k . As we use the last convolutionnal layer, k k will be the number of filters, Z Z is the number of pixels in each feature map (width \\cdot \\cdot height). \\begin{align} \\alpha_k^c = \\frac{1}{Z} \\sum_i\\sum_j \\frac{ \\partial{y^c}} {\\partial{A_{ij}^k} } \\\\ L^c = max(0, \\sum_k \\alpha_k^c A^k) \\end{align} \\begin{align} \\alpha_k^c = \\frac{1}{Z} \\sum_i\\sum_j \\frac{ \\partial{y^c}} {\\partial{A_{ij}^k} } \\\\ L^c = max(0, \\sum_k \\alpha_k^c A^k) \\end{align} Notice that the size of the explanation depends on the size (height, width) of the last feature map, so we have to interpolate in order to find the same dimensions as the input. Example \u00b6 from xplique.attributions import GradCAM # load images, labels and model # ... method = GradCAM ( model ) explanations = method . explain ( images , labels ) GradCAM \u00b6 Used to compute the Grad-CAM visualization method. __init__( self , model : tf.keras.Model , output_layer : Union[str, int, None] = -1 , batch_size : Union[int, None] = 32 , conv_layer : Union[str, int, None] = None) \u00b6 Parameters model : tf.keras.Model Model used for computing explanations. output_layer : Union[str, int, None] = -1 Layer to target for the output (e.g logits or after softmax), if int, will be be interpreted as layer index, if string will look for the layer name. Default to the last layer, it is recommended to use the layer before Softmax. batch_size : Union[int, None] = 32 Number of samples to explain at once, if None compute all at once. conv_layer : Union[str, int, None] = None Layer to target for Grad-CAM algorithm, if int, will be be interpreted as layer index, if string will look for the layer name. explain( self , inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] , targets : Union[tf.Tensor, numpy.ndarray, None] = None) -> tf.Tensor \u00b6 Compute and resize explanations to match inputs shape. Accept Tensor, numpy array or tf.data.Dataset (in that case targets is None) Parameters inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] Input samples to be explained. targets : Union[tf.Tensor, numpy.ndarray, None] = None One-hot encoded labels or regression target (e.g {+1, -1}), one for each sample. Return grad_cam : tf.Tensor Grad-CAM explanations, same shape as the inputs except for the channels. Visual Explanations from Deep Networks via Gradient-based Localization (2016). \u21a9","title":"Grad-CAM"},{"location":"api/grad_cam/#grad-cam","text":"Grad-CAM is a technique for producing visual explanations that can be used on Convolutional Neural Netowrk (CNN) which uses both gradients and the feature maps of the last convolutional layer. Quote Grad-CAM uses the gradients of any target concept (say logits for \u201cdog\u201d or even a caption), flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. -- Visual Explanations from Deep Networks via Gradient-based Localization (2016). More precisely, to obtain the localization map for a class L_c L_c , we need to compute the weights \\alpha_k^c \\alpha_k^c associated to each of the feature map activation A^k A^k . As we use the last convolutionnal layer, k k will be the number of filters, Z Z is the number of pixels in each feature map (width \\cdot \\cdot height). \\begin{align} \\alpha_k^c = \\frac{1}{Z} \\sum_i\\sum_j \\frac{ \\partial{y^c}} {\\partial{A_{ij}^k} } \\\\ L^c = max(0, \\sum_k \\alpha_k^c A^k) \\end{align} \\begin{align} \\alpha_k^c = \\frac{1}{Z} \\sum_i\\sum_j \\frac{ \\partial{y^c}} {\\partial{A_{ij}^k} } \\\\ L^c = max(0, \\sum_k \\alpha_k^c A^k) \\end{align} Notice that the size of the explanation depends on the size (height, width) of the last feature map, so we have to interpolate in order to find the same dimensions as the input.","title":"Grad-CAM"},{"location":"api/grad_cam/#example","text":"from xplique.attributions import GradCAM # load images, labels and model # ... method = GradCAM ( model ) explanations = method . explain ( images , labels )","title":"Example"},{"location":"api/grad_cam/#GradCAM","text":"Used to compute the Grad-CAM visualization method.","title":"GradCAM"},{"location":"api/grad_cam_pp/","text":"Grad-CAM++ \u00b6 Grad-CAM++ is a technique for producing visual explanations that can be used on Convolutional Neural Netowrk (CNN) which uses both gradients and the feature maps of the last convolutional layer. Example \u00b6 from xplique.attributions import GradCAMPP # load images, labels and model # ... method = GradCAMPP ( model , conv_layer =- 3 ) explanations = method . explain ( images , labels ) GradCAMPP \u00b6 Used to compute the Grad-CAM++ visualization method. __init__( self , model : tf.keras.Model , output_layer : Union[str, int, None] = -1 , batch_size : Union[int, None] = 32 , conv_layer : Union[str, int, None] = None) \u00b6 Parameters model : tf.keras.Model Model used for computing explanations. output_layer : Union[str, int, None] = -1 Layer to target for the output (e.g logits or after softmax), if int, will be be interpreted as layer index, if string will look for the layer name. Default to the last layer, it is recommended to use the layer before Softmax. batch_size : Union[int, None] = 32 Number of samples to explain at once, if None compute all at once. conv_layer : Union[str, int, None] = None Layer to target for Grad-CAM++ algorithm, if int, will be be interpreted as layer index, if string will look for the layer name. explain( self , inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] , targets : Union[tf.Tensor, numpy.ndarray, None] = None) -> tf.Tensor \u00b6 Compute and resize explanations to match inputs shape. Accept Tensor, numpy array or tf.data.Dataset (in that case targets is None) Parameters inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] Input samples to be explained. targets : Union[tf.Tensor, numpy.ndarray, None] = None One-hot encoded labels or regression target (e.g {+1, -1}), one for each sample. Return grad_cam : tf.Tensor Grad-CAM explanations, same shape as the inputs except for the channels. Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks (2017). \u21a9","title":"Grad-CAM++"},{"location":"api/grad_cam_pp/#grad-cam","text":"Grad-CAM++ is a technique for producing visual explanations that can be used on Convolutional Neural Netowrk (CNN) which uses both gradients and the feature maps of the last convolutional layer.","title":"Grad-CAM++"},{"location":"api/grad_cam_pp/#example","text":"from xplique.attributions import GradCAMPP # load images, labels and model # ... method = GradCAMPP ( model , conv_layer =- 3 ) explanations = method . explain ( images , labels )","title":"Example"},{"location":"api/grad_cam_pp/#GradCAMPP","text":"Used to compute the Grad-CAM++ visualization method.","title":"GradCAMPP"},{"location":"api/gradient_input/","text":"Gradient \\odot \\odot Input \u00b6 Gradient \\odot \\odot Input is a visualization techniques based on the gradient of a class score relative to the input, element-wise with the input. This method was introduced by Shrikumar et al., 2016 1 , in an old version of their DeepLIFT paper 2 . Quote Gradient inputs was at first proposed as a technique to improve the sharpness of the attribution maps. The attribution is computed taking the (signed) partial derivatives of the output with respect to the input and multiplying them with the input itself. -- Towards better understanding of the gradient-based attribution methods for Deep Neural Networks (2017) 3 A theoretical analysis conducted by Ancona et al, 2018 3 showed that Gradient \\odot \\odot Input is equivalent to \\epsilon \\epsilon -LRP and DeepLift methods under certain conditions: using a baseline of zero, and with all biases to zero. More precisely, the explanation \\phi_x \\phi_x for an input x x , for a given class c c is defined as \\phi_x = x \\odot \\frac{\\partial{S_c(x)}}{\\partial{x}} \\phi_x = x \\odot \\frac{\\partial{S_c(x)}}{\\partial{x}} with S_c S_c the unormalized class score (layer before softmax). Example \u00b6 from xplique.attributions import GradientInput # load images, labels and model # ... method = GradientInput ( model ) explanations = method . explain ( images , labels ) GradientInput \u00b6 Used to compute elementwise product between the saliency maps of Simonyan et al. and the input (Gradient x Input). __init__( self , model : tf.keras.Model , output_layer : Union[str, int, None] = None , batch_size : Union[int, None] = 64) \u00b6 Parameters model : tf.keras.Model Model used for computing explanations. output_layer : Union[str, int, None] = None Layer to target for the output (e.g logits or after softmax), if int, will be be interpreted as layer index, if string will look for the layer name. Default to the last layer, it is recommended to use the layer before Softmax. batch_size : Union[int, None] = 64 Number of samples to explain at once, if None compute all at once. explain( self , inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] , targets : Union[tf.Tensor, numpy.ndarray, None] = None) -> tf.Tensor \u00b6 Compute gradients x inputs for a batch of samples. Parameters inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] Input samples to be explained. targets : Union[tf.Tensor, numpy.ndarray, None] = None One-hot encoded labels or regression target (e.g {+1, -1}), one for each sample. Return explanations : tf.Tensor Gradients x Inputs, with the same shape as the inputs. Not Just a Black Box: Learning Important Features Through Propagating Activation Differences \u21a9 Learning Important Features Through Propagating Activation Differences \u21a9 Towards better understanding of gradient-based attribution methods for Deep Neural Networks \u21a9 \u21a9","title":"Gradient Input"},{"location":"api/gradient_input/#gradient-odotodot-input","text":"Gradient \\odot \\odot Input is a visualization techniques based on the gradient of a class score relative to the input, element-wise with the input. This method was introduced by Shrikumar et al., 2016 1 , in an old version of their DeepLIFT paper 2 . Quote Gradient inputs was at first proposed as a technique to improve the sharpness of the attribution maps. The attribution is computed taking the (signed) partial derivatives of the output with respect to the input and multiplying them with the input itself. -- Towards better understanding of the gradient-based attribution methods for Deep Neural Networks (2017) 3 A theoretical analysis conducted by Ancona et al, 2018 3 showed that Gradient \\odot \\odot Input is equivalent to \\epsilon \\epsilon -LRP and DeepLift methods under certain conditions: using a baseline of zero, and with all biases to zero. More precisely, the explanation \\phi_x \\phi_x for an input x x , for a given class c c is defined as \\phi_x = x \\odot \\frac{\\partial{S_c(x)}}{\\partial{x}} \\phi_x = x \\odot \\frac{\\partial{S_c(x)}}{\\partial{x}} with S_c S_c the unormalized class score (layer before softmax).","title":"Gradient \\odot\\odot Input"},{"location":"api/gradient_input/#example","text":"from xplique.attributions import GradientInput # load images, labels and model # ... method = GradientInput ( model ) explanations = method . explain ( images , labels )","title":"Example"},{"location":"api/gradient_input/#GradientInput","text":"Used to compute elementwise product between the saliency maps of Simonyan et al. and the input (Gradient x Input).","title":"GradientInput"},{"location":"api/guided_backpropagation/","text":"Guided Backpropagation \u00b6 Example \u00b6 from xplique.attributions import GuidedBackprop # load images, labels and model # ... method = GuidedBackprop ( model ) explanations = method . explain ( images , labels ) GuidedBackprop \u00b6 Used to compute the Guided Backpropagation, which modifies the classic Saliency procedure on ReLU's non linearities, allowing only the positive gradients from positive activations to pass through. __init__( self , model : tf.keras.Model , output_layer : Union[str, int, None] = -1 , batch_size : Union[int, None] = 32) \u00b6 Parameters model : tf.keras.Model Model used for computing explanations. output_layer : Union[str, int, None] = -1 Layer to target for the output (e.g logits or after softmax), if int, will be be interpreted as layer index, if string will look for the layer name. Default to the last layer, it is recommended to use the layer before Softmax. batch_size : Union[int, None] = 32 Number of samples to explain at once, if None compute all at once. explain( self , inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] , targets : Union[tf.Tensor, numpy.ndarray, None] = None) -> tf.Tensor \u00b6 Compute Guided Backpropagation for a batch of samples. Parameters inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] Input samples to be explained. targets : Union[tf.Tensor, numpy.ndarray, None] = None One-hot encoded labels or regression target (e.g {+1, -1}), one for each sample. Return explanations : tf.Tensor Guided Backpropagation maps.","title":"Guided Backprop"},{"location":"api/guided_backpropagation/#guided-backpropagation","text":"","title":"Guided Backpropagation"},{"location":"api/guided_backpropagation/#example","text":"from xplique.attributions import GuidedBackprop # load images, labels and model # ... method = GuidedBackprop ( model ) explanations = method . explain ( images , labels )","title":"Example"},{"location":"api/guided_backpropagation/#GuidedBackprop","text":"Used to compute the Guided Backpropagation, which modifies the classic Saliency procedure on ReLU's non linearities, allowing only the positive gradients from positive activations to pass through.","title":"GuidedBackprop"},{"location":"api/insertion/","text":"Deletion \u00b6 The Insertion Fidelity metric measures how well a saliency-map\u2013based explanation can find elements that are minimal for the predictions. Quote The insertion metric, on the other hand, captures the importance of the pixels in terms of their ability to synthesize an image and is measured by the rise in the probability of the class of interest as pixels are added according to the generated importance map. -- RISE: Randomized Input Sampling for Explanation of Black-box Models (2018) 1 The better the method, the higher the score. Example \u00b6 from xplique.metrics import Insertion from xplique.attributions import Saliency # load images, labels and model # ... explainer = Saliency ( model ) explanations = explainer ( inputs , labels ) metric = Insertion ( model , inputs , labels ) score = metric . evaluate ( explanations ) Insertion \u00b6 The insertion metric, on the other hand, captures the importance of the pixels in terms of their ability to synthesize an image and is measured by the rise in the probability of the class of interest as pixels are added according to the generated importance map. __init__( self , model : tf.keras.Model , inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] , targets : Union[tf.Tensor, numpy.ndarray, None] = None , batch_size : Union[int, None] = 64 , baseline_mode : Union[float, Callable] = 0.0 , steps : int = 10) \u00b6 Parameters model : tf.keras.Model Model used for computing metric. inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] Input samples under study. targets : Union[tf.Tensor, numpy.ndarray, None] = None One-hot encoded labels or regression target (e.g {+1, -1}), one for each sample. batch_size : Union[int, None] = 64 Number of samples to explain at once, if None compute all at once. baseline_mode : Union[float, Callable] = 0.0 Value of the baseline state, will be called with the inputs if it is a function. steps : int = 10 Number of steps between the start and the end state. evaluate( self , explanations : Union[tf.Tensor, numpy.ndarray]) -> float \u00b6 Evaluate the causal score. Parameters explanations : Union[tf.Tensor, numpy.ndarray] Explanation for the inputs, labels to evaluate. Return causal_score : float Metric score, area over the deletion (lower is better) or insertion (higher is better) curve. RISE: Randomized Input Sampling for Explanation of Black-box Models (2018) \u21a9","title":"Insertion"},{"location":"api/insertion/#deletion","text":"The Insertion Fidelity metric measures how well a saliency-map\u2013based explanation can find elements that are minimal for the predictions. Quote The insertion metric, on the other hand, captures the importance of the pixels in terms of their ability to synthesize an image and is measured by the rise in the probability of the class of interest as pixels are added according to the generated importance map. -- RISE: Randomized Input Sampling for Explanation of Black-box Models (2018) 1 The better the method, the higher the score.","title":"Deletion"},{"location":"api/insertion/#example","text":"from xplique.metrics import Insertion from xplique.attributions import Saliency # load images, labels and model # ... explainer = Saliency ( model ) explanations = explainer ( inputs , labels ) metric = Insertion ( model , inputs , labels ) score = metric . evaluate ( explanations )","title":"Example"},{"location":"api/insertion/#Insertion","text":"The insertion metric, on the other hand, captures the importance of the pixels in terms of their ability to synthesize an image and is measured by the rise in the probability of the class of interest as pixels are added according to the generated importance map.","title":"Insertion"},{"location":"api/integrated_gradients/","text":"Integrated Gradients \u00b6 Integrated Gradients is a visualization technique resulting of a theoretical search for an explanatory method that satisfies two axioms, Sensitivity and Implementation Invariance (Sundararajan et al 1 ). Quote We consider the straightline path (in R^n R^n ) from the baseline \\bar{x} \\bar{x} to the input x x , and compute the gradients at all points along the path. Integrated gradients are obtained by cumulating these gradients. -- Axiomatic Attribution for Deep Networks (2017) 1 Rather than calculating only the gradient relative to the image, the method consists of averaging the gradient values along the path from a baseline state to the current value. The baseline state is often set to zero, representing the complete absence of features. More precisely, with \\bar{x} \\bar{x} the baseline state, x x the image, c c the class of interest and S_c S_c the unormalized class score (layer before softmax). The Integrated Gradient is defined as IG(x) = (x - \\bar{x}) \\cdot \\int_0^1{ \\frac { \\partial{S_c(\\tilde{x})} } { \\partial{\\tilde{x}} } \\Big|_{ \\tilde{x} = \\bar{x} + \\alpha(x - \\bar{x}) } d\\alpha } IG(x) = (x - \\bar{x}) \\cdot \\int_0^1{ \\frac { \\partial{S_c(\\tilde{x})} } { \\partial{\\tilde{x}} } \\Big|_{ \\tilde{x} = \\bar{x} + \\alpha(x - \\bar{x}) } d\\alpha } In order to approximate from a finite number of steps, the implementation here use the Trapezoidal rule 3 and not a left-Riemann summation, which allows for more accurate results and improved performance. (see the paper below for a comparison of the methods 2 ). Example \u00b6 from xplique.attributions import IntegratedGradients # load images, labels and model # ... method = IntegratedGradients ( model , steps = 50 , baseline_value = 0.0 ) explanations = method . explain ( images , labels ) IntegratedGradients \u00b6 Used to compute the Integrated Gradients, by cumulating the gradients along a path from a baseline to the desired point. __init__( self , model : tf.keras.Model , output_layer : Union[str, int, None] = -1 , batch_size : Union[int, None] = 32 , steps : int = 50 , baseline_value : float = 0.0) \u00b6 Parameters model : tf.keras.Model Model used for computing explanations. output_layer : Union[str, int, None] = -1 Layer to target for the output (e.g logits or after softmax), if int, will be be interpreted as layer index, if string will look for the layer name. Default to the last layer, it is recommended to use the layer before Softmax. batch_size : Union[int, None] = 32 Number of samples to explain at once, if None compute all at once. steps : int = 50 Number of points to interpolate between the baseline and the desired point. baseline_value : float = 0.0 Scalar used to create the the baseline point. explain( self , inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] , targets : Union[tf.Tensor, numpy.ndarray, None] = None) -> tf.Tensor \u00b6 Compute Integrated Gradients for a batch of samples. Parameters inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] Input samples to be explained. targets : Union[tf.Tensor, numpy.ndarray, None] = None One-hot encoded labels or regression target (e.g {+1, -1}), one for each sample. Return explanations : tf.Tensor Integrated gradients, same shape as the inputs. Axiomatic Attribution for Deep Networks \u21a9 \u21a9 Computing Linear Restrictions of Neural Networks \u21a9 Trapezoidal rule \u21a9","title":"Integrated Gradient"},{"location":"api/integrated_gradients/#integrated-gradients","text":"Integrated Gradients is a visualization technique resulting of a theoretical search for an explanatory method that satisfies two axioms, Sensitivity and Implementation Invariance (Sundararajan et al 1 ). Quote We consider the straightline path (in R^n R^n ) from the baseline \\bar{x} \\bar{x} to the input x x , and compute the gradients at all points along the path. Integrated gradients are obtained by cumulating these gradients. -- Axiomatic Attribution for Deep Networks (2017) 1 Rather than calculating only the gradient relative to the image, the method consists of averaging the gradient values along the path from a baseline state to the current value. The baseline state is often set to zero, representing the complete absence of features. More precisely, with \\bar{x} \\bar{x} the baseline state, x x the image, c c the class of interest and S_c S_c the unormalized class score (layer before softmax). The Integrated Gradient is defined as IG(x) = (x - \\bar{x}) \\cdot \\int_0^1{ \\frac { \\partial{S_c(\\tilde{x})} } { \\partial{\\tilde{x}} } \\Big|_{ \\tilde{x} = \\bar{x} + \\alpha(x - \\bar{x}) } d\\alpha } IG(x) = (x - \\bar{x}) \\cdot \\int_0^1{ \\frac { \\partial{S_c(\\tilde{x})} } { \\partial{\\tilde{x}} } \\Big|_{ \\tilde{x} = \\bar{x} + \\alpha(x - \\bar{x}) } d\\alpha } In order to approximate from a finite number of steps, the implementation here use the Trapezoidal rule 3 and not a left-Riemann summation, which allows for more accurate results and improved performance. (see the paper below for a comparison of the methods 2 ).","title":"Integrated Gradients"},{"location":"api/integrated_gradients/#example","text":"from xplique.attributions import IntegratedGradients # load images, labels and model # ... method = IntegratedGradients ( model , steps = 50 , baseline_value = 0.0 ) explanations = method . explain ( images , labels )","title":"Example"},{"location":"api/integrated_gradients/#IntegratedGradients","text":"Used to compute the Integrated Gradients, by cumulating the gradients along a path from a baseline to the desired point.","title":"IntegratedGradients"},{"location":"api/kernel_shap/","text":"Kernel Shap \u00b6 By setting appropriately the pertubation function, the similarity kernel and the interpretable model in the LIME framework we can theoretically obtain the Shapley Values more efficiently. Therefore, KernelShap is a method based on LIME with specific attributes. Quote The exact computation of SHAP values is challenging. However, by combining insights from current additive feature attribution methods, we can approximate them. We describe two model-agnostic approximation methods, [...] and another that is novel (Kernel SHAP) -- A Unified Approach to Interpreting Model Predictions 1 Example \u00b6 from xplique.attributions import KernelShap # load images, labels and model # define a custom map_to_interpret_space function # ... method = KernelShap ( model , map_to_interpret_space = custom_map ) explanations = method . explain ( images , labels ) The choice of the map function will have a great deal toward the quality of explanation. By default, the map function use the quickshift segmentation of scikit-images KernelShap \u00b6 By setting appropriately the pertubation function, the similarity kernel and the interpretable model in the LIME framework we can theoretically obtain the Shapley Values more efficiently. Therefore, KernelShap is a method based on LIME with specific attributes. __init__( self , model : Callable , batch_size : int = 64 , map_to_interpret_space : Union[Callable, None] = None , nb_samples : int = 800 , ref_value : Union[numpy.ndarray, None] = None) \u00b6 explain( self , inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] , targets : Union[tf.Tensor, numpy.ndarray, None] = None) -> numpy.ndarray \u00b6 This method attributes the output of the model with given targets to the inputs of the model using the approach described above, training an interpretable model and returning a representation of the interpretable model. Parameters inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] Tensor or numpy array of shape (N, W (, H, C)) Input samples, with N number of samples, W (& H) the sample dimension(s) (and C the number of channels). targets : Union[tf.Tensor, numpy.ndarray, None] = None Tensor or numpy array of shape (N, L) One-hot encoded labels or regression target (e.g {+1, -1}), one for each input.s. Return explanations : numpy.ndarray Numpy array of shape: (N, W (, H)) Coefficients of the interpretable model. Those coefficients having the size of the interpretable space will be given the same value to coefficient which were grouped together (e.g belonging to the same super-pixel). A Unified Approach to Interpreting Model Predictions \u21a9","title":"KernelSHAP"},{"location":"api/kernel_shap/#kernel-shap","text":"By setting appropriately the pertubation function, the similarity kernel and the interpretable model in the LIME framework we can theoretically obtain the Shapley Values more efficiently. Therefore, KernelShap is a method based on LIME with specific attributes. Quote The exact computation of SHAP values is challenging. However, by combining insights from current additive feature attribution methods, we can approximate them. We describe two model-agnostic approximation methods, [...] and another that is novel (Kernel SHAP) -- A Unified Approach to Interpreting Model Predictions 1","title":"Kernel Shap"},{"location":"api/kernel_shap/#example","text":"from xplique.attributions import KernelShap # load images, labels and model # define a custom map_to_interpret_space function # ... method = KernelShap ( model , map_to_interpret_space = custom_map ) explanations = method . explain ( images , labels ) The choice of the map function will have a great deal toward the quality of explanation. By default, the map function use the quickshift segmentation of scikit-images","title":"Example"},{"location":"api/kernel_shap/#KernelShap","text":"By setting appropriately the pertubation function, the similarity kernel and the interpretable model in the LIME framework we can theoretically obtain the Shapley Values more efficiently. Therefore, KernelShap is a method based on LIME with specific attributes.","title":"KernelShap"},{"location":"api/lime/","text":"LIME \u00b6 The Lime method use an interpretable model to provide an explanation. More specifically, you map inputs ( x \\in R^d x \\in R^d ) to an interpretable space (e.g super-pixels) of size num_interpetable_features. From there you generate pertubed interpretable samples ( z' \\in \\{0,1\\}^{num\\_interpretable\\_samples} z' \\in \\{0,1\\}^{num\\_interpretable\\_samples} where 1 1 means we keep this specific interpretable feature). Once you have your interpretable samples you can map them back to their original space (the pertubed samples z \\in R^d z \\in R^d ) and obtain the label prediction of your model for each pertubed samples. In the Lime method you define a similarity kernel which compute the similarity between an input and its pertubed representations (either in the original input space or in the interpretable space): \\pi_x(z',z) \\pi_x(z',z) . Finally, you train an interpretable model per input, using interpretable samples along the corresponding pertubed labels and it will draw interpretable samples weighted by the similarity kernel. Thus, you will have an interpretable explanation (i.e in the interpretable space) which can be broadcasted afterwards to the original space considering the mapping you used. Quote The overall goal of LIME is to identify an interpretable model over the interpretable representation that is locally faithful to the classifier. -- \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier. 1 Example \u00b6 from xplique.attributions import Lime # load images, labels and model # define a custom map_to_interpret_space function # ... method = Lime ( model , map_to_interpret_space = custom_map ) explanations = method . explain ( images , labels ) The choice of the interpretable model and the map function will have a great deal toward the quality of explanation. By default, the map function use the quickshift segmentation of scikit-images Lime \u00b6 Used to compute the LIME method. __init__( self , model : Callable , batch_size : Union[int, None] = None , interpretable_model : Any = Ridge(alpha=2) , similarity_kernel : Union[Callable[[tf.Tensor, tf.Tensor, tf.Tensor], tf.Tensor], None] = None , pertub_func : Union[Callable[[Union[int, tf.Tensor], int], tf.Tensor], None] = None , map_to_interpret_space : Union[Callable[[tf.Tensor], tf.Tensor], None] = None , ref_value : Union[numpy.ndarray, None] = None , nb_samples : int = 150 , distance_mode : str = 'euclidean' , kernel_width : float = 45.0 , prob : float = 0.5) \u00b6 explain( self , inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] , targets : Union[tf.Tensor, numpy.ndarray, None] = None) -> numpy.ndarray \u00b6 This method attributes the output of the model with given targets to the inputs of the model using the approach described above, training an interpretable model and returning a representation of the interpretable model. Parameters inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] Tensor or numpy array of shape (N, W (, H, C)) Input samples, with N number of samples, W (& H) the sample dimension(s) (and C the number of channels). targets : Union[tf.Tensor, numpy.ndarray, None] = None Tensor or numpy array of shape (N, L) One-hot encoded labels or regression target (e.g {+1, -1}), one for each input.s. Return explanations : numpy.ndarray Numpy array of shape: (N, W (, H)) Coefficients of the interpretable model. Those coefficients having the size of the interpretable space will be given the same value to coefficient which were grouped together (e.g belonging to the same super-pixel). \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier. \u21a9","title":"Lime"},{"location":"api/lime/#lime","text":"The Lime method use an interpretable model to provide an explanation. More specifically, you map inputs ( x \\in R^d x \\in R^d ) to an interpretable space (e.g super-pixels) of size num_interpetable_features. From there you generate pertubed interpretable samples ( z' \\in \\{0,1\\}^{num\\_interpretable\\_samples} z' \\in \\{0,1\\}^{num\\_interpretable\\_samples} where 1 1 means we keep this specific interpretable feature). Once you have your interpretable samples you can map them back to their original space (the pertubed samples z \\in R^d z \\in R^d ) and obtain the label prediction of your model for each pertubed samples. In the Lime method you define a similarity kernel which compute the similarity between an input and its pertubed representations (either in the original input space or in the interpretable space): \\pi_x(z',z) \\pi_x(z',z) . Finally, you train an interpretable model per input, using interpretable samples along the corresponding pertubed labels and it will draw interpretable samples weighted by the similarity kernel. Thus, you will have an interpretable explanation (i.e in the interpretable space) which can be broadcasted afterwards to the original space considering the mapping you used. Quote The overall goal of LIME is to identify an interpretable model over the interpretable representation that is locally faithful to the classifier. -- \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier. 1","title":"LIME"},{"location":"api/lime/#example","text":"from xplique.attributions import Lime # load images, labels and model # define a custom map_to_interpret_space function # ... method = Lime ( model , map_to_interpret_space = custom_map ) explanations = method . explain ( images , labels ) The choice of the interpretable model and the map function will have a great deal toward the quality of explanation. By default, the map function use the quickshift segmentation of scikit-images","title":"Example"},{"location":"api/lime/#Lime","text":"Used to compute the LIME method.","title":"Lime"},{"location":"api/mu_fidelity/","text":"MuFidelity \u00b6 MuFidelity is a fidelity metric measuring the correlation between important variables defined by the explanation method and the decline in the model score when these variables are reset to a baseline state. Quote [...] when we set particular features x_s x_s to a baseline value x_0 x_0 the change in predictor\u2019s output should be proportional to the sum of attribution scores. -- Evaluating and Aggregating Feature-based Model Explanations (2020) 1 Formally, given a predictor f f , an explanation function g g , a point x \\in \\mathbb{R}^n x \\in \\mathbb{R}^n and a subset size k k the MuFidelity metric is defined as: \\mu F = \\underset{S \\subseteq \\{1, ..., d\\} \\\\ |S| = k}{Corr}( \\sum_{i \\in S} g(f, x)_i, f(x) - f(x_{[x_i = x_0 | i \\in S]})) \\mu F = \\underset{S \\subseteq \\{1, ..., d\\} \\\\ |S| = k}{Corr}( \\sum_{i \\in S} g(f, x)_i, f(x) - f(x_{[x_i = x_0 | i \\in S]})) The better the method, the higher the score. Example \u00b6 from xplique.metrics import MuFidelity from xplique.attributions import Saliency # load images, labels and model # ... explainer = Saliency ( model ) explanations = explainer ( inputs , lablels ) metric = MuFidelity ( model , inputs , labels ) score = metric . evaluate ( explainations ) MuFidelity \u00b6 Used to compute the fidelity correlation metric. This metric ensure there is a correlation between a random subset of pixels and their attribution score. For each random subset created, we set the pixels of the subset at a baseline state and obtain the prediction score. This metric measures the correlation between the drop in the score and the importance of the explanation. __init__( self , model : Callable , inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] , targets : Union[tf.Tensor, numpy.ndarray, None] = None , batch_size : Union[int, None] = 64 , grid_size : Union[int, None] = 9 , subset_percent : float = 0.2 , baseline_mode : Union[Callable, float] = 0.0 , nb_samples : int = 200) \u00b6 Parameters model : Callable Model used for computing metric. inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] Input samples under study. targets : Union[tf.Tensor, numpy.ndarray, None] = None One-hot encoded labels or regression target (e.g {+1, -1}), one for each sample. batch_size : Union[int, None] = 64 Number of samples to explain at once, if None compute all at once. grid_size : Union[int, None] = 9 If none, compute the original metric, else cut the image in (grid_size, grid_size) and each element of the subset will be a super pixel representing one element of the grid. You should use this when dealing with medium / large size images. subset_percent : float = 0.2 Percent of the image that will be set to baseline. baseline_mode : Union[Callable, float] = 0.0 Value of the baseline state, will be called with the a single input if it is a function. nb_samples : int = 200 Number of different subsets to try on each input to measure the correlation. evaluate( self , explanations : Union[tf.Tensor, numpy.ndarray]) -> float \u00b6 Evaluate the fidelity score. Parameters explanations : Union[tf.Tensor, numpy.ndarray] Explanation for the inputs, labels to evaluate. Return fidelity_score : float Metric score, average correlation between the drop in score when variables are set to a baseline state and the importance of these variables according to the explanations. Evaluating and Aggregating Feature-based Model Explanations (2020) \u21a9","title":"MuFidelity"},{"location":"api/mu_fidelity/#mufidelity","text":"MuFidelity is a fidelity metric measuring the correlation between important variables defined by the explanation method and the decline in the model score when these variables are reset to a baseline state. Quote [...] when we set particular features x_s x_s to a baseline value x_0 x_0 the change in predictor\u2019s output should be proportional to the sum of attribution scores. -- Evaluating and Aggregating Feature-based Model Explanations (2020) 1 Formally, given a predictor f f , an explanation function g g , a point x \\in \\mathbb{R}^n x \\in \\mathbb{R}^n and a subset size k k the MuFidelity metric is defined as: \\mu F = \\underset{S \\subseteq \\{1, ..., d\\} \\\\ |S| = k}{Corr}( \\sum_{i \\in S} g(f, x)_i, f(x) - f(x_{[x_i = x_0 | i \\in S]})) \\mu F = \\underset{S \\subseteq \\{1, ..., d\\} \\\\ |S| = k}{Corr}( \\sum_{i \\in S} g(f, x)_i, f(x) - f(x_{[x_i = x_0 | i \\in S]})) The better the method, the higher the score.","title":"MuFidelity"},{"location":"api/mu_fidelity/#example","text":"from xplique.metrics import MuFidelity from xplique.attributions import Saliency # load images, labels and model # ... explainer = Saliency ( model ) explanations = explainer ( inputs , lablels ) metric = MuFidelity ( model , inputs , labels ) score = metric . evaluate ( explainations )","title":"Example"},{"location":"api/mu_fidelity/#MuFidelity","text":"Used to compute the fidelity correlation metric. This metric ensure there is a correlation between a random subset of pixels and their attribution score. For each random subset created, we set the pixels of the subset at a baseline state and obtain the prediction score. This metric measures the correlation between the drop in the score and the importance of the explanation.","title":"MuFidelity"},{"location":"api/occlusion/","text":"Occlusion sensitivity \u00b6 The Occlusion sensitivity method sweep a patch that occludes pixels over the images, and use the variations of the model prediction to deduce critical areas. 1 Quote [...] this method, referred to as Occlusion, replacing one feature x_i x_i at the time with a baseline and measuring the effect of this perturbation on the target output. -- Towards better understanding of the gradient-based attribution methods for Deep Neural Networks (2017) 2 with S_c S_c the unormalized class score (layer before softmax) and \\bar{x} \\bar{x} a baseline, the Occlusion sensitivity map \\phi \\phi is defined as : \\phi_i = S_c(x) - S_c(x_{[x_i = \\bar{x}]}) \\phi_i = S_c(x) - S_c(x_{[x_i = \\bar{x}]}) Example \u00b6 from xplique.attributions import Occlusion # load images, labels and model # ... method = Occlusion ( model , patch_size = ( 10 , 10 ), patch_stride = ( 2 , 2 ), occlusion_value = 0.5 ) explanations = method . explain ( images , labels ) Occlusion \u00b6 Used to compute the Occlusion sensitivity method, sweep a patch that occludes pixels over the images and use the variations of the model prediction to deduce critical areas. __init__( self , model : Callable , batch_size : Union[int, None] = 32 , patch_size : Union[int, Tuple[int, int]] = 3 , patch_stride : Union[int, Tuple[int, int]] = 3 , occlusion_value : float = 0.5) \u00b6 Parameters model : Callable Model used for computing explanations. batch_size : Union[int, None] = 32 Number of masked samples to process at once, if None process all at once. patch_size : Union[int, Tuple[int, int]] = 3 Size of the patches to apply, if integer then assume an hypercube. patch_stride : Union[int, Tuple[int, int]] = 3 Stride between two patches, if integer then assume an hypercube. occlusion_value : float = 0.5 Value used as occlusion. explain( self , inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] , targets : Union[tf.Tensor, numpy.ndarray, None] = None) -> tf.Tensor \u00b6 Compute Occlusion sensitivity for a batch of samples. Parameters inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] Input samples to be explained. targets : Union[tf.Tensor, numpy.ndarray, None] = None One-hot encoded labels or regression target (e.g {+1, -1}), one for each sample. Return explanations : tf.Tensor Occlusion sensitivity, same shape as the inputs, except for the channels. Visualizing and Understanding Convolutional Networks (2014). \u21a9 Towards better understanding of gradient-based attribution methods for Deep Neural Networks \u21a9","title":"Occlusion sensitivity"},{"location":"api/occlusion/#occlusion-sensitivity","text":"The Occlusion sensitivity method sweep a patch that occludes pixels over the images, and use the variations of the model prediction to deduce critical areas. 1 Quote [...] this method, referred to as Occlusion, replacing one feature x_i x_i at the time with a baseline and measuring the effect of this perturbation on the target output. -- Towards better understanding of the gradient-based attribution methods for Deep Neural Networks (2017) 2 with S_c S_c the unormalized class score (layer before softmax) and \\bar{x} \\bar{x} a baseline, the Occlusion sensitivity map \\phi \\phi is defined as : \\phi_i = S_c(x) - S_c(x_{[x_i = \\bar{x}]}) \\phi_i = S_c(x) - S_c(x_{[x_i = \\bar{x}]})","title":"Occlusion sensitivity"},{"location":"api/occlusion/#example","text":"from xplique.attributions import Occlusion # load images, labels and model # ... method = Occlusion ( model , patch_size = ( 10 , 10 ), patch_stride = ( 2 , 2 ), occlusion_value = 0.5 ) explanations = method . explain ( images , labels )","title":"Example"},{"location":"api/occlusion/#Occlusion","text":"Used to compute the Occlusion sensitivity method, sweep a patch that occludes pixels over the images and use the variations of the model prediction to deduce critical areas.","title":"Occlusion"},{"location":"api/rise/","text":"RISE \u00b6 The RISE method consist of probing the model with randomly masked versions of the input image and obtaining the corresponding outputs to deduce critical areas. Quote [...] we estimate the importance of pixels by dimming them in random combinations, reducing their intensities down to zero. We model this by multiplying an image with a [0,1] valued mask. -- RISE: Randomized Input Sampling for Explanation of Black-box Models (2018) 1 with S_c S_c the class score after softmax , x x an input, and m \\sim \\mathcal{M} m \\sim \\mathcal{M} a mask (not binary) the RISE importance map \\phi \\phi is defined as : \\phi_i = \\frac{1}{\\mathbb{E}(\\mathcal{M}) N} \\sum_{i=0}^N S_c(x \\odot m_i) m_i \\phi_i = \\frac{1}{\\mathbb{E}(\\mathcal{M}) N} \\sum_{i=0}^N S_c(x \\odot m_i) m_i Example \u00b6 from xplique.attributions import Rise # load images, labels and model # ... method = Rise ( model , nb_samples = 4000 , grid_size = 7 , preservation_probability = 0.5 ) explanations = method . explain ( images , labels ) Rise \u00b6 Used to compute the RISE method, by probing the model with randomly masked versions of the input image and obtaining the corresponding outputs to deduce critical areas. __init__( self , model : Callable , batch_size : Union[int, None] = 32 , nb_samples : int = 4000 , grid_size : int = 7 , preservation_probability : float = 0.5) \u00b6 Parameters model : Callable Model used for computing explanations. batch_size : Union[int, None] = 32 Number of masked samples to explain at once, if None process all at once. nb_samples : int = 4000 Number of masks generated for Monte Carlo sampling. grid_size : int = 7 Size of the grid used to generate the scaled-down masks. Masks are then rescale to and cropped to input_size. preservation_probability : float = 0.5 Probability of preservation for each pixel (or the percentage of non-masked pixels in each masks), also the expectation value of the mask. explain( self , inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] , targets : Union[tf.Tensor, numpy.ndarray, None] = None) -> tf.Tensor \u00b6 Compute RISE for a batch of samples. Parameters inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] Input samples to be explained. targets : Union[tf.Tensor, numpy.ndarray, None] = None One-hot encoded labels or regression target (e.g {+1, -1}), one for each sample. Return explanations : tf.Tensor RISE maps, same shape as the inputs, except for the channels. RISE: Randomized Input Sampling for Explanation of Black-box Models (2018) \u21a9","title":"Rise"},{"location":"api/rise/#rise","text":"The RISE method consist of probing the model with randomly masked versions of the input image and obtaining the corresponding outputs to deduce critical areas. Quote [...] we estimate the importance of pixels by dimming them in random combinations, reducing their intensities down to zero. We model this by multiplying an image with a [0,1] valued mask. -- RISE: Randomized Input Sampling for Explanation of Black-box Models (2018) 1 with S_c S_c the class score after softmax , x x an input, and m \\sim \\mathcal{M} m \\sim \\mathcal{M} a mask (not binary) the RISE importance map \\phi \\phi is defined as : \\phi_i = \\frac{1}{\\mathbb{E}(\\mathcal{M}) N} \\sum_{i=0}^N S_c(x \\odot m_i) m_i \\phi_i = \\frac{1}{\\mathbb{E}(\\mathcal{M}) N} \\sum_{i=0}^N S_c(x \\odot m_i) m_i","title":"RISE"},{"location":"api/rise/#example","text":"from xplique.attributions import Rise # load images, labels and model # ... method = Rise ( model , nb_samples = 4000 , grid_size = 7 , preservation_probability = 0.5 ) explanations = method . explain ( images , labels )","title":"Example"},{"location":"api/rise/#Rise","text":"Used to compute the RISE method, by probing the model with randomly masked versions of the input image and obtaining the corresponding outputs to deduce critical areas.","title":"Rise"},{"location":"api/saliency/","text":"Saliency Maps \u00b6 Saliency is visualization techniques based on the gradient of a class score relative to the input. Quote An interpretation of computing the image-specific class saliency using the class score derivative is that the magnitude of the derivative indicates which pixels need to be changed the least to affect the class score the most. One can expect that such pixels correspond to the object location in the image. -- Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps (2013) 1 More precisely, the explanation \\phi_x \\phi_x for an input x x , for a given class c c is defined as \\phi_x = \\Big{\\|} \\frac{\\partial{S_c(x)}}{\\partial{x}} \\Big{\\|} \\phi_x = \\Big{\\|} \\frac{\\partial{S_c(x)}}{\\partial{x}} \\Big{\\|} with S_c S_c the unormalized class score (layer before softmax). Example \u00b6 from xplique.attributions import Saliency # load images, labels and model # ... method = Saliency ( model ) explanations = method . explain ( images , labels ) Saliency \u00b6 Used to compute the absolute gradient of the output relative to the input. __init__( self , model : tf.keras.Model , output_layer : Union[str, int, None] = None , batch_size : Union[int, None] = 64) \u00b6 Parameters model : tf.keras.Model Model used for computing explanations. output_layer : Union[str, int, None] = None Layer to target for the output (e.g logits or after softmax), if int, will be be interpreted as layer index, if string will look for the layer name. Default to the last layer, it is recommended to use the layer before Softmax. batch_size : Union[int, None] = 64 Number of samples to explain at once, if None compute all at once. explain( self , inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] , targets : Union[tf.Tensor, numpy.ndarray, None] = None) -> tf.Tensor \u00b6 Compute saliency maps for a batch of samples. Parameters inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] Input samples to be explained. targets : Union[tf.Tensor, numpy.ndarray, None] = None One-hot encoded labels or regression target (e.g {+1, -1}), one for each sample. Return explanations : tf.Tensor Saliency maps. Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps \u21a9","title":"Saliency"},{"location":"api/saliency/#saliency-maps","text":"Saliency is visualization techniques based on the gradient of a class score relative to the input. Quote An interpretation of computing the image-specific class saliency using the class score derivative is that the magnitude of the derivative indicates which pixels need to be changed the least to affect the class score the most. One can expect that such pixels correspond to the object location in the image. -- Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps (2013) 1 More precisely, the explanation \\phi_x \\phi_x for an input x x , for a given class c c is defined as \\phi_x = \\Big{\\|} \\frac{\\partial{S_c(x)}}{\\partial{x}} \\Big{\\|} \\phi_x = \\Big{\\|} \\frac{\\partial{S_c(x)}}{\\partial{x}} \\Big{\\|} with S_c S_c the unormalized class score (layer before softmax).","title":"Saliency Maps"},{"location":"api/saliency/#example","text":"from xplique.attributions import Saliency # load images, labels and model # ... method = Saliency ( model ) explanations = method . explain ( images , labels )","title":"Example"},{"location":"api/saliency/#Saliency","text":"Used to compute the absolute gradient of the output relative to the input.","title":"Saliency"},{"location":"api/smoothgrad/","text":"SmoothGrad \u00b6 SmoothGrad is a gradient-based explanation method, which, as the name suggests, averages the gradient at several points corresponding to small perturbations around the point of interest. The smoothing effect induced by the average help reducing the visual noise, and hence improve the explanations. Quote [...] The gradient at any given point will be less meaningful than a local average of gradient values. This suggests a new way to create improved sensitivity maps: instead of basing a visualization directly on the gradient, we could base it on a smoothing of the gradients with a Gaussian kernel. -- SmoothGrad: removing noise by adding noise (2017) 1 More precisely, the explanation \\phi_x \\phi_x for an input x x , for a given class c c is defined as \\phi_x = \\underset{\\xi ~\\sim~ \\mathcal{N}(0, \\sigma^2)}{\\mathbb{E}} \\Big{[} \\frac { \\partial{S_c(x + \\xi)} } { \\partial{x} } \\Big{]} \\phi_x = \\underset{\\xi ~\\sim~ \\mathcal{N}(0, \\sigma^2)}{\\mathbb{E}} \\Big{[} \\frac { \\partial{S_c(x + \\xi)} } { \\partial{x} } \\Big{]} with S_c S_c the unormalized class score (layer before softmax). The \\sigma \\sigma in the formula is controlled using the noise parameter, and the expectation is estimated using multiple samples. Example \u00b6 from xplique.attributions import SmoothGrad # load images, labels and model # ... method = SmoothGrad ( model , nb_samples = 50 , noise = 0.15 ) explanations = method . explain ( images , labels ) SmoothGrad \u00b6 Used to compute the SmoothGrad, by averaging Saliency maps of noisy samples centered on the original sample. __init__( self , model : tf.keras.Model , output_layer : Union[str, int, None] = -1 , batch_size : Union[int, None] = 32 , nb_samples : int = 50 , noise : float = 0.2) \u00b6 Parameters model : tf.keras.Model Model used for computing explanations. output_layer : Union[str, int, None] = -1 Layer to target for the output (e.g logits or after softmax), if int, will be be interpreted as layer index, if string will look for the layer name. Default to the last layer, it is recommended to use the layer before Softmax. batch_size : Union[int, None] = 32 Number of samples to explain at once, if None compute all at once. nb_samples : int = 50 Number of noisy samples generated for the smoothing procedure. noise : float = 0.2 Scalar, noise used as standard deviation of a normal law centered on zero. explain( self , inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] , targets : Union[tf.Tensor, numpy.ndarray, None] = None) -> tf.Tensor \u00b6 Compute SmoothGrad for a batch of samples. Parameters inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] Input samples to be explained. targets : Union[tf.Tensor, numpy.ndarray, None] = None One-hot encoded labels or regression target (e.g {+1, -1}), one for each sample. Return explanations : tf.Tensor Smoothed gradients, same shape as the inputs. SmoothGrad: removing noise by adding noise (2017) \u21a9","title":"SmoothGrad"},{"location":"api/smoothgrad/#smoothgrad","text":"SmoothGrad is a gradient-based explanation method, which, as the name suggests, averages the gradient at several points corresponding to small perturbations around the point of interest. The smoothing effect induced by the average help reducing the visual noise, and hence improve the explanations. Quote [...] The gradient at any given point will be less meaningful than a local average of gradient values. This suggests a new way to create improved sensitivity maps: instead of basing a visualization directly on the gradient, we could base it on a smoothing of the gradients with a Gaussian kernel. -- SmoothGrad: removing noise by adding noise (2017) 1 More precisely, the explanation \\phi_x \\phi_x for an input x x , for a given class c c is defined as \\phi_x = \\underset{\\xi ~\\sim~ \\mathcal{N}(0, \\sigma^2)}{\\mathbb{E}} \\Big{[} \\frac { \\partial{S_c(x + \\xi)} } { \\partial{x} } \\Big{]} \\phi_x = \\underset{\\xi ~\\sim~ \\mathcal{N}(0, \\sigma^2)}{\\mathbb{E}} \\Big{[} \\frac { \\partial{S_c(x + \\xi)} } { \\partial{x} } \\Big{]} with S_c S_c the unormalized class score (layer before softmax). The \\sigma \\sigma in the formula is controlled using the noise parameter, and the expectation is estimated using multiple samples.","title":"SmoothGrad"},{"location":"api/smoothgrad/#example","text":"from xplique.attributions import SmoothGrad # load images, labels and model # ... method = SmoothGrad ( model , nb_samples = 50 , noise = 0.15 ) explanations = method . explain ( images , labels )","title":"Example"},{"location":"api/smoothgrad/#SmoothGrad","text":"Used to compute the SmoothGrad, by averaging Saliency maps of noisy samples centered on the original sample.","title":"SmoothGrad"},{"location":"api/square_grad/","text":"Square Grad \u00b6 Similar to SmoothGrad, Square Grad average the square of the gradients. \\phi_x = \\underset{\\xi ~\\sim~ \\mathcal{N}(0, \\sigma^2)}{\\mathbb{E}} \\Big{[}\\Big{(} \\frac { \\partial{S_c(x + \\xi)} } { \\partial{x} } \\Big{)}^2\\Big{]} \\phi_x = \\underset{\\xi ~\\sim~ \\mathcal{N}(0, \\sigma^2)}{\\mathbb{E}} \\Big{[}\\Big{(} \\frac { \\partial{S_c(x + \\xi)} } { \\partial{x} } \\Big{)}^2\\Big{]} with S_c S_c the unormalized class score (layer before softmax). The \\sigma \\sigma in the formula is controlled using the noise parameter. Example \u00b6 from xplique.attributions import SquareGrad # load images, labels and model # ... method = SquareGrad ( model , nb_samples = 50 , noise = 0.15 ) explanations = method . explain ( images , labels ) SquareGrad \u00b6 SquareGrad (or SmoothGrad^2) is an unpublished variant of classic SmoothGrad which squares each gradients of the noisy inputs before averaging. __init__( self , model : tf.keras.Model , output_layer : Union[str, int, None] = -1 , batch_size : Union[int, None] = 32 , nb_samples : int = 50 , noise : float = 0.2) \u00b6 Parameters model : tf.keras.Model Model used for computing explanations. output_layer : Union[str, int, None] = -1 Layer to target for the output (e.g logits or after softmax), if int, will be be interpreted as layer index, if string will look for the layer name. Default to the last layer, it is recommended to use the layer before Softmax. batch_size : Union[int, None] = 32 Number of samples to explain at once, if None compute all at once. nb_samples : int = 50 Number of noisy samples generated for the smoothing procedure. noise : float = 0.2 Scalar, noise used as standard deviation of a normal law centered on zero. explain( self , inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] , targets : Union[tf.Tensor, numpy.ndarray, None] = None) -> tf.Tensor \u00b6 Compute SmoothGrad for a batch of samples. Parameters inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] Input samples to be explained. targets : Union[tf.Tensor, numpy.ndarray, None] = None One-hot encoded labels or regression target (e.g {+1, -1}), one for each sample. Return explanations : tf.Tensor Smoothed gradients, same shape as the inputs.","title":"SquareGrad"},{"location":"api/square_grad/#square-grad","text":"Similar to SmoothGrad, Square Grad average the square of the gradients. \\phi_x = \\underset{\\xi ~\\sim~ \\mathcal{N}(0, \\sigma^2)}{\\mathbb{E}} \\Big{[}\\Big{(} \\frac { \\partial{S_c(x + \\xi)} } { \\partial{x} } \\Big{)}^2\\Big{]} \\phi_x = \\underset{\\xi ~\\sim~ \\mathcal{N}(0, \\sigma^2)}{\\mathbb{E}} \\Big{[}\\Big{(} \\frac { \\partial{S_c(x + \\xi)} } { \\partial{x} } \\Big{)}^2\\Big{]} with S_c S_c the unormalized class score (layer before softmax). The \\sigma \\sigma in the formula is controlled using the noise parameter.","title":"Square Grad"},{"location":"api/square_grad/#example","text":"from xplique.attributions import SquareGrad # load images, labels and model # ... method = SquareGrad ( model , nb_samples = 50 , noise = 0.15 ) explanations = method . explain ( images , labels )","title":"Example"},{"location":"api/square_grad/#SquareGrad","text":"SquareGrad (or SmoothGrad^2) is an unpublished variant of classic SmoothGrad which squares each gradients of the noisy inputs before averaging.","title":"SquareGrad"},{"location":"api/tcav/","text":"TCAV \u00b6 TCAV or Testing with Concept Activation Vector consist consists in using a concept activation vector (CAV) to quantify the relationship between this concept and a class. This is done by using the directional derivative of the concept vector on several samples of a given class and measuring the percentage of positive (a positive directional derivative indicating that an infinitesimal addition of the concept increases the probability of the class). For a Concept Activation Vector v_l v_l of a layer f_l f_l of a model, and f_{c} f_{c} the logit of the class c c , we measure the directional derivative S_c(x) = v_l \\cdot \\frac{ \\partial{f_c(x)} } { \\partial{f_l}(x) } S_c(x) = v_l \\cdot \\frac{ \\partial{f_c(x)} } { \\partial{f_l}(x) } . The TCAV score is the percentage of elements of the class c c for which the S_c S_c is positive. TCAV_c = \\frac{|x \\in \\mathcal{X}^c : S_c(x) > 0 |}{ | \\mathcal{X}^c | } TCAV_c = \\frac{|x \\in \\mathcal{X}^c : S_c(x) > 0 |}{ | \\mathcal{X}^c | } Example \u00b6 from xplique.concepts import Tcav tcav_renderer = Tcav ( model , 'mixed4d' ) # you can also pass the layer index (e.g -1) tcav_score = tcav_renderer ( samples , class_index , cav ) Tcav \u00b6 Used to Test a Concept Activation Vector, using the sign of the directional derivative of a concept vector relative to a class. __init__( self , model : tf.keras.Model , target_layer : Union[str, int] , batch_size : Union[int, None] = 64) \u00b6 Parameters model : tf.keras.Model Model to extract concept from. target_layer : Union[str, int] Index of the target layer or name of the layer. batch_size : Union[int, None] = 64 Batch size during the predictions. directional_derivative( multi_head_model : tf.keras.Model , inputs : tf.Tensor , label : int , cav : tf.Tensor) -> tf.Tensor \u00b6 Compute the gradient of the label relative to the activations of the CAV layer. Parameters multi_head_model : tf.keras.Model Model reconfigured, first output is the activations of the CAV layer, and the second output is the prediction layer. inputs : tf.Tensor Input sample on which to test the influence of the concept. label : int Index of the class to test. cav : tf.Tensor Concept Activation Vector, same shape as the activations output. Return directional_derivative : tf.Tensor Directional derivative values of each samples. score( self , inputs : tf.Tensor , label : int , cav : tf.Tensor) -> float \u00b6 Compute and return the TCAV score of the CAV associated to class tested. Parameters inputs : tf.Tensor Input sample on which to test the influence of the concept. label : int Index of the class to test. cav : tf.Tensor Concept Activation Vector, see CAV module. Return tcav : float Percentage of sample for which increasing the concept has a positive impact on the class logit. Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV) (2018). \u21a9","title":"Tcav"},{"location":"api/tcav/#tcav","text":"TCAV or Testing with Concept Activation Vector consist consists in using a concept activation vector (CAV) to quantify the relationship between this concept and a class. This is done by using the directional derivative of the concept vector on several samples of a given class and measuring the percentage of positive (a positive directional derivative indicating that an infinitesimal addition of the concept increases the probability of the class). For a Concept Activation Vector v_l v_l of a layer f_l f_l of a model, and f_{c} f_{c} the logit of the class c c , we measure the directional derivative S_c(x) = v_l \\cdot \\frac{ \\partial{f_c(x)} } { \\partial{f_l}(x) } S_c(x) = v_l \\cdot \\frac{ \\partial{f_c(x)} } { \\partial{f_l}(x) } . The TCAV score is the percentage of elements of the class c c for which the S_c S_c is positive. TCAV_c = \\frac{|x \\in \\mathcal{X}^c : S_c(x) > 0 |}{ | \\mathcal{X}^c | } TCAV_c = \\frac{|x \\in \\mathcal{X}^c : S_c(x) > 0 |}{ | \\mathcal{X}^c | }","title":"TCAV"},{"location":"api/tcav/#example","text":"from xplique.concepts import Tcav tcav_renderer = Tcav ( model , 'mixed4d' ) # you can also pass the layer index (e.g -1) tcav_score = tcav_renderer ( samples , class_index , cav )","title":"Example"},{"location":"api/tcav/#Tcav","text":"Used to Test a Concept Activation Vector, using the sign of the directional derivative of a concept vector relative to a class.","title":"Tcav"},{"location":"api/vargrad/","text":"VarGrad \u00b6 Similar to SmoothGrad, VarGrad a variance analog of SmoothGrad, and can be defined as follows: \\phi_x = \\underset{\\xi ~\\sim\\ \\mathcal{N}(0, \\sigma^2)}{\\mathcal{V}} \\Big{[}\\frac { \\partial{S_c(x + \\xi)} } { \\partial{x} }\\Big{]} \\phi_x = \\underset{\\xi ~\\sim\\ \\mathcal{N}(0, \\sigma^2)}{\\mathcal{V}} \\Big{[}\\frac { \\partial{S_c(x + \\xi)} } { \\partial{x} }\\Big{]} with S_c S_c the unormalized class score (layer before softmax). The \\sigma \\sigma in the formula is controlled using the noise parameter. Example \u00b6 from xplique.attributions import VarGrad # load images, labels and model # ... method = VarGrad ( model , nb_samples = 50 , noise = 0.15 ) explanations = method . explain ( images , labels ) VarGrad \u00b6 VarGrad is a variance analog to SmoothGrad. __init__( self , model : tf.keras.Model , output_layer : Union[str, int, None] = -1 , batch_size : Union[int, None] = 32 , nb_samples : int = 50 , noise : float = 0.2) \u00b6 Parameters model : tf.keras.Model Model used for computing explanations. output_layer : Union[str, int, None] = -1 Layer to target for the output (e.g logits or after softmax), if int, will be be interpreted as layer index, if string will look for the layer name. Default to the last layer, it is recommended to use the layer before Softmax. batch_size : Union[int, None] = 32 Number of samples to explain at once, if None compute all at once. nb_samples : int = 50 Number of noisy samples generated for the smoothing procedure. noise : float = 0.2 Scalar, noise used as standard deviation of a normal law centered on zero. explain( self , inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] , targets : Union[tf.Tensor, numpy.ndarray, None] = None) -> tf.Tensor \u00b6 Compute SmoothGrad for a batch of samples. Parameters inputs : Union[tf.Dataset, tf.Tensor, numpy.ndarray] Input samples to be explained. targets : Union[tf.Tensor, numpy.ndarray, None] = None One-hot encoded labels or regression target (e.g {+1, -1}), one for each sample. Return explanations : tf.Tensor Smoothed gradients, same shape as the inputs.","title":"VarGrad"},{"location":"api/vargrad/#vargrad","text":"Similar to SmoothGrad, VarGrad a variance analog of SmoothGrad, and can be defined as follows: \\phi_x = \\underset{\\xi ~\\sim\\ \\mathcal{N}(0, \\sigma^2)}{\\mathcal{V}} \\Big{[}\\frac { \\partial{S_c(x + \\xi)} } { \\partial{x} }\\Big{]} \\phi_x = \\underset{\\xi ~\\sim\\ \\mathcal{N}(0, \\sigma^2)}{\\mathcal{V}} \\Big{[}\\frac { \\partial{S_c(x + \\xi)} } { \\partial{x} }\\Big{]} with S_c S_c the unormalized class score (layer before softmax). The \\sigma \\sigma in the formula is controlled using the noise parameter.","title":"VarGrad"},{"location":"api/vargrad/#example","text":"from xplique.attributions import VarGrad # load images, labels and model # ... method = VarGrad ( model , nb_samples = 50 , noise = 0.15 ) explanations = method . explain ( images , labels )","title":"Example"},{"location":"api/vargrad/#VarGrad","text":"VarGrad is a variance analog to SmoothGrad.","title":"VarGrad"}]}